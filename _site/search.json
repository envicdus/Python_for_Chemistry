[
  {
    "objectID": "outline_checklist.html",
    "href": "outline_checklist.html",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Fundamental concepts of linear systems\nSolution methods\n\nGaussian elimination\nLU decomposition\nSolving overdetermined and underdetermined systems\n\nComputational approaches\n\nNumPy and SciPy linear algebra solvers\n\nIterative solution techniques\nApplications in chemical equilibrium modeling\n\nGravimetry\nTitrimetry\n\n\n\n\n\n\nVector and matrix operations\n\nVector spaces\nLinear transformations\nEigenvalues and eigenvectors\n\nMatrix decomposition techniques\n\nSingular Value Decomposition (SVD)\nPrincipal Component Analysis (PCA)\nCholesky decomposition\n\nSolving linear systems in computational chemistry\n\nQuantum chemical calculations\nMolecular structure optimization\nThermodynamic property predictions\n\n\n\n\n\n\n\nGroup theory fundamentals\n\nSymmetry groups\nMolecular point groups\nRepresentation theory\n\nApplications in quantum mechanics\n\nMolecular orbital theory\nSpectroscopic selection rules\nSymmetry-based computational techniques\n\nAlgebraic structures in chemical modeling\n\nRing theory\nField theory\nCrystallographic group analysis\n\nComputational implementations\n\nSymPy for symbolic algebra\nGroup theory calculations\nSymmetry analysis of molecular structures\n\n\n\n\n\nMolecular symmetry classification\nComputational representation of symmetry operations\nUsing symmetry to simplify quantum chemical calculations\nIdentifying and applying molecular symmetry constraints\n\n\n\n\n\n\nMultivariate calculus\n\nPartial derivatives\nGradient and divergence\nOptimization techniques\n\nNumerical integration methods\n\nQuadrature techniques\nMonte Carlo integration\n\nSolving differential equations\n\nOrdinary Differential Equations (ODEs)\nPartial Differential Equations (PDEs)\n\nEnergy minimization algorithms\nApplications\n\nideal and real gas law calculations\nModel pressure-volume-temperature relationships\ncomputational models for various equations of state\n\n\n\n\n\n\nProbability distributions\n\nGaussian (Normal) distribution\nBoltzmann distribution\n\nStatistical mechanics fundamentals\n\nPartition functions\nThermodynamic ensemble methods\n\nStochastic processes in molecular systems\nError analysis and statistical uncertainty\n\n\n\n\n\nNumerical approximation techniques\n\nTaylor series expansions\nInterpolation methods\nNumerical differentiation\n\nRoot-finding algorithms\n\nNewton-Raphson method\nBisection method\n\nOptimization techniques\n\nGradient descent\nSimulated annealing\nGenetic algorithms\n\n\n\n\n\n\nNumPy and SciPy for mathematical computations\nSymPy for symbolic mathematics\nImplementing mathematical algorithms\nPerformance optimization techniques\nVisualization of mathematical concepts\n\n\n\n\n\n\nOverview of computational chemistry\nRole of Python in scientific computing\nEssential Python libraries for chemistry\n\nNumPy for numerical computing\nSciPy for scientific calculations\nPandas for data manipulation\nMatplotlib and Seaborn for data visualization\n\nConnecting mathematical foundations to computational chemistry\n\n\n\n\n\nMolecular structure representation\n\nSMILES (Simplified Molecular Input Line Entry System)\nMolecular file formats (PDB, MOL, XYZ)\n\nLibraries for molecular handling\n\nRDKit for cheminformatics\nOpenBabel for molecular conversions\nASE (Atomic Simulation Environment)\n\nCreating and manipulating molecular structures programmatically\nCalculating molecular properties\n\nMolecular weight\nElemental composition\nStructural descriptors\n\n\n\n\n\n\nIntroduction to quantum chemistry calculations\nInterfacing with quantum chemistry packages\n\nUtilizing PySCF for electronic structure calculations\nIntegrating with ORCA through subprocess calls\n\nPerforming basic quantum chemistry operations\n\nHartree-Fock calculations\nDensity Functional Theory (DFT) calculations\nGeometry optimizations\n\nExtracting and analyzing quantum chemical data\n\n\n\n\n\nPrinciples of molecular dynamics\nPython libraries for molecular dynamics\n\nMDAnalysis for trajectory analysis\nOpenMM for MD simulations\n\nImplementing basic molecular dynamics simulations\n\nSetting up simulation parameters\nRunning and analyzing trajectories\nCalculating thermodynamic properties\n\n\n\n\n\n\nData preprocessing and cleaning\nStatistical analysis of chemical data\n\nDescriptive statistics\nCorrelation analysis\nPrincipal Component Analysis (PCA)\n\nMachine learning applications\n\nScikit-learn for predictive modeling\nRegression and classification of chemical properties\nBuilding QSAR (Quantitative Structure-Activity Relationship) models\n\n\n\n\n\n\n2D molecular structure visualization\n\nUsing RDKit for molecular drawing\n\n3D molecular visualization\n\nPyMOL scripting\nMatplotlib 3D plotting\n\nAdvanced data visualization techniques\n\nInteractive plots with Plotly\nCreating publication-quality graphics\n\n\n\n\n\n\nDrug discovery workflow\n\nVirtual screening\nMolecular docking simulation\n\nMaterial science applications\n\nCrystal structure analysis\nElectronic property prediction\n\nChemical reaction modeling\n\nReaction pathway analysis\nTransition state calculations\n\n\n\n\n\n\nParallel computing in computational chemistry\n\nMultiprocessing\nGPU acceleration with CuPy\n\nWriting efficient scientific Python code\n\nNumba for just-in-time compilation\nVectorization techniques\n\nCloud computing and remote computation strategies\n\n\n\n\n\nVersion control with Git\nJupyter Notebook for interactive computing\nCreating reproducible computational chemistry workflows\nBest practices in scientific computing\nSetting up a computational chemistry development environment\n\n\n\n\n\nRecommended books and online courses\nOpen-source computational chemistry software\nResearch databases and public datasets\nCommunity resources and forums\nEmerging trends in computational chemistry and Python\n\n\n\n\n\nA: Python library installation guide\nB: Sample scripts and code snippets\nC: Computational chemistry file format references\nD: Recommended reading and research papers",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#mathematical-foundations-for-computational-chemistry",
    "href": "outline_checklist.html#mathematical-foundations-for-computational-chemistry",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Fundamental concepts of linear systems\nSolution methods\n\nGaussian elimination\nLU decomposition\nSolving overdetermined and underdetermined systems\n\nComputational approaches\n\nNumPy and SciPy linear algebra solvers\n\nIterative solution techniques\nApplications in chemical equilibrium modeling\n\nGravimetry\nTitrimetry\n\n\n\n\n\n\nVector and matrix operations\n\nVector spaces\nLinear transformations\nEigenvalues and eigenvectors\n\nMatrix decomposition techniques\n\nSingular Value Decomposition (SVD)\nPrincipal Component Analysis (PCA)\nCholesky decomposition\n\nSolving linear systems in computational chemistry\n\nQuantum chemical calculations\nMolecular structure optimization\nThermodynamic property predictions\n\n\n\n\n\n\n\nGroup theory fundamentals\n\nSymmetry groups\nMolecular point groups\nRepresentation theory\n\nApplications in quantum mechanics\n\nMolecular orbital theory\nSpectroscopic selection rules\nSymmetry-based computational techniques\n\nAlgebraic structures in chemical modeling\n\nRing theory\nField theory\nCrystallographic group analysis\n\nComputational implementations\n\nSymPy for symbolic algebra\nGroup theory calculations\nSymmetry analysis of molecular structures\n\n\n\n\n\nMolecular symmetry classification\nComputational representation of symmetry operations\nUsing symmetry to simplify quantum chemical calculations\nIdentifying and applying molecular symmetry constraints\n\n\n\n\n\n\nMultivariate calculus\n\nPartial derivatives\nGradient and divergence\nOptimization techniques\n\nNumerical integration methods\n\nQuadrature techniques\nMonte Carlo integration\n\nSolving differential equations\n\nOrdinary Differential Equations (ODEs)\nPartial Differential Equations (PDEs)\n\nEnergy minimization algorithms\nApplications\n\nideal and real gas law calculations\nModel pressure-volume-temperature relationships\ncomputational models for various equations of state\n\n\n\n\n\n\nProbability distributions\n\nGaussian (Normal) distribution\nBoltzmann distribution\n\nStatistical mechanics fundamentals\n\nPartition functions\nThermodynamic ensemble methods\n\nStochastic processes in molecular systems\nError analysis and statistical uncertainty\n\n\n\n\n\nNumerical approximation techniques\n\nTaylor series expansions\nInterpolation methods\nNumerical differentiation\n\nRoot-finding algorithms\n\nNewton-Raphson method\nBisection method\n\nOptimization techniques\n\nGradient descent\nSimulated annealing\nGenetic algorithms\n\n\n\n\n\n\nNumPy and SciPy for mathematical computations\nSymPy for symbolic mathematics\nImplementing mathematical algorithms\nPerformance optimization techniques\nVisualization of mathematical concepts",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#introduction-to-computational-chemistry-with-python",
    "href": "outline_checklist.html#introduction-to-computational-chemistry-with-python",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Overview of computational chemistry\nRole of Python in scientific computing\nEssential Python libraries for chemistry\n\nNumPy for numerical computing\nSciPy for scientific calculations\nPandas for data manipulation\nMatplotlib and Seaborn for data visualization\n\nConnecting mathematical foundations to computational chemistry",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#molecular-representation-and-manipulation",
    "href": "outline_checklist.html#molecular-representation-and-manipulation",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Molecular structure representation\n\nSMILES (Simplified Molecular Input Line Entry System)\nMolecular file formats (PDB, MOL, XYZ)\n\nLibraries for molecular handling\n\nRDKit for cheminformatics\nOpenBabel for molecular conversions\nASE (Atomic Simulation Environment)\n\nCreating and manipulating molecular structures programmatically\nCalculating molecular properties\n\nMolecular weight\nElemental composition\nStructural descriptors",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#quantum-chemistry-computations",
    "href": "outline_checklist.html#quantum-chemistry-computations",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Introduction to quantum chemistry calculations\nInterfacing with quantum chemistry packages\n\nUtilizing PySCF for electronic structure calculations\nIntegrating with ORCA through subprocess calls\n\nPerforming basic quantum chemistry operations\n\nHartree-Fock calculations\nDensity Functional Theory (DFT) calculations\nGeometry optimizations\n\nExtracting and analyzing quantum chemical data",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#molecular-dynamics-simulations",
    "href": "outline_checklist.html#molecular-dynamics-simulations",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Principles of molecular dynamics\nPython libraries for molecular dynamics\n\nMDAnalysis for trajectory analysis\nOpenMM for MD simulations\n\nImplementing basic molecular dynamics simulations\n\nSetting up simulation parameters\nRunning and analyzing trajectories\nCalculating thermodynamic properties",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#computational-chemistry-data-analysis",
    "href": "outline_checklist.html#computational-chemistry-data-analysis",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Data preprocessing and cleaning\nStatistical analysis of chemical data\n\nDescriptive statistics\nCorrelation analysis\nPrincipal Component Analysis (PCA)\n\nMachine learning applications\n\nScikit-learn for predictive modeling\nRegression and classification of chemical properties\nBuilding QSAR (Quantitative Structure-Activity Relationship) models",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#visualization-and-advanced-plotting",
    "href": "outline_checklist.html#visualization-and-advanced-plotting",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "2D molecular structure visualization\n\nUsing RDKit for molecular drawing\n\n3D molecular visualization\n\nPyMOL scripting\nMatplotlib 3D plotting\n\nAdvanced data visualization techniques\n\nInteractive plots with Plotly\nCreating publication-quality graphics",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#practical-project-examples",
    "href": "outline_checklist.html#practical-project-examples",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Drug discovery workflow\n\nVirtual screening\nMolecular docking simulation\n\nMaterial science applications\n\nCrystal structure analysis\nElectronic property prediction\n\nChemical reaction modeling\n\nReaction pathway analysis\nTransition state calculations",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#performance-optimization-and-advanced-topics",
    "href": "outline_checklist.html#performance-optimization-and-advanced-topics",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Parallel computing in computational chemistry\n\nMultiprocessing\nGPU acceleration with CuPy\n\nWriting efficient scientific Python code\n\nNumba for just-in-time compilation\nVectorization techniques\n\nCloud computing and remote computation strategies",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#practical-tools-and-workflow-integration",
    "href": "outline_checklist.html#practical-tools-and-workflow-integration",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Version control with Git\nJupyter Notebook for interactive computing\nCreating reproducible computational chemistry workflows\nBest practices in scientific computing\nSetting up a computational chemistry development environment",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#resources-and-further-learning",
    "href": "outline_checklist.html#resources-and-further-learning",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "Recommended books and online courses\nOpen-source computational chemistry software\nResearch databases and public datasets\nCommunity resources and forums\nEmerging trends in computational chemistry and Python",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "outline_checklist.html#appendices",
    "href": "outline_checklist.html#appendices",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "A: Python library installation guide\nB: Sample scripts and code snippets\nC: Computational chemistry file format references\nD: Recommended reading and research papers",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "",
    "text": "Code\nimport numpy as np\nfrom numpy.linalg import solve\nIterative solution techniques are methods used to approximate solutions to mathematical problems, especially when direct (analytical) solutions are difficult or impossible to obtain. In this context, these methods are called Relaxation Methods, as they are used for solving systems of equations, including nonlinear systems. These methods work by iteratively refining an initial guess until a satisfactory solution is reached. Before we begin, I’ll encourage everyone who will read this to skip the example if your not fond of number as you will see a lot of equations and numbers here for my examples as I want to be thorough (sorry 😅). Below are some key iterative solution techniques:",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#jacobi-method",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#jacobi-method",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "1. Jacobi Method",
    "text": "1. Jacobi Method\nThe Jacobi method is an iterative algorithm used to approximate the solution of a system of linear equations. It’s particularly useful for diagonally dominant systems. Here’s a breakdown of the method:\n\\[\n\\displaystyle\n\\begin{align*}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n\\vdots \\hspace{1cm} \\vdots \\hspace{2cm} \\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n &= b_n\n\\end{align*}\n\\] In matrix form, this can be written as:\n\\[Ax = b\\]\nwhere \\(A\\) is an \\(n \\times n\\) matrix, \\(x\\) is an \\(n \\times 1\\) vector of unknowns, and \\(b\\) is an \\(n \\times 1\\) vector of constants.\n\nFormula\nThe Jacobi method rewrites each equation to solve for one variable in terms of the others. For the \\(i\\)-th equation, we solve for \\(x_i\\):\n\\[x_i = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j=1, j\\neq i}^{n} a_{ij}x_j\\right)\\]\nIn iterative form, the formula becomes:\n\\[x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j=1, j\\neq i}^{n} a_{ij}x_j^{(k)}\\right)\\]\nwhere \\(x_i^{(k)}\\) represents the value of \\(x_i\\) at the \\(k\\)-th iteration.\n\n\nAlgorithm\n\n\\(\\textbf{Initialization:}\\) Choose an initial guess for the solution vector \\(x^{(0)} = (x_1^{(0)}, x_2^{(0)}, \\dots, x_n^{(0)})\\).\n\\(\\textbf{Iteration:}\\) For \\(k = 0, 1, 2, \\dots\\) until convergence:\n\nFor each \\(i = 1, 2, \\dots, n\\): \\[x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j=1, j\\neq i}^{n} a_{ij}x_j^{(k)}\\right)\\]\n\n\\(\\textbf{Convergence:}\\) Check for convergence. This can be done by comparing the difference between successive iterations: \\[\\|x^{(k+1)} - x^{(k)}\\| &lt; \\text{tolerance}\\] where \\(\\text{tolerance}\\) is a small positive number. Alternatively, check the absolute value of the difference of each variable between iterations: \\(|x_i^{(k+1)}-x_i^{(k)}| &lt; \\text{tolerance}\\) for all i.\n\\(\\textbf{Output:}\\) The final approximation \\(x^{(k+1)}\\) is the approximate solution.\n\n\n\nMatrix Form\nThe matrix \\(A\\) can be decomposed into:\n\n\\(D\\): a diagonal matrix containing the diagonal elements of \\(A\\).\n\\(L\\): a lower triangular matrix with the negative of the off-diagonal elements below the main diagonal.\n\\(U\\): an upper triangular matrix with the negative of the off-diagonal elements above the main diagonal.\n\nThen, \\(A = D - L - U\\). The iterative formula becomes:\n\\[x^{(k+1)} = D^{-1}(L + U)x^{(k)} + D^{-1}b\\]\nOr:\n\\[x^{(k+1)} = Tx^{(k)} + c\\]\nwhere:\n\n\\(T = D^{-1}(L + U)\\) is the iteration matrix.\n\\(c = D^{-1}b\\).\n\n\n\nConvergence\nThe Jacobi method converges if the matrix \\(A\\) is strictly diagonally dominant. A matrix is strictly diagonally dominant if:\n\\[|a_{ii}| &gt; \\sum_{j=1, j\\neq i}^{n} |a_{ij}|\\]\nfor all \\(i = 1, 2, \\dots, n\\).\nConvergence is also guaranteed if the spectral radius of the iteration matrix \\(T\\) is less than 1, i.e., \\(\\rho(T) &lt; 1\\).\n\n\nAdvantages\n\nSimple to understand and implement.\nRelatively easy to parallelize.\n\n\n\nDisadvantages\n\nCan be slow to converge, especially for large systems.\nConvergence is not guaranteed for all systems.\nThe method requires storage of two vectors, the old values, and the new values.\n\n\n\nExample\n\\[\n\\begin{align*}\n10x_1 - 2x_2 - x_3 - x_4 &= 3 \\\\\n-2x_1 + 10x_2 - x_3 - x_4 &= 15 \\\\\n-x_1 - x_2 + 10x_3 - 2x_4 &= 27 \\\\\n-x_1 - x_2 - 2x_3 + 10x_4 &= -9\n\\end{align*}\n\\] In matrix form, \\(Ax = b\\), where:\n\\[ A = \\begin{pmatrix} 10 & -2 & -1 & -1 \\\\ -2 & 10 & -1 & -1 \\\\ -1 & -1 & 10 & -2 \\\\ -1 & -1 & -2 & 10 \\end{pmatrix}, \\quad x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{pmatrix} \\]\nThis system is diagonally dominant, so the Jacobi method should converge.\nSolution We rewrite each equation to solve for one variable:\n\\[\\begin{align*}\nx_1^{(k+1)} &= \\frac{1}{10}(3 + 2x_2^{(k)} + x_3^{(k)} + x_4^{(k)}) \\\\\nx_2^{(k+1)} &= \\frac{1}{10}(15 + 2x_1^{(k)} + x_3^{(k)} + x_4^{(k)}) \\\\\nx_3^{(k+1)} &= \\frac{1}{10}(27 + x_1^{(k)} + x_2^{(k)} + 2x_4^{(k)}) \\\\\nx_4^{(k+1)} &= \\frac{1}{10}(-9 + x_1^{(k)} + x_2^{(k)} + 2x_3^{(k)})\n\\end{align*}\\]\nLet’s use an initial guess of \\(x^{(0)} = (0, 0, 0, 0)\\). We’ll perform a few iterations:\nIteration 1:\n\\[\\begin{align*}\nx_1^{(1)} &= \\frac{1}{10}(3) = 0.3 \\\\\nx_2^{(1)} &= \\frac{1}{10}(15) = 1.5 \\\\\nx_3^{(1)} &= \\frac{1}{10}(27) = 2.7 \\\\\nx_4^{(1)} &= \\frac{1}{10}(-9) = -0.9\n\\end{align*}\\]\n\\(x^{(1)} = (0.3, 1.5, 2.7, -0.9)\\)\nIteration 2:\n\\[\\begin{align*}\nx_1^{(2)} &= \\frac{1}{10}(3 + 2(1.5) + 2.7 + (-0.9)) = \\frac{1}{10}(7.8) = 0.78 \\\\\nx_2^{(2)} &= \\frac{1}{10}(15 + 2(0.3) + 2.7 + (-0.9)) = \\frac{1}{10}(17.4) = 1.74 \\\\\nx_3^{(2)} &= \\frac{1}{10}(27 + 0.3 + 1.5 + 2(-0.9)) = \\frac{1}{10}(27.9) = 2.79 \\\\\nx_4^{(2)} &= \\frac{1}{10}(-9 + 0.3 + 1.5 + 2(2.7)) = \\frac{1}{10}(-9 + 7.2) = -0.18\n\\end{align*}\\]\n\\(x^{(2)} = (0.78, 1.74, 2.79, -0.18)\\)\nIteration 3:\n\\[\\begin{align*}\nx_1^{(3)} &= \\frac{1}{10}(3 + 2(1.74) + 2.79 + (-0.18)) = 0.909 \\\\\nx_2^{(3)} &= \\frac{1}{10}(15 + 2(0.78) + 2.79 + (-0.18)) = 1.917 \\\\\nx_3^{(3)} &= \\frac{1}{10}(27 + 0.78 + 1.74 + 2(-0.18)) = 2.916 \\\\\nx_4^{(3)} &= \\frac{1}{10}(-9 + 0.78 + 1.74 + 2(2.79)) = 0.01\n\\end{align*}\\]\n\\(x^{(3)} = (0.909, 1.917, 2.916, 0.01)\\)\nAs it may take up more space, we’ll just skip to Iteration 10:\nSo after 10 iterations we get \\(x^{(10)} = (0.99991, 1.99992, 2.99993, 0.00002)\\)\nExact Solution: The exact solution to the system is \\(x = (1, 2, 3, 0)\\).\nAs you can see, the convergence is slow, but the values are approaching the solution.\nHere is how it will be shown in Python:\n\n\nCode\ndef jacobi(A, b, x0, tol=1e-6, max_iter=100):\n    \"\"\"\n    Jacobi method for solving Ax = b.\n\n    Args:\n        A: Coefficient matrix (numpy array).\n        b: Right-hand side vector (numpy array).\n        x0: Initial guess (numpy array).\n        tol: Tolerance for convergence.\n        max_iter: Maximum number of iterations.\n\n    Returns:\n        x: Approximate solution (numpy array).\n    \"\"\"\n\n    n = len(b)\n    x = x0.copy()\n    x_new = np.zeros(n)\n    iteration = 0\n\n    for k in range(max_iter):\n        iteration = k + 1 #track iterations\n        for i in range(n):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n\n        if np.linalg.norm(x_new - x, ord=np.inf) &lt; tol:\n            return x_new, iteration\n\n        x[:] = x_new\n    return x_new, iteration # returns the value after max_iter reached.\n\n# Example usage\nA = np.array([[10, -2, -1, -1],\n              [-2, 10, -1, -1],\n              [-1, -1, 10, -2],\n              [-1, -1, -2, 10]])\n\nb = np.array([3, 15, 27, -9])\nx0 = np.zeros(4)\n\nsolution, iterations = jacobi(A, b, x0)\nprint(\"Approximate solution:\", solution)\nprint(\"Iterations:\", iterations)\n\n# Check the solution more precisely\nAx = np.dot(A, solution)\nprint(\"A*x =\", Ax)\nprint(\"b =\", b)\n\n\nApproximate solution: [ 9.99999356e-01  1.99999936e+00  2.99999936e+00 -6.44235264e-07]\nIterations: 16\nA*x = [ 2.99999613 14.99999613 26.99999613 -9.00000387]\nb = [ 3 15 27 -9]\n\n\nThe python code to solve this has a tolerance of \\(1 \\times 10^{-6}\\), which takes about 16 iterations to solve the system.\n\n\nSummary\n\nUpdates each variable using values from the previous iteration.\nSimple but may converge slowly.",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#gauss-seidel-method",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#gauss-seidel-method",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "2. Gauss-Seidel Method",
    "text": "2. Gauss-Seidel Method\nThe Gauss-Seidel method is an iterative technique used to solve a system of linear equations. It is an improvement over the Jacobi method, as it uses the most recently computed values of the unknowns in subsequent calculations.\nConsider a system of \\(n\\) linear equations with \\(n\\) unknowns:\n\\[\n\\begin{aligned}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n\\vdots \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad & \\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n &= b_n\n\\end{aligned}\n\\]\nIn matrix form, this can be written as:\n\\[\nAx = b\n\\]\nwhere \\(A\\) is the coefficient matrix, \\(x\\) is the vector of unknowns, and \\(b\\) is the vector of constants.\n\nFormula\nTo apply the Gauss-Seidel method, we solve each equation for the corresponding unknown:\n\\[\n\\begin{aligned}\nx_1 &= \\frac{1}{a_{11}} \\left( b_1 - a_{12}x_2 - a_{13}x_3 - \\cdots - a_{1n}x_n \\right) \\\\\nx_2 &= \\frac{1}{a_{22}} \\left( b_2 - a_{21}x_1 - a_{23}x_3 - \\cdots - a_{2n}x_n \\right) \\\\\n\\vdots \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad & \\vdots \\\\\nx_n &= \\frac{1}{a_{nn}} \\left( b_n - a_{n1}x_1 - a_{n2}x_2 - \\cdots - a_{n,n-1}x_{n-1} \\right)\n\\end{aligned}\n\\]\nThe iterative formula for the Gauss-Seidel method is:\n\\[\nx_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \\right)\n\\]\nwhere:\n\n\\(x_i^{(k)}\\) is the \\(i\\)-th component of the solution vector at the \\(k\\)-th iteration.\n\\(x_i^{(k+1)}\\) is the \\(i\\)-th component of the solution vector at the \\((k+1)\\)-th iteration.\nThe key difference from Jacobi is that we use the most recent values \\(x_j^{(k+1)}\\) for \\(j &lt; i\\) as soon as they are computed.\n\n\n\nAlgorithm\n\nChoose an initial guess \\(x^{(0)}\\).\nFor \\(k = 0, 1, 2, \\dots\\) until convergence:\n\nFor \\(i = 1, 2, \\dots, n\\):\n\nCalculate \\(x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \\right)\\).\n\n\nCheck for convergence using a suitable criterion, such as:\n\n\\(||x^{(k+1)} - x^{(k)}|| &lt; \\epsilon\\), where \\(\\epsilon\\) is a small tolerance.\n\\(||Ax^{(k+1)} - b|| &lt; \\epsilon\\).\n\n\n\n\nConvergence\nThe Gauss-Seidel method converges if the matrix \\(A\\) is:\n\nStrictly diagonally dominant: \\(|a_{ii}| &gt; \\sum_{j=1, j \\neq i}^{n} |a_{ij}|\\) for all \\(i\\).\nSymmetric positive definite.\n\n\n\nAdvantages and Disadvantages\nAdvantages:\n\nGenerally converges faster than the Jacobi method.\nUses less memory than some direct methods.\n\nDisadvantages:\n\nConvergence is not guaranteed for all matrices.\nCan be slower than direct methods for small systems.\nThe order in which equations are solved can affect convergence.\n\n\n\nExample\nwe will use the same example as above, but we will solve the system of linear equations using the Gauss-Seidel method:\n\\[\n\\begin{align*}\n10x_1 - 2x_2 - x_3 - x_4 &= 3 \\\\\n-2x_1 + 10x_2 - x_3 - x_4 &= 15 \\\\\n-x_1 - x_2 + 10x_3 - 2x_4 &= 27 \\\\\n-x_1 - x_2 - 2x_3 + 10x_4 &= -9\n\\end{align*}\n\\]\n\nIterative Formulas\nFirst, we solve each equation for the corresponding unknown:\n\\[\n\\begin{aligned}\nx_1 &= \\frac{1}{10}(3 + 2x_2 + x_3 + x_4) \\\\\nx_2 &= \\frac{1}{10}(15 + 2x_1 + x_3 + x_4) \\\\\nx_3 &= \\frac{1}{10}(27 + x_1 + x_2 + 2x_4) \\\\\nx_4 &= \\frac{1}{10}(-9 + x_1 + x_2 + 2x_3)\n\\end{aligned}\n\\]\n\n\nIterations\nWe start with an initial guess of \\(x^{(0)} = (0, 0, 0, 0)\\).\nIteration 1:\n\\[\n\\begin{aligned}\nx_1^{(1)} &= \\frac{1}{10}(3 + 2(0) + 0 + 0) = 0.3 \\\\\nx_2^{(1)} &= \\frac{1}{10}(15 + 2(0.3) + 0 + 0) = 1.56 \\\\\nx_3^{(1)} &= \\frac{1}{10}(27 + 0.3 + 1.56 + 2(0)) = 2.886 \\\\\nx_4^{(1)} &= \\frac{1}{10}(-9 + 0.3 + 1.56 + 2(2.886)) = -0.1368\n\\end{aligned}\n\\]\n\\(x^{(1)} = (0.3, 1.56, 2.886, -0.1368)\\)\nIteration 2:\n\\[\n\\begin{aligned}\nx_1^{(2)} &= \\frac{1}{10}(3 + 2(1.56) + 2.886 - 0.1368) = 0.88692 \\\\\nx_2^{(2)} &= \\frac{1}{10}(15 + 2(0.88692) + 2.886 - 0.1368) = 1.952304 \\\\\nx_3^{(2)} &= \\frac{1}{10}(27 + 0.88692 + 1.952304 + 2(-0.1368)) = 2.9698624 \\\\\nx_4^{(2)} &= \\frac{1}{10}(-9 + 0.88692 + 1.952304 + 2(2.9698624)) = 0.084894928\n\\end{aligned}\n\\]\n\\(x^{(2)} = (0.88692, 1.952304, 2.9698624, 0.084894928)\\)\nIteration 3:\n\\[\n\\begin{aligned}\nx_1^{(3)} &= \\frac{1}{10}(3 + 2(1.952304) + 2.9698624 + 0.084894928) = 0.995936576 \\\\\nx_2^{(3)} &= \\frac{1}{10}(15 + 2(0.995936576) + 2.9698624 + 0.084894928) = 1.9996590552 \\\\\nx_3^{(3)} &= \\frac{1}{10}(27 + 0.995936576 + 1.9996590552 + 2(0.084894928)) = 2.99652854864 \\\\\nx_4^{(3)} &= \\frac{1}{10}(-9 + 0.995936576 + 1.9996590552 + 2(2.99652854864)) = 0.0988652727488 \\\\\n\\end{aligned}\n\\]\n\\(x^{(3)} = (0.995936576, 1.9996590552, 2.99652854864, 0.0988652727488)\\)\nAfter a few more iterations, the solution converges to approximately:\n\\[\nx \\approx (1, 2, 3, 1)\n\\]\n\n\n\nExample on Python code\n\n\nCode\ndef gauss_seidel(A, b, x0, tol=1e-6, max_iter=100):\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Gauss-Seidel method.\n\n    Args:\n        A: The coefficient matrix (numpy array).\n        b: The constant vector (numpy array).\n        x0: The initial guess vector (numpy array).\n        max_iter: The maximum number of iterations.\n        tolerance: The tolerance for convergence.\n\n    Returns:\n        A tuple containing the solution vector (numpy array) and the number of iterations,\n        or (None, number of iterations) if convergence fails.\n    \"\"\"\n\n    n = len(b)\n    x = x0.copy()\n    x_new = x0.copy()\n    num_iter = 0\n\n    for k in range(max_iter):\n        num_iter += 1\n        for i in range(n):\n            s1 = sum(A[i][j] * x_new[j] for j in range(i))\n            s2 = sum(A[i][j] * x[j] for j in range(i + 1, n))\n            x_new[i] = (b[i] - s1 - s2) / A[i][i]\n\n        if np.linalg.norm(x_new - x) &lt; tol:\n            return x_new, num_iter\n\n        x = x_new.copy()\n\n    return None, num_iter  # Convergence failed\n\n# Example system of equations\nA = np.array([[10, -2, -1, -1],\n              [-2, 10, -1, -1],\n              [-1, -1, 10, -2],\n              [-1, -1, -2, 10]], dtype=float)\n\nb = np.array([3, 15, 27, -9], dtype=float)\nx0 = np.array([0, 0, 0, 0], dtype=float)\n\n# Solve using Gauss-Seidel\nsolution, iterations = gauss_seidel(A, b, x0)\n\nif solution is not None:\n    print(\"Solution:\")\n    print(solution)\n    print(\"Number of iterations:\", iterations)\n    print(\"Verification: Ax-b\")\n    print(np.dot(A,solution)-b)\n\nelse:\n    print(\"Gauss-Seidel method did not converge within the given iterations.\")\n    print(\"Number of iterations performed:\", iterations)\n\n\nSolution:\n[ 9.99999890e-01  1.99999994e+00  2.99999995e+00 -2.65476061e-08]\nNumber of iterations: 10\nVerification: Ax-b\n[-8.99262862e-07 -3.32124884e-07 -2.40979077e-07  0.00000000e+00]\n\n\nThis code illustrates the Gauss-Seidel method’s efficiency compared to the Jacobi method, as evidenced by the reduced number of iterations for convergence.\n\n\nSummary\n\nIterative Solution: Gauss-Seidel solves linear equation systems by iteratively refining an initial guess.\nImmediate Updates: It improves on the Jacobi method by using newly calculated variable values within the same iteration.\nConvergence Criteria: Convergence is guaranteed for strictly diagonally dominant or symmetric positive definite matrices.\nPerformance: Generally faster than Jacobi, but convergence is not universally guaranteed.",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#successive-over-relaxation-sor",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#successive-over-relaxation-sor",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "3. Successive Over-Relaxation (SOR)",
    "text": "3. Successive Over-Relaxation (SOR)\nSuccessive Over-Relaxation (SOR) is an iterative method used to solve a system of linear equations, particularly large sparse systems. It’s a variant of the Gauss-Seidel method, designed to accelerate its convergence.\n\nFormula\nConsider a system of linear equations:\n\\[Ax = b\\]\nwhere \\(A\\) is an \\(n \\times n\\) matrix, \\(x\\) is an \\(n \\times 1\\) vector of unknowns, and \\(b\\) is an \\(n \\times 1\\) vector of constants.\nWe can decompose the matrix \\(A\\) into its diagonal (\\(D\\)), lower triangular (\\(L\\)), and upper triangular (\\(U\\)) components:\n\\[A = D - L - U\\]\nThe Gauss-Seidel method updates the solution iteratively using the following formula:\n\\[x^{(k+1)} = (D - L)^{-1}Ux^{(k)} + (D - L)^{-1}b\\]\nIn component form, this is:\n\\[x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \\right)\\]\nThe SOR method introduces a relaxation parameter \\(\\omega\\) to accelerate convergence:\n\\[x^{(k+1)} = (D - \\omega L)^{-1} \\left[ (1 - \\omega)D + \\omega U \\right] x^{(k)} + \\omega (D - \\omega L)^{-1}b\\]\nIn component form:\n\\[x_i^{(k+1)} = (1 - \\omega)x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \\right)\\]\nOr, equivalently:\n\\[x_i^{(k+1)} = x_i^{(k)} + \\omega \\left( \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i}^{n} a_{ij}x_j^{(k)} \\right) \\right)\\]\nwhere \\(\\omega\\) is the relaxation parameter.\n\n\nAlgorithm\n\nIf \\(\\omega = 1\\), SOR reduces to the Gauss-Seidel method.\nIf \\(\\omega &lt; 1\\), it’s called under-relaxation, which can be used to ensure convergence for some systems.\nIf \\(\\omega &gt; 1\\), it’s called over-relaxation, which aims to accelerate convergence.\n\nThe optimal value of \\(\\omega\\) depends on the properties of the matrix \\(A\\). For certain classes of matrices (e.g., symmetric positive-definite matrices), an optimal \\(\\omega\\) can be determined analytically. For general matrices, it often requires numerical experimentation.\n\n\nConvergence\nThe convergence of SOR depends on the choice of \\(\\omega\\) and the properties of the matrix \\(A\\). For SOR to converge, the spectral radius of the iteration matrix must be less than 1.\n\n\nAdvantages and Disadvantages\nAdvantages: - Can significantly accelerate convergence compared to Gauss-Seidel. - Relatively simple to implement.\nDisadvantages: - Finding the optimal \\(\\omega\\) can be challenging. - Convergence is not guaranteed for all matrices.\n\n\nExample\nUsing the example from the previous methods:\n\\[\n\\begin{align*}\n10x_1 - 2x_2 - x_3 - x_4 &= 3 \\\\\n-2x_1 + 10x_2 - x_3 - x_4 &= 15 \\\\\n-x_1 - x_2 + 10x_3 - 2x_4 &= 27 \\\\\n-x_1 - x_2 - 2x_3 + 10x_4 &= -9\n\\end{align*}\n\\]\nSOR Formula Recap:\n\\[\n\\begin{align*}\nx_1^{(k+1)} &= (1 - \\omega)x_1^{(k)} + \\frac{\\omega}{10}\\left(3 + 2x_2^{(k+1)} + x_3^{(k)} + x_4^{(k)}\\right) \\\\\nx_2^{(k+1)} &= (1 - \\omega)x_2^{(k)} + \\frac{\\omega}{10}\\left(15 + 2x_1^{(k+1)} + x_3^{(k)} + x_4^{(k)}\\right) \\\\\nx_3^{(k+1)} &= (1 - \\omega)x_3^{(k)} + \\frac{\\omega}{10}\\left(27 + x_1^{(k+1)} + x_2^{(k+1)} + 2x_4^{(k)}\\right) \\\\\nx_4^{(k+1)} &= (1 - \\omega)x_4^{(k)} + \\frac{\\omega}{10}\\left(-9 + x_1^{(k+1)} + x_2^{(k+1)} + 2x_3^{(k+1)}\\right)\n\\end{align*}\n\\]\nInitial Guess:\n\\[\nx_1^{(0)} = 0, \\quad x_2^{(0)} = 0, \\quad x_3^{(0)} = 0, \\quad x_4^{(0)} = 0\n\\]\nWe’ll use ($ = 1.25 $) as the relaxation factor (a common value).\n\nSolution\nIteration 1:\n\\[\nx_1^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(3 + 2(0) + 0 + 0) = 0 + 0.125(3) = 0.375\n\\] \\[\nx_2^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(15 + 2(0.375) + 0 + 0) = 1.96875\n\\] \\[\nx_3^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(27 + 0.375 + 1.96875 + 0) = 3.66796875\n\\] \\[\nx_4^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(-9 + 0.375 + 1.96875 + 2(3.66796875)) = 0.0849609375\n\\]\nIteration 1 Result:\n\\[\n\\begin{cases}\nx_1^{(1)} = 0.375 \\\\\nx_2^{(1)} = 1.96875 \\\\\nx_3^{(1)} = 3.66796875 \\\\\nx_4^{(1)} = 0.0849609375\n\\end{cases}\n\\]\nIteration 2:\n\\[\nx_1^{(2)} = (1 - 1.25)(0.375) + 0.125(3 + 2(1.96875) + 3.66796875 + 0.0849609375) = 1.2425537109375\n\\] \\[\nx_2^{(2)} = (1 - 1.25)(1.96875) + 0.125(15 + 2(1.2425537109375) + 3.66796875 + 0.0849609375) = 2.16244189453125\n\\] \\[\nx_3^{(2)} = (1 - 1.25)(3.66796875) + 0.125(27 + 1.2425537109375 + 2.16244189453125 + 2(0.0849609375)) = 2.9048724975585938\n\\] \\[\nx_4^{(2)} = (1 - 1.25)(0.0849609375) + 0.125(-9 + 1.2425537109375 + 2.16244189453125 + 2(2.9048724975585938)) = 0.005202293396\n\\]\nIteration 2 Result:\n\\[\n\\begin{cases}\nx_1^{(2)} ≈ 1.2426 \\\\\nx_2^{(2)} ≈ 2.1624 \\\\\nx_3^{(2)} ≈ 2.9049 \\\\\nx_4^{(2)} ≈ 0.0052\n\\end{cases}\n\\]\nIteration 3: \\[\nx_1^{(3)} = (1 - 1.25)(1.2425537109375) + 0.125(3 + 2(2.16244189453125) + 2.9048724975585938 + 0.005202293396) = 0.9688563947677\n\\] \\[\nx_2^{(3)} = (1 - 1.25)(2.16244189453125) + 0.125(15 + 2(0.9688563947677) + 2.9048724975585938 + 0.005202293396) = 1.9403629738789\n\\] \\[\nx_3^{(3)} = (1 - 1.25)(2.9048724975585938) + 0.125(27 + 0.9688563947677 + 1.9403629738789 + 2(0.005202293396)) = 3.0137349575401\n\\] \\[\nx_4^{(3)} = (1 - 1.25)(0.005202293396) + 0.125(-9 + 0.9688563947677 + 1.9403629738789 + 2(3.0137349575401)) = 0.1153360380873\n\\]\nIteration 3 Result: \\[\n\\begin{cases}\nx_1^{(3)} ≈ 0.9689 \\\\\nx_2^{(3)} ≈ 1.9404 \\\\\nx_3^{(3)} ≈ 3.0137 \\\\\nx_4^{(3)} ≈ 0.1153\n\\end{cases}\n\\]\nThis will converge eventually to approximately:\n\\[x \\approx (1, 2, 3, 1)^T\\]\nthis is how it will look in Python:\n\n\nCode\ndef sor(A, b, x0, omega, tol=1e-6, max_iter=100):\n    n = len(b)\n    x = x0.copy()\n    x_new = x0.copy()\n\n    for k in range(max_iter):\n        for i in range(n):\n            s1 = sum(A[i][j] * x_new[j] for j in range(i))\n            s2 = sum(A[i][j] * x[j] for j in range(i + 1, n))\n            x_new[i] = (1 - omega) * x[i] + omega * (b[i] - s1 - s2) / A[i][i]\n\n        if max(abs(x_new[i] - x[i]) for i in range(n)) &lt; tol:\n            return x_new, k + 1\n\n        x = x_new.copy()\n\n    return x_new, max_iter\n\nA = [[10, -2, -1, -1],\n     [-2, 10, -1, -1],\n     [-1, -1, 10, -2],\n     [-1, -1, -2, 10]]\n\nb = [3, 15, 27, -9]\nx0 = [0, 0, 0, 0]\nomega = 1.25\n\nsolution, iterations = sor(A, b, x0, omega)\nprint(\"Solution:\", solution)\nprint(\"Iterations:\", iterations)\n\n\nSolution: [1.0000000557875983, 2.000000137898985, 2.99999986169086, 3.934596124183633e-08]\nIterations: 13\n\n\n\n\n\nSummary\n\nIterative Refinement: SOR is an iterative technique used to solve systems of linear equations by progressively refining an initial guess until a solution is reached.\nRelaxation Parameter (ω): It introduces a relaxation parameter (ω) to accelerate convergence, where values between 1 and 2 (typically) are used to “over-correct” the solution at each iteration.\nSequential Updates: SOR updates each variable sequentially, using the most recently computed values of other variables within the same iteration, thus incorporating immediate feedback.\nEnhanced Convergence: By strategically applying the relaxation parameter, SOR aims to achieve faster convergence compared to the Gauss-Seidel method, particularly for certain types of matrices.",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#symmetric-successive-over-relaxation-ssor",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#symmetric-successive-over-relaxation-ssor",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "4. Symmetric Successive Over-Relaxation (SSOR)",
    "text": "4. Symmetric Successive Over-Relaxation (SSOR)\nThe Symmetric Successive Over-Relaxation (SSOR) method is an iterative technique used to solve systems of linear equations of the form:\n\\[\nAx = b\n\\]\nwhere \\(A\\) is a matrix, \\(x\\) is the unknown vector, and \\(b\\) is a known vector.\n\nMatrix Decomposition\nThe matrix \\(A\\) is decomposed into:\n\\[\nA = D + L + U\n\\]\nwhere: - \\(D\\) is the diagonal matrix. - \\(L\\) is the strictly lower triangular matrix. - \\(U\\) is the strictly upper triangular matrix.\n\n\nSSOR Iteration\nThe SSOR iteration consists of two steps: a forward SOR sweep and a backward SOR sweep. The forward SOR sweep is given by:\n\\[\nx^{(k+1/2)} = (D + \\omega L)^{-1} \\left[ (1 - \\omega) D - \\omega U \\right] x^{(k)} + \\omega (D + \\omega L)^{-1} b\n\\]\nThe backward SOR sweep is given by:\n\\[\nx^{(k+1)} = (D + \\omega U)^{-1} \\left[ (1 - \\omega) D - \\omega L \\right] x^{(k+1/2)} + \\omega (D + \\omega U)^{-1} b\n\\]\nwhere \\(\\omega\\) is the relaxation parameter.\n\n\nSSOR Iteration Matrix\nThe SSOR iteration matrix \\(S_\\omega\\) is given by:\n\\[\nS_\\omega = (D + \\omega U)^{-1} \\left[ (1 - \\omega) D - \\omega L \\right] (D + \\omega L)^{-1} \\left[ (1 - \\omega) D - \\omega U \\right]\n\\]\n\n\nPreconditioning\nSSOR is often used as a preconditioner for other iterative methods. The SSOR preconditioned system is:\n\\[\nM^{-1} A x = M^{-1} b\n\\]\nwhere \\(M\\) is the SSOR preconditioner, given by:\n\\[\nM = \\frac{1}{\\omega(2-\\omega)}(D+\\omega L)D^{-1}(D+\\omega U)\n\\]\n\n\nConvergence\nThe convergence of SSOR depends on the properties of the matrix \\(A\\) and the relaxation parameter \\(\\omega\\).\n\n\nExample\nUsing same example:\nGiven:\n\\[\nA = \\begin{pmatrix}\n10 & -2 & -1 & -1 \\\\\n-2 & 10 & -1 & -1 \\\\\n-1 & -1 & 10 & -2 \\\\\n-1 & -1 & -2 & 10\n\\end{pmatrix}, \\quad\n\\mathbf{b} = \\begin{pmatrix}\n3 \\\\ 15 \\\\ 27 \\\\ -9\n\\end{pmatrix}\n\\]\nRelaxation factor ($ = 1.25 $)\nInitial guess:\n\\[\n\\mathbf{x}^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\]\nGeneral formula for SOR:\n\\[\nx_i^{(k+1)} = (1 - \\omega)x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j &lt; i} a_{ij}x_j^{(k+1)} - \\sum_{j &gt; i} a_{ij}x_j^{(k)} \\right)\n\\]\nSSOR: 1. Forward sweep (1 → 4) 2. Backward sweep (4 → 1)\nSolution:\nIteration 1:\nForward Sweep:\n\\[\nx_1^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(3 + 2(0) + 0 + 0) = 0 + 0.125(3) = 0.375\n\\]\n\\[\nx_2^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(15 + 2(0.375) + 0 + 0) = 0 + 0.125(15 + 0.75) = 1.96875\n\\]\n\\[\nx_3^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(27 + 0.375 + 1.96875 + 0) = 0 + 0.125(29.34375) = 3.66796875\n\\]\n\\[\nx_4^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(-9 + 0.375 + 1.96875 + 2(3.66796875)) = 0 + 0.125(0.6796875) = 0.0849609375\n\\]\nBackward Sweep:\n\\[\nx_4^{(1)} = (1 - 1.25)(0.0849609375) + \\frac{1.25}{10}(-9 + 0.375 + 1.96875 + 2(3.66796875)) = 0.0849609375\n\\]\n\\[\nx_3^{(1)} = (1 - 1.25)(3.66796875) + \\frac{1.25}{10}(27 + 0.375 + 1.96875 + 2(0.0849609375)) = 2.764892578125\n\\]\n\\[\nx_2^{(1)} = (1 - 1.25)(1.96875) + \\frac{1.25}{10}(15 + 2(0.375) + 2.764892578125 + 0.0849609375) = 1.8377685546875\n\\]\n\\[\nx_1^{(1)} = (1 - 1.25)(0.375) + \\frac{1.25}{10}(3 + 2(1.8377685546875) + 2.764892578125 + 0.0849609375) = 1.096466064453125\n\\]\nIteration 1 Result:\n\\[\n\\begin{cases}\nx_1^{(1)} = 1.096466064453125 \\\\\nx_2^{(1)} = 1.8377685546875 \\\\\nx_3^{(1)} = 2.764892578125 \\\\\nx_4^{(1)} = 0.0849609375\n\\end{cases}\n\\]\nIteration 2:\nForward Sweep:\n\\[\nx_1^{(2)} = (1 - 1.25)(1.096466064453125) + \\frac{1.25}{10}(3 + 2(1.8377685546875) + 2.764892578125 + 0.0849609375) = 1.675684928894043\n\\]\n\\[\nx_2^{(2)} = (1 - 1.25)(1.8377685546875) + \\frac{1.25}{10}(15 + 2(1.675684928894043) + 2.764892578125 + 0.0849609375) = 2.466885447502136\n\\]\n\\[\nx_3^{(2)} = (1 - 1.25)(2.764892578125) + \\frac{1.25}{10}(27 + 1.675684928894043 + 2.466885447502136 + 2(0.0849609375)) = 3.241812765598297\n\\]\n\\[\nx_4^{(2)} = (1 - 1.25)(0.0849609375) + \\frac{1.25}{10}(-9 + 1.675684928894043 + 2.466885447502136 + 2(3.241812765598297)) = 0.9704861893057823\n\\]\nBackward Sweep:\n\\[\nx_4^{(2)} = 0.9704861893057823\n\\]\n\\[\nx_3^{(2)} = (1 - 1.25)(3.241812765598297) + \\frac{1.25}{10}(27 + 1.675684928894043 + 2.466885447502136 + 2(0.9704861893057823)) = 3.170768279492855\n\\]\n\\[\nx_2^{(2)} = (1 - 1.25)(2.466885447502136) + \\frac{1.25}{10}(15 + 2(1.675684928894043) + 3.170768279492855 + 0.9704861893057823) = 2.344833124428034\n\\]\n\\[\nx_1^{(2)} = (1 - 1.25)(1.675684928894043) + \\frac{1.25}{10}(3 + 2(2.344833124428034) + 3.170768279492855 + 0.9704861893057823) = 1.6019394582211971\n\\]\nIteration 2 Result:\n\\[\n\\begin{cases}\nx_1^{(2)} = 1.6019394582211971 \\\\\nx_2^{(2)} = 2.344833124428034 \\\\\nx_3^{(2)} = 3.170768279492855 \\\\\nx_4^{(2)} = 0.9704861893057823\n\\end{cases}\n\\]\nIteration 3:\nForward Sweep:\n\\[\nx_1^{(3)} = (1 - 1.25)(1.6019394582211971) + \\frac{1.25}{10}(3 + 2(2.344833124428034) + 3.170768279492855 + 0.9704861893057823) = 1.6020043636265095\n\\]\n\\[\nx_2^{(3)} = (1 - 1.25)(2.344833124428034) + \\frac{1.25}{10}(15 + 2(1.6020043636265095) + 3.170768279492855 + 0.9704861893057823) = 2.3439992931822126\n\\]\n\\[\nx_3^{(3)} = (1 - 1.25)(3.170768279492855) + \\frac{1.25}{10}(27 + 1.6020043636265095 + 2.3439992931822126 + 2(0.9704861893057823)) = 3.1700007750166693\n\\]\n\\[\nx_4^{(3)} = (1 - 1.25)(0.9704861893057823) + \\frac{1.25}{10}(-9 + 1.6020043636265095 + 2.3439992931822126 + 2(3.1700007750166693)) = 0.9600021139037759\n\\]\nBackward Sweep:\n\\[\nx_4^{(3)} = 0.9600021139037759\n\\]\n\\[\nx_3^{(3)} = (1 - 1.25)(3.1700007750166693) + \\frac{1.25}{10}(27 + 1.6020043636265095 + 2.3439992931822126 + 2(0.9600021139037759)) = 3.170000002953419\n\\]\n\\[\nx_2^{(3)} = (1 - 1.25)(2.3439992931822126) + \\frac{1.25}{10}(15 + 2(1.6020043636265095) + 3.170000002953419 + 0.9600021139037759) = 2.3440000004163684\n\\]\n\\[\nx_1^{(3)} = (1 - 1.25)(1.6020043636265095) + \\frac{1.25}{10}(3 + 2(2.3440000004163684) + 3.170000002953419 + 0.9600021139037759) = 1.6020000000535157\n\\]\nIteration 3 Result:\n\\[\n\\begin{cases}\nx_1^{(3)} = 1.6020000000535157 \\\\\nx_2^{(3)} = 2.3440000004163684 \\\\\nx_3^{(3)} = 3.170000002953419 \\\\\nx_4^{(3)} = 0.9600021139037759\n\\end{cases}\n\\]\nWhich will converge eventually to: \\[x \\approx (1, 2, 3, 1)^T\\]\nWe will code this in Python and check how many iterations it takes to converge:\nPython Code\n\n\nCode\n# Define matrix A and vector b\nA = np.array([\n    [10, -2, -1, -1],\n    [-2, 10, -1, -1],\n    [-1, -1, 10, -2],\n    [-1, -1, -2, 10]\n], dtype=float)\n\nb = np.array([3, 15, 27, -9], dtype=float)\n\n# Parameters\nomega = 1.25  # Relaxation factor (can tweak between 1 &lt; omega &lt; 2 for convergence)\ntol = 1e-6   # Convergence tolerance\nmax_iterations = 1000\n\n# Initial guess\nx = np.zeros(len(b))\n\n# Precompute matrices\nD = np.diag(np.diag(A))\nL = np.tril(A, -1)\nU = np.triu(A, 1)\n\n# SSOR iterative process\nfor iteration in range(max_iterations):\n    x_old = x.copy()\n\n    # Forward SOR\n    for i in range(len(A)):\n        sum1 = np.dot(A[i, :i], x[:i])\n        sum2 = np.dot(A[i, i+1:], x_old[i+1:])\n        x[i] = (1 - omega) * x_old[i] + (omega / A[i, i]) * (b[i] - sum1 - sum2)\n\n    # Backward SOR\n    for i in reversed(range(len(A))):\n        sum1 = np.dot(A[i, :i], x[:i])\n        sum2 = np.dot(A[i, i+1:], x[i+1:])\n        x[i] = (1 - omega) * x[i] + (omega / A[i, i]) * (b[i] - sum1 - sum2)\n\n    # Check convergence\n    if np.linalg.norm(x - x_old, ord=np.inf) &lt; tol:\n        print(f\"Converged in {iteration + 1} iterations\")\n        break\nelse:\n    print(\"Did not converge within the maximum number of iterations\")\n\nprint(\"Solution:\", x)\n\n\nConverged in 9 iterations\nSolution: [ 1.00000001e+00  1.99999998e+00  2.99999999e+00 -1.16701435e-08]",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#accelerated-over-relaxation-aor",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#accelerated-over-relaxation-aor",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "5. Accelerated Over-Relaxation (AOR)",
    "text": "5. Accelerated Over-Relaxation (AOR)\n\nGeneralizes SOR by using two parameters for additional flexibility.\n\nThe Accelerated Over-Relaxation (AOR) method is an iterative technique used to solve linear systems of equations \\(Ax = b\\) by introducing two relaxation parameters, \\(\\omega\\) and \\(\\gamma\\), to accelerate convergence beyond the Successive Over-Relaxation (SOR) method.\n\nFormula\nGiven the decomposition \\(A = D - L - U\\), where \\(D\\) is diagonal, \\(L\\) is lower triangular, and \\(U\\) is upper triangular, the AOR iteration is:\n\\[x^{(k+1)} = (D - \\omega \\gamma L)^{-1} [(\\omega \\gamma U + (1 - \\omega \\gamma) D) x^{(k)} + \\omega b]\\]\nAlternatively:\n\\[x^{(k+1)} = x^{(k)} + \\omega (D - \\gamma L)^{-1} (b - Ax^{(k)})\\]\nwhere \\(\\omega\\) and \\(\\gamma\\) are relaxation parameters.\n\n\nAlgorithm\n\nInitialize \\(x^{(0)}\\).\nChoose relaxation parameters \\(\\omega\\) and \\(\\gamma\\).\nIterate for \\(k = 0, 1, 2, \\dots\\) until convergence:\n\nUpdate:\n\\[\nx^{(k+1)} = (D - \\omega \\gamma L)^{-1} \\left[ (\\omega \\gamma U + (1 - \\omega \\gamma) D) x^{(k)} + \\omega b \\right]\n\\]\nCheck convergence criterion:\n\\[\n\\| x^{(k+1)} - x^{(k)} \\| &lt; \\text{tolerance}\n\\]\n\nReturn \\(x^{(k+1)}\\).\n\n\n\nConvergence\nThe AOR method converges if and only if the spectral radius of the iteration matrix \\(T_{\\omega, \\gamma}\\) is less than 1:\n\\[\\rho(T_{\\omega, \\gamma}) &lt; 1\\]\nwhere \\(T_{\\omega, \\gamma} = (D - \\omega \\gamma L)^{-1} (\\omega \\gamma U + (1 - \\omega \\gamma) D)\\). The optimal values of \\(\\omega\\) and \\(\\gamma\\) are problem-dependent.\n\n\nAdvantages and Disadvantages\n\nAdvantages\n\nPotentially faster convergence than Gauss-Seidel and SOR with optimal \\(\\omega\\) and \\(\\gamma\\).\nIncreased flexibility with two relaxation parameters.\n\n\n\nDisadvantages\n\nOptimal \\(\\omega\\) and \\(\\gamma\\) are difficult to determine.\nConvergence is highly sensitive to the choice of \\(\\omega\\) and \\(\\gamma\\).\nHigher computational cost per iteration compared to Gauss-Seidel.\n\n\n\n\nExample\nUsing the same Example but using AOR to solve the problem\nSolution:\nStep 1: Write the system in matrix form\n\\[\n\\begin{bmatrix}\n10 & -2 & -1 & -1 \\\\\n-2 & 10 & -1 & -1 \\\\\n-1 & -1 & 10 & -2 \\\\\n-1 & -1 & -2 & 10\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 \\\\ 15 \\\\ 27 \\\\ -9\n\\end{bmatrix}\n\\]\nDenote: - Coefficient matrix: ($ A $) - Solution vector: ( $ = [x_1, x_2, x_3, x_4]^T $) - RHS vector: ( $ = [3, 15, 27, -9]^T $)\nStep 2: Split matrix A\nSplit ($ A = D - L - U \\(\\), where:\n- \\(\\) D \\(\\) is diagonal,\n- \\(\\) L \\(\\) is strictly lower triangular,\n- \\(\\) U $) is strictly upper triangular.\n\\[\nD =\n\\begin{bmatrix}\n10 & 0 & 0 & 0 \\\\\n0 & 10 & 0 & 0 \\\\\n0 & 0 & 10 & 0 \\\\\n0 & 0 & 0 & 10\n\\end{bmatrix}, \\quad\nL =\n\\begin{bmatrix}\n0 & 0 & 0 & 0 \\\\\n2 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 1 & 2 & 0\n\\end{bmatrix}, \\quad\nU =\n\\begin{bmatrix}\n0 & 2 & 1 & 1 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 0 & 2 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\n(Note: Negative signs absorbed accordingly.)\nStep 3: AOR formula\nThe AOR iteration formula is:\n\\[\n\\mathbf{x}^{(k+1)} = (D - \\omega L)^{-1} \\left[ (1 - \\omega) D + \\omega U \\right] \\mathbf{x}^{(k)} + \\omega (D - \\omega L)^{-1} \\mathbf{b}\n\\]\nWhere: - ($ \\(\\) = relaxation factor (commonly \\(\\) 1 &lt; &lt; 2 \\(\\))\n- Start with an initial guess \\(\\) ^{(0)} = [0, 0, 0, 0]^{T} $)\nStep 4: Choose parameters\nFor SOR (Successive Over-Relaxation), ($ = 1.25 \\(\\) is often a good choice, but for **AOR**, there is also a parameter \\(\\) $). AOR generalizes SOR and Gauss-Seidel.\nHowever, AOR is usually applied in a modified form: \\[\n\\mathbf{x}^{(k+1)} = (1 - \\lambda) \\mathbf{x}^{(k)} + \\lambda \\cdot \\mathbf{T} \\mathbf{x}^{(k)} + \\mathbf{C}\n\\]\nTo keep it manageable, let’s use SOR (as a special case of AOR with ($ = 1 $)).\nStep 5: Iterative process\nSOR formula for each variable: \\[\nx_i^{(k+1)} = (1 - \\omega) x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j &lt; i} a_{ij} x_j^{(k+1)} - \\sum_{j &gt; i} a_{ij} x_j^{(k)} \\right)\n\\]\nIteration 1:\n\\[\nx_1^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(3 + 2(0) + 0 + 0) = 0 + 0.125(3) = 0.375\n\\]\n\\[\nx_2^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(15 + 2(0.375) + 0 + 0) = 0 + 0.125(15 + 0.75) = 1.96875\n\\]\n\\[\nx_3^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(27 + 0.375 + 1.96875 + 0) = 0 + 0.125(29.34375) = 3.66796875\n\\]\n\\[\nx_4^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(-9 + 0.375 + 1.96875 + 2(3.66796875)) = 0 + 0.125(0.6796875) = 0.0849609375\n\\]\nIteration 1 Result:\n\\[\n\\begin{cases}\nx_1^{(1)} = 0.375 \\\\\nx_2^{(1)} = 1.96875 \\\\\nx_3^{(1)} = 3.66796875 \\\\\nx_4^{(1)} = 0.0849609375\n\\end{cases}\n\\]\nIteration 2:\n\\[\nx_1^{(2)} = (1 - 1.25)(0.375) + \\frac{1.25}{10}(3 + 2(1.96875) + 1(3.66796875) + 1(0.0849609375)) = 1.242553710938\n\\]\n\\[\nx_2^{(2)} = (1 - 1.25)(1.96875) + \\frac{1.25}{10}(15 + 2(1.242553710938) + 1(3.66796875) + 1(0.0849609375)) = 2.162561035157\n\\]\n\\[\nx_3^{(2)} = (1 - 1.25)(3.66796875) + \\frac{1.25}{10}(27 + 1(1.242553710938) + 1(2.162561035157) + 2(0.0849609375)) = 3.902415527387\n\\]\n\\[\nx_4^{(2)} = (1 - 1.25)(0.0849609375) + \\frac{1.25}{10}(-9 + 1(1.242553710938) + 1(2.162561035157) + 2(3.902415527387)) = 0.255702941884\n\\]\nIteration 2 Result:\n\\[\n\\begin{cases}\nx_1^{(2)} = 1.242553710938 \\\\\nx_2^{(2)} = 2.162561035157 \\\\\nx_3^{(2)} = 3.902415527387 \\\\\nx_4^{(2)} = 0.255702941884\n\\end{cases}\n\\]",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#iteration-3",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#iteration-3",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "Iteration 3:",
    "text": "Iteration 3:\n\\[\nx_1^{(3)} = (1 - 1.25)(1.242553710938) + \\frac{1.25}{10}(3 + 2(2.162561035157) + 1(3.902415527387) + 1(0.255702941884)) = 1.124766643465\n\\]\n\\[\nx_2^{(3)} = (1 - 1.25)(2.162561035157) + \\frac{1.25}{10}(15 + 2(1.124766643465) + 1(3.902415527387) + 1(0.255702941884)) = 2.136316710236\n\\]\n\\[\nx_3^{(3)} = (1 - 1.25)(3.902415527387) + \\frac{1.25}{10}(27 + 1(1.124766643465) + 1(2.136316710236) + 2(0.255702941884)) = 2.896957722992\n\\]\n\\[\nx_4^{(3)} = (1 - 1.25)(0.255702941884) + \\frac{1.25}{10}(-9 + 1(1.124766643465) + 1(2.136316710236) + 2(2.896957722992)) = -0.05795038548\n\\]\nIteration 3 Result:\n\\[\n\\begin{cases}\nx_1^{(3)} = 1.124766643465 \\\\\nx_2^{(3)} = 2.136316710236 \\\\\nx_3^{(3)} = 2.896957722992 \\\\\nx_4^{(3)} = -0.05795038548\n\\end{cases}\n\\] This, same as above methods, will converge eventually to approximately:\n\\[x \\approx (1, 2, 3, 1)^T\\]\nWe will code this in Python and check how many iterations it takes to converge:\nPython Code\n\n\nCode\n# Coefficient matrix\nA = np.array([\n    [10, -2, -1, -1],\n    [-2, 10, -1, -1],\n    [-1, -1, 10, -2],\n    [-1, -1, -2, 10]\n], dtype=float)\n\n# Right-hand side vector\nb = np.array([3, 15, 27, -9], dtype=float)\n\n# Initial guess\nx = np.zeros(4)\n\n# Parameters\nomega = 1.25  # Relaxation factor\ntol = 1e-6    # Tolerance\nmax_iterations = 100\n\n# Iteration\nfor iteration in range(max_iterations):\n    x_new = np.copy(x)\n    for i in range(4):\n        sigma = 0\n        for j in range(4):\n            if j != i:\n                sigma += A[i, j] * x_new[j] if j &lt; i else A[i, j] * x[j]\n        x_new[i] = (1 - omega) * x[i] + (omega / A[i, i]) * (b[i] - sigma)\n    \n    # Check convergence\n    if np.linalg.norm(x_new - x, np.inf) &lt; tol:\n        break\n    x = x_new\n\nprint(f\"Solution after {iteration+1} iterations:\")\nprint(x)\n\n\nSolution after 13 iterations:\n[ 9.99999533e-01  1.99999963e+00  3.00000045e+00 -1.98849693e-07]",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#chebyshev-semi-iterative-method",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#chebyshev-semi-iterative-method",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "6. Chebyshev Semi-Iterative Method",
    "text": "6. Chebyshev Semi-Iterative Method\nThe Chebyshev semi-iterative method is an iterative method used to solve linear systems of equations of the form \\(Ax = b\\), where \\(A\\) is a symmetric positive definite matrix. It is a variant of the Richardson method that utilizes Chebyshev polynomials to accelerate convergence. It is particularly effective when the eigenvalues of \\(A\\) are clustered, leading to faster convergence compared to simple iterative methods.\n\nFormula\nThe Chebyshev semi-iterative method generates a sequence of approximations \\(x_k\\) to the solution \\(x\\) using the following recursive formula:\n\\[x_{k+1} = \\omega_{k+1} \\left( \\alpha_k (b - Ax_k) + x_k - x_{k-1} \\right) + x_{k-1}\\]\nwhere \\(\\omega_{k+1}\\) and \\(\\alpha_k\\) are parameters derived from the Chebyshev polynomials and the spectral radius of the iteration matrix.\nMore precisely, the iteration can be expressed as:\n\\[x_{k+1} = \\rho_k \\left( \\frac{r_k}{\\sigma} \\right) + (1-\\rho_k)x_{k-1}\\]\nwhere \\(r_k = b - Ax_k\\) is the residual, \\(\\sigma = \\frac{\\lambda_{max} + \\lambda_{min}}{2}\\), and \\(\\rho_k\\) is computed using Chebyshev polynomials:\n\\[\\rho_1 = 1, \\quad \\rho_2 = \\frac{2\\sigma^2}{\\lambda_{max}^2+\\lambda_{min}^2}, \\quad \\rho_{k+1} = \\frac{1}{1 - \\frac{\\rho_k(\\lambda_{max} - \\lambda_{min})^2}{4\\sigma^2}}\\]\nwhere \\(\\lambda_{min}\\) and \\(\\lambda_{max}\\) are the smallest and largest eigenvalues of \\(A\\), respectively.\n\n\nAlgorithm\n\nInitialization:\n\nChoose an initial guess \\(x_0\\).\nChoose \\(x_1\\) (often \\(x_1 = x_0 + \\alpha_0 (b-Ax_0)\\)).\nCompute \\(\\lambda_{min}\\) and \\(\\lambda_{max}\\) (or estimates).\nCompute \\(\\sigma = \\frac{\\lambda_{max} + \\lambda_{min}}{2}\\).\nCompute \\(\\rho_1 = 1\\) and \\(\\rho_2 = \\frac{2\\sigma^2}{\\lambda_{max}^2+\\lambda_{min}^2}\\).\n\nIteration:\n\nFor \\(k = 1, 2, \\dots\\) until convergence:\n\nCompute the residual \\(r_k = b - Ax_k\\).\nCompute \\(x_{k+1} = \\rho_k \\left( \\frac{r_k}{\\sigma} \\right) + (1-\\rho_k)x_{k-1}\\).\nCompute \\(\\rho_{k+1} = \\frac{1}{1 - \\frac{\\rho_k(\\lambda_{max} - \\lambda_{min})^2}{4\\sigma^2}}\\).\nCheck for convergence (e.g., \\(\\|r_k\\| &lt; \\text{tolerance}\\)).\n\n\nOutput:\n\nReturn the approximate solution \\(x_{k+1}\\).\n\n\n\n\nConvergence\nThe Chebyshev semi-iterative method converges if \\(A\\) is a symmetric positive definite matrix. The convergence rate depends on the condition number of \\(A\\), defined as \\(\\kappa(A) = \\frac{\\lambda_{max}}{\\lambda_{min}}\\).\nThe convergence rate is roughly proportional to \\(\\sqrt{\\kappa(A)}\\), which is better than the linear convergence rate of simple iterative methods like the Jacobi or Gauss-Seidel methods.\nThe convergence is guaranteed if the eigenvalues of \\(A\\) are real and positive. The method’s effectiveness is enhanced when the eigenvalues are clustered, leading to a smaller condition number and faster convergence.\n\n\nAdvantages and Disadvantages\nAdvantages:\n\nFaster Convergence: Compared to basic iterative methods, it often converges much faster, especially for well-conditioned systems.\nEffective for Clustered Eigenvalues: Performs well when the eigenvalues of \\(A\\) are clustered.\nPredictable Convergence: The convergence rate can be estimated based on the eigenvalues of \\(A\\).\n\nDisadvantages:\n\nRequires Eigenvalue Estimates: Requires estimates of the largest and smallest eigenvalues of \\(A\\), which can be computationally expensive to obtain.\nSensitivity to Eigenvalue Estimates: The convergence rate is sensitive to the accuracy of the eigenvalue estimates. Poor estimates can lead to slow or even divergent behavior.\nNot Applicable to General Matrices: It is primarily designed for symmetric positive definite matrices; it may not converge for general matrices.\nMore Complex Implementation: The recursive formula is more complex than simple iterative methods.\n\n\n\nExample\nusing the same given System:\n\\[\n\\begin{align*}\n10x_1 - 2x_2 - x_3 - x_4 &= 3 \\quad (1) \\\\\n-2x_1 + 10x_2 - x_3 - x_4 &= 15 \\quad (2) \\\\\n-x_1 - x_2 + 10x_3 - 2x_4 &= 27 \\quad (3) \\\\\n-x_1 - x_2 - 2x_3 + 10x_4 &= -9 \\quad (4)\n\\end{align*}\n\\]\nStep 1: Rewrite the system in matrix form:\n\\[\nA = \\begin{pmatrix}\n10 & -2 & -1 & -1 \\\\\n-2 & 10 & -1 & -1 \\\\\n-1 & -1 & 10 & -2 \\\\\n-1 & -1 & -2 & 10\n\\end{pmatrix},\n\\quad\n\\mathbf{b} = \\begin{pmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{pmatrix}\n\\]\nStep 2: Initialize:\nLet’s take:\n\\[\n\\mathbf{x}^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\]\nStep 3: Chebyshev Semi-Iterative Formula: \nFor the first iteration, it simplifies to the Jacobi method:\n\\[\nx_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)} \\right)\n\\]\nIterations\nIteration 1\n\\[\nx_1^{(1)} = \\frac{1}{10} \\left( 3 - (-2)(0) - (-1)(0) - (-1)(0) \\right) = \\frac{3}{10} = 0.3\n\\] \\[\nx_2^{(1)} = \\frac{1}{10} \\left( 15 - (-2)(0) - (-1)(0) - (-1)(0) \\right) = \\frac{15}{10} = 1.5\n\\] \\[\nx_3^{(1)} = \\frac{1}{10} \\left( 27 - (-1)(0) - (-1)(0) - (-2)(0) \\right) = \\frac{27}{10} = 2.7\n\\] \\[\nx_4^{(1)} = \\frac{1}{10} \\left( -9 - (-1)(0) - (-1)(0) - (-2)(0) \\right) = \\frac{-9}{10} = -0.9\n\\]\nIteration 1 Result:\n\\[\n\\begin{cases}\nx_1^{(1)} = 0.3 \\\\\nx_2^{(1)} = 1.5 \\\\\nx_3^{(1)} = 2.7 \\\\\nx_4^{(1)} = -0.9\n\\end{cases}\n\\]\nIteration 2\n[ \\[\\begin{align*}\nx_1^{(2)} &= \\frac{1}{10} \\left( 3 - (-2)(1.5) - (-1)(2.7) - (-1)(-0.9) \\right) \\\\\n&= \\frac{1}{10} \\left( 3 + 3 + 2.7 - 0.9 \\right) \\\\\n&= \\frac{1}{10} (7.8) \\\\\n&= 0.78\n\\end{align*}\\] ] [ \\[\\begin{align*}\nx_2^{(2)} &= \\frac{1}{10} \\left( 15 - (-2)(0.3) - (-1)(2.7) - (-1)(-0.9) \\right) \\\\\n&= \\frac{1}{10} \\left( 15 + 0.6 + 2.7 - 0.9 \\right) \\\\\n&= \\frac{1}{10} (17.4) \\\\\n&= 1.74\n\\end{align*}\\] ] [ \\[\\begin{align*}\nx_3^{(2)} &= \\frac{1}{10} \\left( 27 - (-1)(0.3) - (-1)(1.5) - (-2)(-0.9) \\right) \\\\\n&= \\frac{1}{10} \\left( 27 + 0.3 + 1.5 - 1.8 \\right) \\\\\n&= \\frac{1}{10} (27.0) \\\\\n&= 2.7\n\\end{align*}\\] ] [ \\[\\begin{align*}\nx_4^{(2)} &= \\frac{1}{10} \\left( -9 - (-1)(0.3) - (-1)(1.5) - (-2)(2.7) \\right) \\\\\n&= \\frac{1}{10} \\left( -9 + 0.3 + 1.5 + 5.4 \\right) \\\\\n&= \\frac{1}{10} (-1.8) \\\\\n&= -0.18\n\\end{align*}\\] ]\nIteration 2 Result:\n[\n\\[\\begin{cases}\nx_1^{(2)} = 0.78 \\\\\nx_2^{(2)} = 1.74 \\\\\nx_3^{(2)} = 2.7 \\\\\nx_4^{(2)} = -0.18\n\\end{cases}\\]\n]",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#iteration-3-1",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#iteration-3-1",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "Iteration 3",
    "text": "Iteration 3\nNow, use the results from Iteration 2:\n[ \\[\\begin{align*}\nx_1^{(3)} &= \\frac{1}{10} \\left( 3 - (-2)(1.74) - (-1)(2.7) - (-1)(-0.18) \\right) \\\\\n&= \\frac{1}{10} \\left( 3 + 3.48 + 2.7 - 0.18 \\right) \\\\\n&= \\frac{1}{10} (9.0) \\\\\n&= 0.9\n\\end{align*}\\] ] [ \\[\\begin{align*}\nx_2^{(3)} &= \\frac{1}{10} \\left( 15 - (-2)(0.78) - (-1)(2.7) - (-1)(-0.18) \\right) \\\\\n&= \\frac{1}{10} \\left( 15 + 1.56 + 2.7 - 0.18 \\right) \\\\\n&= \\frac{1}{10} (19.08) \\\\\n&= 1.908\n\\end{align*}\\] ] [ \\[\\begin{align*}\nx_3^{(3)} &= \\frac{1}{10} \\left( 27 - (-1)(0.78) - (-1)(1.74) - (-2)(-0.18) \\right) \\\\\n&= \\frac{1}{10} \\left( 27 + 0.78 + 1.74 - 0.36 \\right) \\\\\n&= \\frac{1}{10} (29.16) \\\\\n&= 2.916\n\\end{align*}\\] ] [ \\[\\begin{align*}\nx_4^{(3)} &= \\frac{1}{10} \\left( -9 - (-1)(0.78) - (-1)(1.74) - (-2)(2.7) \\right) \\\\\n&= \\frac{1}{10} \\left( -9 + 0.78 + 1.74 + 5.4 \\right) \\\\\n&= \\frac{1}{10} (-1.08) \\\\\n&= -0.108\n\\end{align*}\\] ]\nIteration 3 Result:\n[\n\\[\\begin{cases}\nx_1^{(3)} = 0.9 \\\\\nx_2^{(3)} = 1.908 \\\\\nx_3^{(3)} = 2.916 \\\\\nx_4^{(3)} = -0.108\n\\end{cases}\\]\n]",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#summary-of-all-iterations",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#summary-of-all-iterations",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "Summary of All Iterations:",
    "text": "Summary of All Iterations:\n\n\n\nIteration\n(x_1)\n(x_2)\n(x_3)\n(x_4)\n\n\n\n\n1\n0.3\n1.5\n2.7\n-0.9\n\n\n2\n0.78\n1.74\n2.7\n-0.18\n\n\n3\n0.9\n1.908\n2.916\n-0.108\n\n\n\nHere is the python code, which I translated from MATLAB code shown in wikipedia, with revision where it only shows the converged solution and maximum iteration:\n\n\nCode\n\ndef SolChebyshev(A, b, x0, iter_num, l_max, l_min, tol=1e-6):\n    d = (l_max + l_min) / 2\n    c = (l_max - l_min) / 2\n    pre_cond = np.eye(A.shape[0])\n    x = x0.copy()\n    r = b - A @ x\n\n    for i in range(1, iter_num + 1):\n        z = np.linalg.solve(pre_cond, r)\n\n        if i == 1:\n            p = z\n            alpha = 1 / d\n        elif i == 2:\n            beta = 0.5 * (c * alpha) ** 2\n            alpha = 1 / (d - beta / alpha)\n            p = z + beta * p\n        else:\n            beta = (c * alpha / 2) ** 2\n            alpha = 1 / (d - beta / alpha)\n            p = z + beta * p\n\n        x = x + alpha * p\n        r = b - A @ x\n\n        if np.linalg.norm(r) &lt; tol:\n            break\n\n    return x, i  # You can print externally\n\n\nA = np.array([\n    [10, -2, -1, -1],\n    [-2, 10, -1, -1],\n    [-1, -1, 10, -2],\n    [-1, -1, -2, 10]\n], dtype=float)\n\nb = np.array([3, 15, 27, -9], dtype=float)\nx0 = np.zeros_like(b)\n\nx_sol, num_iters = SolChebyshev(A, b, x0, iter_num=100, l_max=12, l_min=6) # change l_max and l_min for different eigenvalues\nprint(f\"Solution: {x_sol}, Iterations: {num_iters}\")\n\neg = np.linalg.eigvals(A) \n#print(f\"Eigenvalues: {eg}\") # calculate the actual eigenvalues of the matrix for l_max and l_min\n\n\nSolution: [ 9.99999985e-01  1.99999999e+00  3.00000000e+00 -2.27554890e-08], Iterations: 11",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#richardson-iteration",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#richardson-iteration",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "7. Richardson Iteration",
    "text": "7. Richardson Iteration\nThe Richardson iteration is a basic iterative method used to solve systems of linear equations of the form \\(Ax = b\\), where \\(A\\) is a known matrix, \\(b\\) is a known vector, and \\(x\\) is the unknown vector we aim to find. It refines an initial guess of the solution with each iteration by using the residual.\n\nFormula:\n\nThe core formula for the Richardson iteration is: \\[x^{(k+1)} = x^{(k)} + \\alpha (b - Ax^{(k)})\\]\n\nWhere:\n\n\\(x^{(k)}\\) is the approximation of the solution at the k-th iteration.\n\\(\\alpha\\) is a scalar parameter (the step size or relaxation parameter) that influences convergence.\n\\(b - Ax^{(k)}\\) is the residual vector, representing the error at the k-th iteration.\n\n\n\n\n\nAlgorithm:\n\nInitialization:\n\nChoose an initial guess for the solution, \\(x^{(0)}\\).\nChoose a suitable value for the parameter \\(\\alpha\\).\nset k = 0.\n\nIteration:\n\nCalculate the residual: \\(r^{(k)} = b - Ax^{(k)}\\).\nUpdate the solution: \\(x^{(k+1)} = x^{(k)} + \\alpha r^{(k)}\\).\nincrement k, k = k+1.\n\nConvergence Check:\n\nCheck if the residual \\(r^{(k)}\\) or the difference between successive solutions (\\(x^{(k+1)} - x^{(k)}\\)) is below a specified tolerance.\nIf the convergence criterion is met, stop. Otherwise, return to step 2.\n\n\n\n\nConvergence:\n\nThe convergence of the Richardson iteration is highly dependent on the choice of \\(\\alpha\\) and the properties of the matrix A.\nFor convergence, the eigenvalues of the matrix \\((I - \\alpha A)\\) must lie within the unit circle.\nFinding the optimal \\(\\alpha\\) can be challenging and often requires knowledge of the spectral properties of A.\nIf the value of alpha is poorly chosen, the method can diverge, or converge very slowly.\nThe matrix A must be such that the iteration will converge.\n\n\n\nAdvantages and Disadvantages:\n\nAdvantages:\n\nSimplicity: The algorithm is relatively easy to understand and implement.\nBasic foundation: It provides a fundamental understanding of iterative methods for solving linear systems.\n\nDisadvantages:\n\nConvergence sensitivity: The convergence is highly sensitive to the choice of \\(\\alpha\\), making it difficult to use in many practical situations.\nSlow convergence: It can converge very slowly, especially for ill-conditioned matrices.\nLimited applicability: It is not as efficient as other iterative methods, such as conjugate gradient or GMRES, for general linear systems.\nFinding the optimal alpha value can be very difficult.\n\n\n\n\nExample:\n\\[\n\\begin{aligned}\n10x_1 - 2x_2 - x_3 - x_4 &= 3, \\\\\n-2x_1 + 10x_2 - x_3 - x_4 &= 15, \\\\\n- x_1 - x_2 + 10x_3 - 2x_4 &= 27, \\\\\n- x_1 - x_2 - 2x_3 + 10x_4 &= -9.\n\\end{aligned}\n\\]\nTo solve this same system using the Richardson iteration method, we first express it in matrix form:\n\\[\nA \\mathbf{x} = \\mathbf{b}\n\\]\nwhere:\n\\[\nA = \\begin{bmatrix} 10 & -2 & -1 & -1 \\\\ -2 & 10 & -1 & -1 \\\\ -1 & -1 & 10 & -2 \\\\ -1 & -1 & -2 & 10 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{bmatrix}\n\\]\nFormula:\n\\[\n\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha (\\mathbf{b} - A \\mathbf{x}^{(k)})\n\\]\nwhere ($ $) is a relaxation parameter.\n**Choosing ($ \\(\\)**&lt;br&gt;\nA good choice for \\(\\) $) is:\n\\[\n\\alpha = \\frac{2}{\\lambda_{\\max} + \\lambda_{\\min}}\n\\]\nwhere ($ {} \\(\\) and \\(\\) {} \\(\\) are the largest and smallest eigenvalues of \\(\\) A \\(\\).\n\\\nI estimated \\(\\) {} \\(\\), \\(\\) {} $), leading to: \\[\n    \\alpha = \\frac{2}{12 + 6} = 0.111\n    \\]\nThough I will $ = 0.1$ for ease of typing the solution, but if you want an exact solution use $ = 0.111$\nHere are the first three iterations of the Richardson method:\n\nInitial guess:\n\\[\nx^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n\\]\nFirst iteration:\n\nCompute ($ r^{(0)} = b - A x^{(0)} \\(\\):\\)$ r^{(0)} =\n\\[\\begin{bmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{bmatrix}\\]\n$$\nUpdate ( x^{(1)} ): \\[\nx^{(1)} = x^{(0)} + 0.1 \\cdot r^{(0)}\n\\] \\[\nx^{(1)} = (0,0,0,0) + 0.1 \\times \\begin{bmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{bmatrix}\n\\] \\[\nx^{(1)} = (0.3, 1.5, 2.7, -0.9).\n\\]\n\n\nIteration 1 Result: \\[\n\\begin{cases}\nx_1^{(1)} = 0.3 \\\\\nx_2^{(1)} = 1.5 \\\\\nx_3^{(1)} = 2.7 \\\\\nx_4^{(1)} = -0.9\n\\end{cases}\n\\]\n\nSecond iteration:\n\nCompute ($ r^{(1)} = b - A x^{(1)} \\(\\):\\)$ r^{(1)} =\n\\[\\begin{bmatrix} 4.8 \\\\ 2.4 \\\\ 0 \\\\ 7.2 \\end{bmatrix}\\]\n$$\nUpdate ( x^{(2)} ): \\[\nx^{(2)} = x^{(1)} + 0.1 \\cdot r^{(1)}\n\\] \\[\nx^{(2)} = (0.3, 1.5, 2.7, -0.9) + 0.1 \\times \\begin{bmatrix} 4.8 \\\\ 2.4 \\\\ 0 \\\\ 7.2 \\end{bmatrix}\n\\] \\[\nx^{(2)} = (0.78, 1.74, 2.7, -0.18).\n\\] Iteration 2 Result: \\[\n\\begin{cases}\nx_1^{(2)} = 0.78 \\\\\nx_2^{(2)} = 1.74 \\\\\nx_3^{(2)} = 2.7 \\\\\nx_4^{(2)} = -0.18\n\\end{cases}\n\\]\n\nThird iteration:\nCompute ($ r^{(2)} = b - A x^{(2)} \\(\\):\\)$ r^{(2)} =\n\\[\\begin{bmatrix} 1.2 \\\\ 1.68 \\\\ 2.16 \\\\ 0.72 \\end{bmatrix}\\]\n$$\nUpdate ($ x^{(3)} \\(\\):\\)$ x^{(3)} = x^{(2)} + 0.1 r^{(2)} \\[\n\\] x^{(3)} = (0.78, 1.74, 2.7, -0.18) + 0.1 \n\\[\\begin{bmatrix} 1.2 \\\\ 1.68 \\\\ 2.16 \\\\ 0.72 \\end{bmatrix}\\]\n\\[\n\\] x^{(3)} = (0.9, 1.908, 2.916, -0.108). $$\n\nIteration 3 Result: \\[\n\\begin{cases}\nx_1^{(3)} = 0.9 \\\\\nx_2^{(3)} = 1.908 \\\\\nx_3^{(3)} = 2.916 \\\\\nx_4^{(3)} = -0.108\n\\end{cases}\n\\]\n\n\nCode\nA = np.array([[10, -2, -1, -1],\n              [-2, 10, -1, -1],\n              [-1, -1, 10, -2],\n              [-1, -1, -2, 10]], dtype=float)\n\n# Compute the eigenvalues of matrix A\neigenvalues = np.linalg.eigvals(A)\n\n# Extract λmax and λmin\nlambda_max = np.max(eigenvalues).round(2)\nlambda_min = np.min(eigenvalues).round(2)\n\nprint(f\"lambda_max = {lambda_max}\")\nprint(f\"lambda_min = {lambda_min}\")\n\nalpha = (2/(lambda_max + lambda_min)).round(2)\nprint(f\"alpha={alpha}\")\n\n\nlambda_max = 12.0\nlambda_min = 6.0\nalpha=0.11\n\n\n\n\nCode\ndef richardson_iter(A, b, alpha, x0, max_iter=1000, tolerance=1e-6):\n    \"\"\"\n    Solves the linear system Ax = b using the Richardson iteration method.\n\n    Args:\n        A: The coefficient matrix (numpy array).\n        b: The right-hand side vector (numpy array).\n        alpha: The relaxation parameter.\n        x0: The initial guess for the solution vector (numpy array).\n        max_iter: The maximum number of iterations.\n        tolerance: The tolerance for convergence.\n\n    Returns:\n        A tuple containing the approximate solution vector (numpy array) and the number of iterations.\n    \"\"\"\n\n    n = len(b)\n    x = x0.copy()\n\n    for k in range(max_iter):\n        x_new = x + alpha * (b - np.dot(A, x))\n        if np.linalg.norm(x_new - x) &lt; tolerance:\n            return x_new, k + 1  # Return solution and number of iterations\n        x = x_new\n\n    return x, max_iter  # Return solution and max_iter if no convergence\n\n# Example usage:\nA = np.array([\n    [10, -2, -1, -1],\n    [-2, 10, -1, -1],\n    [-1, -1, 10, -2],\n    [-1, -1, -2, 10]\n], dtype=float)\n\nb = np.array([3, 15, 27, -9], dtype=float)\n\nn = len(b)\nx0 = np.zeros(n)  # Initial guess\nalpha = 0.111 # using alpha = 0.111 reduces iterations by 2\n\nsolution, iterations = richardson_iter(A, b, alpha, x0)\n\nprint(\"Solution:\", solution)\nprint(\"Iterations:\", iterations)\n\n\nSolution: [ 9.99999859e-01  1.99999993e+00  2.99999999e+00 -2.06155935e-07]\nIterations: 15",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#summary-table",
    "href": "Notebooks/Linear_Algebra/04.Iterative_solution_techniques.html#summary-table",
    "title": "Iterative Solution Techniques(Relaxation Methods)",
    "section": "📝 Summary Table",
    "text": "📝 Summary Table\n\n\n\n\n\n\n\n\nMethod\nRelaxation Parameter(s)\nKey Feature\n\n\n\n\nJacobi\nNone\nSimple, independent updates\n\n\nGauss-Seidel\nNone\nUses latest updates immediately\n\n\nSuccessive Over-Relaxation (SOR)\n$ $\nTunable parameter accelerates convergence\n\n\nSymmetric SOR (SSOR)\n$ $\nSymmetric application, preconditioning\n\n\nAccelerated Over-Relaxation (AOR)\nTwo parameters\nFlexible convergence control\n\n\nChebyshev Method\nNone (Polynomial-based)\nAcceleration via Chebyshev polynomials\n\n\nRichardson Iteration\n$ $\nSimple residual scaling",
    "crumbs": [
      "Linear Algebra",
      "Iterative Solution Techniques"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html",
    "href": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html",
    "title": "Fundamental concepts of linear systems",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sympy as sp\nfrom scipy.linalg import lu_factor, lu_solve, lu\nfrom IPython.display import display, Math, Latex\nfrom itables import init_notebook_mode\n\ninit_notebook_mode(all_interactive=True)\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nThis is the init_notebook_mode cell from ITables v2.2.5\n(you should not see this message - is your notebook trusted?)\nA system of linear equations is a collection of linear equations involving the same set of variables. These equations can be solved using various methods depending on the number of equations and unknowns.",
    "crumbs": [
      "Linear Algebra",
      "Linear Systems Concepts"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#representation",
    "href": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#representation",
    "title": "Fundamental concepts of linear systems",
    "section": "Representation",
    "text": "Representation\nA system of linear equations can be represented in the following ways: 1. Equation Form: A system of mm equations with nn variables: \\[\n\\begin{align}\na_{11}x_1 + a_{12}x_2 + \\dots + a_{1n}x_n &= b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\dots + a_{2n}x_n &= b_2 \\\\\n\\vdots \\\\\na_{m1}x_1 + a_{m2}x_2 + \\dots + a_{mn}x_n &= b_m\n\\end{align}\n\\]\nWhere: - \\(x_1, x_2, \\ldots, x_n\\) are the unknown variables - \\(a_{ij}\\) are the coefficients of the system - \\(b_1, b_2, \\ldots, b_m\\) are the constants (right-hand side values)\n\nMatrix Form: A matrix equation of the form AX=BAX=B, where AA is the coefficient matrix, XX is the variable matrix, and BB is the constant matrix.\nAugmented Matrix Form: An augmented matrix of the form [A∣B][A∣B], where AA is the coefficient matrix and BB is the constant matrix.\n\nExample. Consider the system of linear equations:\n\\[\n\\begin{aligned}\n    2x + 3y - z &= 5 \\\\\n    4x - y + 5z &= 6 \\\\\n    -2x + 7y + 2z &= -3\n\\end{aligned}\n\\]\nThis system can be written in matrix form as:\n\\[ AX = B \\]\nwhere:\n\\[\nA =\n\\begin{bmatrix}\n    2 & 3 & -1 \\\\\n    4 & -1 & 5 \\\\\n    -2 & 7 & 2\n\\end{bmatrix},\n\\quad\nX =\n\\begin{bmatrix}\n    x \\\\\n    y \\\\\n    z\n\\end{bmatrix},\n\\quad\nB =\n\\begin{bmatrix}\n    5 \\\\\n    6 \\\\\n    -3\n\\end{bmatrix}\n\\]\nThus, the system is represented as:\n\\[\n\\begin{bmatrix}\n    2 & 3 & -1 \\\\\n    4 & -1 & 5 \\\\\n    -2 & 7 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\n    x \\\\\n    y \\\\\n    z\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    5 \\\\\n    6 \\\\\n    -3\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Linear Algebra",
      "Linear Systems Concepts"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#solution-methods",
    "href": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#solution-methods",
    "title": "Fundamental concepts of linear systems",
    "section": "Solution Methods",
    "text": "Solution Methods\nSolving systems of linear equations involves finding the values of variables that satisfy all equations in the system simultaneously. There are several methods to solve these systems:\n\n1. Graphical Method\nThe graphical method for solving a system of linear equations involves plotting each equation on a coordinate plane and identifying the point where the lines intersect. Each equation represents a straight line, and the solution to the system is the coordinates of the intersection point. If the lines intersect at a single point, the system has a unique solution. If the lines are parallel, there is no solution since they never meet, indicating an inconsistent system. If the lines coincide, the system has infinitely many solutions because every point on the line satisfies both equations. While the graphical method provides a visual representation of the solution, it is most practical for systems with two variables and may be less accurate for complex or large systems, where algebraic methods are preferred.\nExample:\nSolve the system: \\[\n\\begin{aligned}\n2x + 3y &= 8 \\\\\n4x - y &= 2\n\\end{aligned}\n\\]\n\n\nCode\n# Define the equations in the form y = mx + b\ndef eq1(x):\n    return (8 - 2*x) / 3  # Rearranged from 2x + 3y = 8 =&gt; y = (8 - 2x) / 3\n\ndef eq2(x):\n    return (4*x - 2)  # Rearranged from 4x - y = 2 =&gt; y = 4x - 2\n\n# Generate x values\nx = np.linspace(-5, 5, 100)\n\n# Plot the lines\nplt.plot(x, eq1(x), label=\"2x + 3y = 8\", color='b')\nplt.plot(x, eq2(x), label=\"4x - y = 2\", color='r')\n\n# Solve for intersection\nA = np.array([[2, 3], [4, -1]])\nB = np.array([8, 2])\nsolution = np.linalg.solve(A, B)\n\n# Plot the intersection point\nplt.scatter(solution[0], solution[1], color='black', marker='o', label=f'Intersection ({solution[0]}, {solution[1]})')\n\n# Labels and legend\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.xlabel(\"x-axis\")\nplt.ylabel(\"y-axis\")\nplt.legend()\nplt.title(\"Graphical Solution of System of Equations\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2. Substitution Method\ninvolves solving one equation for one variable in terms of the other and substituting it into the second equation. It simplifies the system into a single-variable equation, making it easier to solve step by step.\nUsing the same Example:\n\n\nCode\n# Step 1: Solve for y in terms of x from the first equation\ndef equation1_y(x):\n    return (8 - 2*x) / 3  # Rearranged from 2x + 3y = 8\n\n# Step 2: Substitute y into the second equation\ndef equation2(x):\n    return 4*x - equation1_y(x) - 2  # 4x - y = 2, with equation1_y(x) replacing y\n\n# Solve the system using the substitution method\ndef solve_by_substitution():\n    x = sp.Symbol('x')  # Define x as a symbolic variable\n    x_value = sp.solve(equation2(x), x)[0]  # Solve for x\n    y_value = equation1_y(x_value)  # Find y using equation1_y\n\n    return x_value, y_value\n\n# Get the solution\nsolution = solve_by_substitution()\nprint(f\"Solution: x = {solution[0]}, y = {solution[1]}\")\n\n\nSolution: x = 1, y = 2\n\n\n\n\n3. Elimination Method\nIn this method, equations are manipulated by adding or subtracting them to eliminate one variable, making it easier to solve for the remaining variable. By systematically reducing the system, this approach is efficient for solving linear equations algebraically.\n\n\nCode\n# Define variables\nx, y = sp.symbols('x y')\n\n# Define the equations\neq1 = sp.Eq(2*x + 3*y, 8)  # 2x + 3y = 8\neq2 = sp.Eq(4*x - y, 2)    # 4x - y = 2\n\n# Solve using elimination method\nsolution = sp.solve((eq1, eq2), (x, y))\n\n# Display the result\nprint(f\"Solution: x = {solution[x]}, y = {solution[y]}\")\n\n\nSolution: x = 1, y = 2\n\n\n\n\n4. Matrix Method (Gaussian Elimination)\nThe system is written as an augmented matrix and row operations are applied to transform it into row echelon form or reduced row echelon form. Once simplified, back-substitution is used to find the solution of the variables.\nUsing the same example:",
    "crumbs": [
      "Linear Algebra",
      "Linear Systems Concepts"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#solving-the-system-using-gaussian-elimination",
    "href": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#solving-the-system-using-gaussian-elimination",
    "title": "Fundamental concepts of linear systems",
    "section": "Solving the System Using Gaussian Elimination",
    "text": "Solving the System Using Gaussian Elimination\nWe are given the system of equations:\n\\[\n\\begin{aligned}\n2x + 3y &= 8 \\\\\n4x - y &= 2\n\\end{aligned}\n\\]\n\nStep 1: Represent the System as an Augmented Matrix\n\\[\n\\begin{bmatrix} 2 & 3 & | 8 \\\\ 4 & -1 & | 2 \\end{bmatrix}\n\\]\n\n\nStep 2: Forward Elimination\nMake the first column in the second row zero by eliminating ( x ).\nMultiply row 1 by ( = 2 ) and subtract from row 2:\n\\[\nR_2 = R_2 - 2R_1\n\\]\nUpdated matrix:\n\\[\n\\begin{bmatrix} 2 & 3 & | 8 \\\\ 0 & -7 & | -14 \\end{bmatrix}\n\\]\n\n\nStep 3: Back Substitution\nSolve for ( y ):\n\\[\ny = \\frac{-14}{-7} = 2\n\\]\nSubstituting ( y = 2 ) into the first equation:\n\\[\nx = \\frac{8 - 3(2)}{2} = \\frac{8 - 6}{2} = \\frac{2}{2} = 1\n\\]\n\n\nFinal Solution:\n\\[\n\\boxed{(x = 1, y = 2)}\n\\]\nOr, in code:\n\n\nCode\n# Define the augmented matrix [A|B]\nA = np.array([[2, 3],   # Coefficients of x and y in the first equation\n              [4, -1]], dtype=float) # Coefficients of x and y in the second equation\nB = np.array([8, 2], dtype=float)    # Constants on the right-hand side\n\n# Forward Elimination: Make the first column zero in the second row\nfactor = A[1, 0] / A[0, 0]\nA[1] = A[1] - factor * A[0]\nB[1] = B[1] - factor * B[0]\n\n# Back Substitution: Solve for y\ny = B[1] / A[1, 1]\n\n# Solve for x using the first equation\nx = (B[0] - A[0, 1] * y) / A[0, 0]\n\n# Display the result\nprint(f\"Solution: x = {x}, y = {y}\")\n\n\nSolution: x = 1.0, y = 2.0\n\n\nAnother example: Solve a 3x3 System Using Gaussian Elimination\n\\[\n\\begin{aligned}\n2x + y - z &= 8  \\\\\n-3x - y + 2z &= -11 \\\\\n-2x + y + 2z &= -3\n\\end{aligned}\n\\]\nStep 1: Represent the System as an Augmented Matrix\n\\[\n\\begin{bmatrix}\n2 & 1 & -1 & | 8 \\\\\n-3 & -1 & 2 & | -11 \\\\\n-2 & 1 & 2 & | -3\n\\end{bmatrix}\n\\]\nStep 2: Forward Elimination First Pivot (Making the first diagonal element 1) Divide the first row by 2 to make the leading coefficient 1:\n\\[\n\\begin{bmatrix}\n1 & \\frac{1}{2} & -\\frac{1}{2} & | 4 \\\\\n-3 & -1 & 2 & | -11 \\\\\n-2 & 1 & 2 & | -3\n\\end{bmatrix}\n\\]\nEliminate the first column for rows 2 and 3:\n\\[\nR_2 = R_2 + 3R_1, \\quad R_3 = R_3 + 2R_1\n\\]\nUpdated matrix:\n\\[\n\\begin{bmatrix}\n1 & \\frac{1}{2} & -\\frac{1}{2} & | 4 \\\\\n0 & \\frac{1}{2} & \\frac{1}{2} & | 1 \\\\\n0 & 2 & 1 & | 5\n\\end{bmatrix}\n\\]\nSecond Pivot (Making the second diagonal element 1) Multiply the second row by 2:\n\\[\n\\begin{bmatrix}\n1 & \\frac{1}{2} & -\\frac{1}{2} & | 4 \\\\\n0 & 1 & 1 & | 2 \\\\\n0 & 2 & 1 & | 5\n\\end{bmatrix}\n\\]\nEliminate the second column in the third row:\n\\[\nR_3 = R_3 - 2R_2\n\\]\nUpdated matrix:\n\\[\n\\begin{bmatrix}\n1 & \\frac{1}{2} & -\\frac{1}{2} & | 4 \\\\\n0 & 1 & 1 & | 2 \\\\\n0 & 0 & -1 & | 1\n\\end{bmatrix}\n\\]\nThird Pivot (Making the third diagonal element 1) Multiply the third row by -1:\n\\[\n\\begin{bmatrix}\n1 & \\frac{1}{2} & -\\frac{1}{2} & | 4 \\\\\n0 & 1 & 1 & | 2 \\\\\n0 & 0 & 1 & | -1\n\\end{bmatrix}\n\\]\nStep 3: Back Substitution Solve for ( z ):\n\\[\nz = -1\n\\]\nSubstituting ( z = -1 ) into the second equation:\n\\[\ny + (-1) = 2 \\Rightarrow y = 3\n\\]\nSubstituting ( y = 3 ) and ( z = -1 ) into the first equation:\n\\[\nx + \\frac{1}{2}(3) - \\frac{1}{2}(-1) = 4\n\\]\n\\[\nx + \\frac{3}{2} + \\frac{1}{2} = 4\n\\]\n\\[\nx + 2 = 4 \\Rightarrow x = 2\n\\]\nFinal Solution: \\[\n\\boxed{(x = 2, y = 3, z = -1)}\n\\]\nit will look like this in python code:\n\n\nCode\nimport numpy as np\n\n# Define the augmented matrix [A|B]\nA = np.array([[2, 1, -1], \n              [-3, -1, 2],  \n              [-2, 1, 2]], dtype=float)  \nB = np.array([8, -11, -3], dtype=float)  \n\nn = len(A)\n\n# Forward Elimination\nfor i in range(n):\n    # Make the diagonal element 1 by dividing the row\n    factor = A[i, i] # this is the diagonal element or the pivot\n    A[i] = A[i] / factor # divide the row by the pivot\n    B[i] = B[i] / factor # divide the right-hand side by the pivot\n\n    for j in range(i + 1, n):\n        factor = A[j, i] # this is the element below the pivot\n        A[j] = A[j] - factor * A[i] # subtract the row below the pivot\n        B[j] = B[j] - factor * B[i] # subtract the right-hand side\n\n# Back Substitution\nx = np.zeros(n)\nfor i in range(n - 1, -1, -1):\n    x[i] = B[i] - np.sum(A[i, i+1:] * x[i+1:]) \n\n# Display the result\nprint(f\"Solution: x = {x[0]}, y = {x[1]}, z = {x[2]}\")\n\n\nSolution: x = 2.0, y = 3.0, z = -1.0\n\n\n\n\n5. Cramer’s Rule\nThis method is applicable to square systems with a unique solution and uses determinants to solve for each variable. By replacing a column in the coefficient matrix with the constants and computing determinants, the values of the variables are determined.\nCramer’s Rule states that for a system of n equations with n unknowns, written as:\n\\[\nAX = B\n\\]\nwhere: - ( A ) is the coefficient matrix, - ( X ) is the variable vector ([x, y, z]T ), - ( B ) is the constant matrix.\nThe solution is given by:\n\\[\nx = \\frac{\\det(A_x)}{\\det(A)}, \\quad y = \\frac{\\det(A_y)}{\\det(A)}, \\quad z = \\frac{\\det(A_z)}{\\det(A)}\n\\]\nwhere: - ( \\(\\det(A)\\) ) is the determinant of the coefficient matrix ( A ). - ( Ax, Ay, Az ) are obtained by replacing the respective columns of ( A ) with the constant matrix ( B ).\nExample System\n\\[\n\\begin{aligned}\n2x + y - z &= 8  \\\\\n-3x - y + 2z &= -11 \\\\\n-2x + y + 2z &= -3\n\\end{aligned}\n\\]\nStep 1: Define the Matrices The coefficient matrix ( A ):\n\\[\nA = \\begin{bmatrix}\n2 & 1 & -1 \\\\\n-3 & -1 & 2 \\\\\n-2 & 1 & 2\n\\end{bmatrix}\n\\]\nThe constant matrix ( B ):\n\\[\nB = \\begin{bmatrix} 8 \\\\ -11 \\\\ -3 \\end{bmatrix}\n\\]\nStep 2: Compute Determinants  - Compute ( \\(\\det(A)\\) ). - Construct new matrices by replacing columns with ( B ): - ( \\(A_{x}\\) ) (replace column 1 with ( B )), - ( \\(A_{y}\\) ) (replace column 2 with ( B )), - ( \\(A_{z}\\) ) (replace column 3 with ( B )). - Compute ( \\(\\det(A_{x}), \\det(A_{y}), \\det(A_{z})\\) ).\nFor a 3×3 matrix:\n\\[\nA = \\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i\n\\end{bmatrix}\n\\]\nThe determinant is computed as:\n\\[\n\\det(A) = a \\begin{vmatrix} e & f \\\\ h & i \\end{vmatrix}\n- b \\begin{vmatrix} d & f \\\\ g & i \\end{vmatrix}\n+ c \\begin{vmatrix} d & e \\\\ g & h \\end{vmatrix}\n\\]\nEach 2×2 determinant is found using:\n\\[\n\\begin{vmatrix} e & f \\\\ h & i \\end{vmatrix} = (e \\cdot i - f \\cdot h)\n\\]\n\\[\n\\begin{vmatrix} d & f \\\\ g & i \\end{vmatrix} = (d \\cdot i - f \\cdot g)\n\\]\n\\[\n\\begin{vmatrix} d & e \\\\ g & h \\end{vmatrix} = (d \\cdot h - e \\cdot g)\n\\]\nso\n\\[\nA = \\begin{bmatrix}\n2 & 1 & -1 \\\\\n-3 & -1 & 2 \\\\\n-2 & 1 & 2\n\\end{bmatrix}\n\\]\nExpanding along the first row:\n\\[\n\\det(A) = 2 \\begin{vmatrix} -1 & 2 \\\\ 1 & 2 \\end{vmatrix}\n- 1 \\begin{vmatrix} -3 & 2 \\\\ -2 & 2 \\end{vmatrix}\n+ (-1) \\begin{vmatrix} -3 & -1 \\\\ -2 & 1 \\end{vmatrix}\n\\]\nCalculate the 2×2 determinants:\n\\[\n\\begin{vmatrix} -1 & 2 \\\\ 1 & 2 \\end{vmatrix} = (-1 \\times 2) - (2 \\times 1) = -2 - 2 = -4\n\\]\n\\[\n\\begin{vmatrix} -3 & 2 \\\\ -2 & 2 \\end{vmatrix} = (-3 \\times 2) - (2 \\times -2) = -6 + 4 = -2\n\\]\n\\[\n\\begin{vmatrix} -3 & -1 \\\\ -2 & 1 \\end{vmatrix} = (-3 \\times 1) - (-1 \\times -2) = -3 - 2 = -5\n\\]\nNow, substitute these values back:\n\\[\n\\det(A) = (2 \\times -4) - (1 \\times -2) + (-1 \\times -5)\n\\]\n\\[\n\\det(A) = -8 + 2 + 5 = -1\n\\]\nThus, the determinant of ( A ) is:\n\\[\n\\boxed{\\det(A) = -1}\n\\]\nStep 3: Compute Solutions Using the formula:\n\\[\nx = \\frac{\\det(A_x)}{\\det(A)}, \\quad y = \\frac{\\det(A_y)}{\\det(A)}, \\quad z = \\frac{\\det(A_z)}{\\det(A)}\n\\]\nThis yields:\n\\[\n\\boxed{(x = 2, y = 3, z = -1)}\n\\] in Python code:\n\n\nCode\n# Define coefficient matrix A\nA = np.array([[2, 1, -1], \n              [-3, -1, 2],  \n              [-2, 1, 2]], dtype=float)\n\n# Define constant matrix B\nB = np.array([8, -11, -3], dtype=float)\n\n# Compute determinant of A\ndet_A = np.linalg.det(A)\n\nif det_A == 0:\n    print(\"The system has no unique solution (determinant is zero).\")\nelse:\n    # Compute determinants for A_x, A_y, A_z\n    A_x = A.copy()\n    A_x[:, 0] = B  # Replace first column with B\n    det_Ax = np.linalg.det(A_x)\n\n    A_y = A.copy()\n    A_y[:, 1] = B  # Replace second column with B\n    det_Ay = np.linalg.det(A_y)\n\n    A_z = A.copy()\n    A_z[:, 2] = B  # Replace third column with B\n    det_Az = np.linalg.det(A_z)\n\n    # Compute solutions\n    x = round(det_Ax / det_A, 2) # round to 2 decimal places\n    y = round(det_Ay / det_A, 2)\n    z = round(det_Az / det_A, 2)\n\n    # Display results\n    print(f\"Solution: x = {x}, y = {y}, z = {z}\")\n\n\nSolution: x = 2.0, y = 3.0, z = -1.0\n\n\n\n\n6. Inverse Matrix Method\nThe system is expressed in matrix form as AX = B, where A is the coefficient matrix, X is the variable matrix, and B is the constant matrix. The solution is found by computing X = A⁻¹B, provided that A is invertible.\nusing the same example as above: \\[\n\\begin{aligned}\n2x + y - z &= 8 \\\\\n-3x - y + 2z &= -11 \\\\\n-2x + y + 2z &= -3\n\\end{aligned}\n\\]\nThis system can be written in matrix form as:\n\\[\nA \\cdot X = B\n\\]\nwhere:\n\\[\nA = \\begin{bmatrix} 2 & 1 & -1 \\\\ -3 & -1 & 2 \\\\ -2 & 1 & 2 \\end{bmatrix}, \\quad\nX = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} 8 \\\\ -11 \\\\ -3 \\end{bmatrix}\n\\]\nTo solve for ( X ), we use the inverse matrix method:\n\\[\nX = A^{-1} \\cdot B\n\\]\nThe inverse of a matrix ( \\(A^{-1}\\) ) satisfies:\n\\[\nA \\cdot A^{-1} = I\n\\]\nwhere ( I ) is the identity matrix. The computed inverse of ( A ) is:\n\\[\nA^{-1} =\n\\begin{bmatrix}\n1 & 1 & 0 \\\\\n7 & 5 & 1 \\\\\n1 & 2 & 1\n\\end{bmatrix}\n\\]\nMultiplying ( \\(A^{-1}\\) ) by ( B ):\n\\[\nX = A^{-1} \\cdot B =\n\\begin{bmatrix} 1 & 1 & 0 \\\\ 7 & 5 & 1 \\\\ 1 & 2 & 1 \\end{bmatrix}\n\\begin{bmatrix} 8 \\\\ -11 \\\\ -3 \\end{bmatrix}\n=\n\\begin{bmatrix} 2 \\\\ 3 \\\\ -1 \\end{bmatrix}\n\\]\nThus, the solution is:\n\\[\nx = 2, \\quad y = 3, \\quad z = -1\n\\]\n\n\nCode\n# Define the coefficient matrix A\nA = np.array([[2, 1, -1], \n              [-3, -1, 2],  \n              [-2, 1, 2]], dtype=float)\n\n# Define the constant matrix B\nB = np.array([8, -11, -3], dtype=float)\n\n# Compute the inverse of A\nA_inv = np.linalg.inv(A)\n\n# Compute the solution X = A_inv * B\nX = np.dot(A_inv, B)\n\n# Display the solution rounded to avoid floating-point errors\nx, y, z = np.round(X, decimals=2)\n\nprint(f\"Solution: x = {x}, y = {y}, z = {z}\")\n\n\nSolution: x = 2.0, y = 3.0, z = -1.0",
    "crumbs": [
      "Linear Algebra",
      "Linear Systems Concepts"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#other-methods-simpler-ones",
    "href": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#other-methods-simpler-ones",
    "title": "Fundamental concepts of linear systems",
    "section": "Other methods, simpler Ones",
    "text": "Other methods, simpler Ones\n\n1. Mention Methods but easier way\nin python, there is an easiest to solve systems of linear equation, and that can be done using arrays and NumPy. For example, \\[\n\\begin{aligned}\n    8x + 3y - 2z &= 9 \\\\\n   -4x + 7y + 5z &= 15 \\\\\n    3x + 4y - 12z &= 35\n\\end{aligned}\n\\] NumPy’s np.linalg.solve() function can be used to solve this system of equations for the variables x, y and z. notice that this is the same function used to calculate the intersection point in the example for graphical method.\n\n\nCode\nA = np.array([[8, 3, -2], \n              [-4, 7, 5],\n              [3, 4, -12]])\nB = np.array([9, 15, 35])\nx = np.linalg.solve(A, B).round(2)\nprint(f\"Solution: x = {x[0]}, y = {x[1]}, z = {x[2]}\")\n\n\nSolution: x = -0.58, y = 3.23, z = -1.99\n\n\nor through Inverse Matrix method\n\n\nCode\nA_inv = np.linalg.inv(A)\nanother_x = np.dot(A_inv, B).round(2)\n\nprint(f\"Solution: x = {another_x[0]}, y = {another_x[1]}, z = {another_x[2]}\")\n\n\nSolution: x = -0.58, y = 3.23, z = -1.99\n\n\n\n\n2. LU Decomposition Method\nA method for solving a system of linear equations by factoring a square matrix ( \\(A\\) ) into two triangular matrices:\n\\[\nA = LU\n\\]\nwhere:\n\n( \\(L\\) ) is a lower triangular matrix (with 1s on the diagonal),\n( \\(U\\) ) is an upper triangular matrix (with nonzero values above the diagonal).\n\nIn some cases, a permutation matrix ( \\(P\\) ) is introduced to improve numerical stability, leading to:\n\\[\nPA = LU\n\\]\nUsing example above, given the system:\n\\[\n\\begin{aligned}\n    8x + 3y - 2z &= 9 \\\\\n   -4x + 7y + 5z &= 15 \\\\\n    3x + 4y - 12z &= 35\n\\end{aligned}\n\\]\nWe express it in matrix form:\n\\[\nA = \\begin{bmatrix} 8 & 3 & -2 \\\\ -4 & 7 & 5 \\\\ 3 & 4 & -12 \\end{bmatrix}, \\quad\nb = \\begin{bmatrix} 9 \\\\ 15 \\\\ 35 \\end{bmatrix}\n\\]\nAfter LU decomposition, we obtain:\n\\[\nL = \\begin{bmatrix} 1 & 0 & 0 \\\\ -0.5 & 1 & 0 \\\\ 0.375 & 0.558 & 1 \\end{bmatrix}, \\quad\nU = \\begin{bmatrix} 8 & 3 & -2 \\\\ 0 & 8.5 & 4 \\\\ 0 & 0 & -9.647 \\end{bmatrix}\n\\]\nWe then solve the system in two steps:\n\nForward substitution to solve ( \\(Ly = b\\) ). We solve for ( \\(y\\) ) in the equation:\n\n\\[\nL y = Pb\n\\]\nGiven:\n\\[\nL =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n-0.5 & 1 & 0 \\\\\n0.375 & 0.3382 & 1\n\\end{bmatrix},\n\\quad\nb =\n\\begin{bmatrix}\n9 \\\\ 15 \\\\ 35\n\\end{bmatrix}\n\\]\nThe system of equations is:\n\\[\n\\begin{aligned}\n    y_1 &= 9 \\\\\n   -0.5 y_1 + y_2 &= 15 \\\\\n    0.375 y_1 + 0.3382 y_2 + y_3 &= 35\n\\end{aligned}\n\\]\nSolving step-by-step:\n\nSolve for ( \\(y_{1}\\) ):\n\\[\ny_1 = 9\n\\]\nSolve for ( \\(y_{2}\\) ):\n\\[\n-0.5(9) + y_2 = 15\n\\]\n\\[\ny_2 = 15 + 4.5 = 19.5\n\\]\nSolve for ( \\(y_{3}\\) ):\n\\[\n0.375(9) + 0.3382(19.5) + y_3 = 35\n\\]\n\\[\n3.375 + 6.601 + y_3 = 35\n\\]\n\\[\ny_3 = 35 - 9.976 = 25.029\n\\]\n\nThus, we obtain:\n\\[\ny =\n\\begin{bmatrix}\n9 \\\\\n19.5 \\\\\n25.029\n\\end{bmatrix}\n\\]\n\n\nBackward substitution to solve ( \\(Ux = y\\) ). We now solve for ( \\(x\\) ) in:\n\n\\[\nUx = y\n\\]\nGiven:\n\\[\nU =\n\\begin{bmatrix}\n8 & 3 & -2 \\\\\n0 & 8.5 & 4 \\\\\n0 & 0 & -12.6029\n\\end{bmatrix},\n\\quad\ny =\n\\begin{bmatrix}\n9 \\\\\n19.5 \\\\\n25.029\n\\end{bmatrix}\n\\]\nThe system of equations is:\n\\[\n\\begin{aligned}\n    8x_1 + 3x_2 - 2x_3 &= 9 \\\\\n    8.5x_2 + 4x_3 &= 19.5 \\\\\n    -12.6029x_3 &= 25.029\n\\end{aligned}\n\\]\n\n\nSolving step-by-step:\n\nSolve for ( \\(x_{3}\\) ):\n\\[\nx_3 = \\frac{25.029}{-12.6029} = -1.986\n\\]\nSolve for ( \\(x_{2}\\) ):\n\\[\n8.5x_2 + 4(-1.986) = 19.5\n\\]\n\\[\n8.5x_2 - 7.944 = 19.5\n\\]\n\\[\nx_2 = \\frac{19.5 + 7.944}{8.5} = 3.2287\n\\]\nSolve for ( \\(x_{1}\\) ):\n\\[\n8x_1 + 3(3.2287) - 2(-1.986) = 9\n\\]\n\\[\n8x_1 + 9.686 + 3.972 = 9\n\\]\n\\[\n8x_1 = 9 - 13.658\n\\]\n\\[\nx_1 = \\frac{-4.658}{8} = -0.5823\n\\]\n\nThus, we obtain:\n\\[\nx =\n\\begin{bmatrix}\n-0.5823 \\\\\n3.2287 \\\\\n-1.986\n\\end{bmatrix}\n\\]\nIn python code, the flow will look like this\n\n\nCode\ndef forward_substitution(L, b):\n    \"\"\"Solve Ly = b using forward substitution.\"\"\"\n    y = np.zeros_like(b, dtype=np.float64)\n    for i in range(len(b)):\n        y[i] = b[i] - np.dot(L[i, :i], y[:i])\n    return y\n\ndef backward_substitution(U, y):\n    \"\"\"Solve Ux = y using backward substitution.\"\"\"\n    x = np.zeros_like(y, dtype=np.float64)\n    for i in range(len(y)-1, -1, -1):\n        x[i] = (y[i] - np.dot(U[i, i+1:], x[i+1:])) / U[i, i]\n    return x\n\n# Define matrix A and vector b\nA = np.array([[8, 3, -2],\n              [-4, 7, 5],\n              [3, 4, -12]], dtype=float)\n\nb = np.array([9, 15, 35], dtype=float)\n\n# Perform LU decomposition\nP, L, U = lu(A)\n\n# Compute Pb\nPb = np.dot(P, b)\n\n# Solve for y using forward substitution\ny = forward_substitution(L, Pb)\n\n# Solve for x using backward substitution\nx = backward_substitution(U, y).round(2)\n\n# Display the results\nprint(\"Lower Triangular Matrix (L):\\n\", L)\nprint(\"Upper Triangular Matrix (U):\\n\", U)\nprint(\"Solution (x):\\n\", x)\n\n\nLower Triangular Matrix (L):\n [[ 1.          0.          0.        ]\n [-0.5         1.          0.        ]\n [ 0.375       0.33823529  1.        ]]\nUpper Triangular Matrix (U):\n [[  8.           3.          -2.        ]\n [  0.           8.5          4.        ]\n [  0.           0.         -12.60294118]]\nSolution (x):\n [-0.58  3.23 -1.99]\n\n\nor you can just use the lu_factor and lu_solve function from scipy directly\n\n\nCode\n# Define matrix A and vector b\nA = np.array([[8, 3, -2],\n              [-4, 7, 5],\n              [3, 4, -12]], dtype=float)\n\nb = np.array([9, 15, 35], dtype=float)\n\n# Perform LU factorization\nlu, piv = lu_factor(A)\n\n# Solve for x\nx = lu_solve((lu, piv), b).round(2)\n\n# Display the solution\nprint(f\"Solution: x = {x[0]}, y = {x[1]}, z = {x[2]}\")\n\n\nSolution: x = -0.58, y = 3.23, z = -1.99",
    "crumbs": [
      "Linear Algebra",
      "Linear Systems Concepts"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#solving-overdetermined-and-underdetermined-systems",
    "href": "Notebooks/Linear_Algebra/02.Linear_Systems_concepts.html#solving-overdetermined-and-underdetermined-systems",
    "title": "Fundamental concepts of linear systems",
    "section": "Solving overdetermined and underdetermined systems",
    "text": "Solving overdetermined and underdetermined systems\n\nOverdetermined Systems\nA system of equations is classified as overdetermined when it has more equations than variables (m&gt;nm&gt;n), meaning there are more constraints than unknowns. Such systems typically have no exact solution unless the equations are dependent or redundant. In practical applications, overdetermined systems often arise in data fitting problems, where an approximate solution can be found using least squares methods, minimizing the error between the given equations and the best possible solution.\nExample:\nConsider the system of equations:\n\\[\n\\begin{aligned}\n    x + y &= 2 \\\\\n    2x + 3y &= 5 \\\\\n    4x + 5y &= 7\n\\end{aligned}\n\\]\nThis system is overdetermined since it has more equations (3) than unknowns (2). In most cases, an exact solution does not exist, so we use the least squares method to find an approximate solution.\nSolution via Least Squares Method Given the system of equations:\n\\[\n\\begin{aligned}\n    x + y &= 2 \\\\\n    2x + 3y &= 5 \\\\\n    4x + 5y &= 7\n\\end{aligned}\n\\]\nwe express it in matrix form as:\n\\[\nA =\n\\begin{bmatrix} 1 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\end{bmatrix}, \\quad\nx =\n\\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\quad\nb =\n\\begin{bmatrix} 2 \\\\ 5 \\\\ 7 \\end{bmatrix}\n\\]\nThe least squares solution satisfies the normal equation:\n\\[\nA^T A x = A^T b\n\\]\n\n\nStep 1: Compute ( A^T A )\n\\[\nA^T A =\n\\begin{bmatrix} 1 & 2 & 4 \\\\ 1 & 3 & 5 \\end{bmatrix}\n\\begin{bmatrix} 1 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\end{bmatrix}\n=\n\\begin{bmatrix} 21 & 26 \\\\ 26 & 35 \\end{bmatrix}\n\\]\n\n\nStep 2: Compute ( A^T b )\n\\[\nA^T b =\n\\begin{bmatrix} 1 & 2 & 4 \\\\ 1 & 3 & 5 \\end{bmatrix}\n\\begin{bmatrix} 2 \\\\ 5 \\\\ 7 \\end{bmatrix}\n=\n\\begin{bmatrix} 48 \\\\ 61 \\end{bmatrix}\n\\]\n\n\nStep 3: Solve for ( x )\nWe now solve:\n\\[\n\\begin{bmatrix} 21 & 26 \\\\ 26 & 35 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\end{bmatrix}\n=\n\\begin{bmatrix} 48 \\\\ 61 \\end{bmatrix}\n\\]\nBy computing the inverse:\n\\[\n\\begin{bmatrix} x \\\\ y \\end{bmatrix}\n=\n\\begin{bmatrix} 21 & 26 \\\\ 26 & 35 \\end{bmatrix}^{-1}\n\\begin{bmatrix} 48 \\\\ 61 \\end{bmatrix}\n\\]\nSolving, we obtain:\n\\[\nx \\approx -0.6667, \\quad y \\approx 2.0\n\\]\n\n\nFinal Answer:\n\\[\n\\begin{bmatrix} x \\\\ y \\end{bmatrix}\n\\approx\n\\begin{bmatrix} -0.6667 \\\\ 2.0 \\end{bmatrix}\n\\] Or in Python,\n\n\nCode\n# Define the coefficient matrix A and the right-hand side vector b\nA = np.array([[1, 1], [2, 3], [4, 5]])\nb = np.array([2, 5, 7])\n\n# Solve using NumPy's least squares method\nx, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n\n# Display the solution\nprint(\"Least Squares Solution (x, y):\", x)\n\n\nLeast Squares Solution (x, y): [-0.66666667  2.        ]\n\n\n\n\nUnderdetermined Systems\na system is underdetermined when it has fewer equations than variables (m&lt;nm&lt;n), leading to infinitely many possible solutions unless additional constraints are imposed. These solutions are typically expressed in terms of free variables, with specific choices such as the minimum-norm solution providing a unique answer. Regularization techniques, like Lasso or Ridge regression, are often used in applied settings to impose further constraints and refine the solution.\n\n\nExample: Solving an Underdetermined System\nConsider the system of equations:\n\\[\n\\begin{aligned}\n    x + y + z &= 4 \\\\\n    2x + 3y + 5z &= 7\n\\end{aligned}\n\\]\nThis system is underdetermined because it has fewer equations (2) than unknowns (3), meaning it has infinitely many solutions.\nStep 1: Matrix Representation We express the system in matrix form:\n\\[\nA =\n\\begin{bmatrix} 1 & 1 & 1 \\\\ 2 & 3 & 5 \\end{bmatrix}, \\quad\nx =\n\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}, \\quad\nb =\n\\begin{bmatrix} 4 \\\\ 7 \\end{bmatrix}\n\\]\nwhere the system is written as:\n\\[\nA x = b\n\\]\nStep 2: Express the General Solution Since the system has more variables than equations, we express the solution in terms of free variables. The augmented matrix is:\n\\[\n\\left[\n\\begin{array}{ccc|c}\n1 & 1 & 1 & 4 \\\\\n2 & 3 & 5 & 7\n\\end{array}\n\\right]\n\\]\nUsing row reduction (Gaussian elimination):\n\nSubtract 2 times the first row from the second:\n\n\\[\n\\left[\n\\begin{array}{ccc|c}\n1 & 1 & 1 & 4 \\\\\n0 & 1 & 3 & -1\n\\end{array}\n\\right]\n\\]\n\nExpress variables in terms of free variable ( \\(z = t\\) ):\n\nFrom the second row:\n\\[ y + 3z = -1 \\quad \\Rightarrow \\quad y = -1 - 3t \\]\nFrom the first row:\n\\[ x + y + z = 4 \\quad \\Rightarrow \\quad x = 4 - y - z \\]\nSubstituting ( \\(y = -1 - 3t\\) ):\n\\[ x = 4 - (-1 - 3t) - t = 5 + 2t \\]\n\n\n\n\nStep 3: Final Parametric Solution\nSince ( \\(z\\) ) is a free variable, let ( \\(z = t\\) ). The general solution is:\n\\[\n\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}\n=\n\\begin{bmatrix} 5 \\\\ -1 \\\\ 0 \\end{bmatrix}\n+ t \\begin{bmatrix} 2 \\\\ -3 \\\\ 1 \\end{bmatrix}, \\quad t \\in \\mathbb{R}\n\\]\nThis means the solution forms a one-dimensional subspace in (\\(\\mathbb{R}^3\\)), where ( \\(t\\) ) can take any real value.\n\n\nCode\n# Define the coefficient matrix A and the right-hand side vector b\nA = np.array([[1, 1, 1], [2, 3, 5]])\nb = np.array([4, 7])\n\n# Solve using NumPy's least squares method to find a particular solution\nx_p, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n\n# Find the null space (general solution component)\nU, S, Vt = np.linalg.svd(A)  # Singular Value Decomposition\nnull_space = Vt.T[:, S.size:]  # Columns corresponding to zero singular values\n\n# Display results\nprint(\"Particular Solution (one specific solution):\")\nprint(x_p)\n\nprint(\"\\nNull Space Basis (general solution component):\")\nprint(null_space)\n\nprint(\"\\nFinal General Solution:\")\nprint(f\"x = {x_p} + t * {null_space.flatten()}, where t is any real number\")\n\n\nParticular Solution (one specific solution):\n[ 3.14285714  1.78571429 -0.92857143]\n\nNull Space Basis (general solution component):\n[[ 0.53452248]\n [-0.80178373]\n [ 0.26726124]]\n\nFinal General Solution:\nx = [ 3.14285714  1.78571429 -0.92857143] + t * [ 0.53452248 -0.80178373  0.26726124], where t is any real number\n\n\nNow, don’t be confused why as to why they have different answer, the discrepancy comes from the different methods used to find the particular solution and null space basis. I manually solved the system using row reduction while Python np.linalg.lstsq(A, b, rcond=None) finds the least-norm particular solution, which minimizes the Euclidean norm \\(∥x∥\\). This solution may be different from the manual row reduction, but it is still valid.",
    "crumbs": [
      "Linear Algebra",
      "Linear Systems Concepts"
    ]
  },
  {
    "objectID": "Notebooks/Analytical_Chemistry/06.UV-Vis_Spectroscopy.html",
    "href": "Notebooks/Analytical_Chemistry/06.UV-Vis_Spectroscopy.html",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "format: html: embed-resources: false fig-width: 9 fig-height: 6 html-math-method: method: mathjax url: “https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js” jupyter: python3 code-fold: true code-overflow: wrap",
    "crumbs": [
      "Analytical Chemistry",
      "UV-Vis Spectroscopy"
    ]
  },
  {
    "objectID": "Notebooks/Analytical_Chemistry/04.Electrochemistry.html",
    "href": "Notebooks/Analytical_Chemistry/04.Electrochemistry.html",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "format: html: embed-resources: false fig-width: 9 fig-height: 6 html-math-method: method: mathjax url: “https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js” jupyter: python3 code-fold: true code-overflow: wrap",
    "crumbs": [
      "Analytical Chemistry",
      "Electrochemistry"
    ]
  },
  {
    "objectID": "Notebooks/Analytical_Chemistry/02.Gravimetry.html",
    "href": "Notebooks/Analytical_Chemistry/02.Gravimetry.html",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "format: html: embed-resources: false fig-width: 9 fig-height: 6 html-math-method: method: mathjax url: “https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js” jupyter: python3 code-fold: true code-overflow: wrap",
    "crumbs": [
      "Analytical Chemistry",
      "Gravimetry"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python for Chemistry",
    "section": "",
    "text": "I created this site out of my passion for computational chemistry and my desire to refine my skills in the field. Here, I explore how Python can be used to solve complex chemical problems, from molecular modeling to quantum chemistry and data analysis. Whether you’re a student, researcher, or fellow enthusiast, you’ll find tutorials, projects, and insights that showcase the power of Python in chemistry.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Python for Chemistry",
    "section": "",
    "text": "I created this site out of my passion for computational chemistry and my desire to refine my skills in the field. Here, I explore how Python can be used to solve complex chemical problems, from molecular modeling to quantum chemistry and data analysis. Whether you’re a student, researcher, or fellow enthusiast, you’ll find tutorials, projects, and insights that showcase the power of Python in chemistry.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#is-this-for-me",
    "href": "index.html#is-this-for-me",
    "title": "Python for Chemistry",
    "section": "Is this for me?",
    "text": "Is this for me?\nThis site is designed for those who already have a basic understanding of Python and want to apply it to computational chemistry. If you’re familiar with Python’s syntax, data structures, and basic programming concepts, you’ll be able to follow along and deepen your skills in scientific computing. Whether you’re a chemist looking to integrate coding into your research or a programmer eager to explore chemical simulations, this site will provide valuable insights and hands-on applications.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#why-python",
    "href": "index.html#why-python",
    "title": "Python for Chemistry",
    "section": "Why Python?",
    "text": "Why Python?\nPython is a powerful and versatile programming language that has become a standard tool in computational chemistry. Here’s why Python is ideal for this field:\n\nEase of Use – Python’s simple and readable syntax makes it accessible for chemists and researchers without extensive programming backgrounds.\nExtensive Libraries – With libraries like NumPy, Pandas, RDKit, ASE, Psi4, and LAMMPS, Python provides robust tools for molecular modeling, quantum chemistry, and data analysis.\nAutomation & Efficiency – Python allows you to automate repetitive calculations, data processing, and simulations, saving time and reducing errors.\nVisualization Capabilities – With Matplotlib, Seaborn, and Py3Dmol, Python enables clear and effective visualization of molecular structures and simulation results.\nInteroperability – Python seamlessly integrates with other computational chemistry software, allowing you to interface with LAMMPS, ORCA, and Psi4 for advanced simulations.\nCommunity & Open Source – A large, active community of scientists and developers continuously improves Python’s chemistry-related tools, making it a cost-effective and evolving solution.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-software-will-you-use",
    "href": "index.html#what-software-will-you-use",
    "title": "Python for Chemistry",
    "section": "What Software Will You Use?",
    "text": "What Software Will You Use?\nTo follow along with the tutorials and projects, you’ll need:\n\nPython (Latest Version) – The core programming language for all computations.\nJupyter Notebook – An interactive coding environment for writing and running Python scripts.\nNumPy, Pandas, Sympy, Scipy – Essential libraries for numerical computing and data manipulation.\nMatplotlib, Seaborn, and Plotly – Tools for visualizing chemical data and simulation results.\nRDKit – A powerful toolkit for cheminformatics and molecular modeling.\nASE (Atomic Simulation Environment) – For working with atomic structures and simulations.\nPsi4 / ORCA – Quantum chemistry packages for molecular calculations.\nLAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator) – For performing molecular dynamics simulations.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Python for Chemistry",
    "section": "What You Will Learn?",
    "text": "What You Will Learn?\nThis site is designed to help you apply Python to computational chemistry through hands-on tutorials and projects. Along the way, you’ll also deepen your understanding of key theoretical concepts by applying them in real-world scenarios. By following along, you will:\n\nApply Theoretical Concepts in Practice – Reinforce your understanding of computational chemistry principles, such as molecular mechanics, electronic structure theory, and thermodynamics, by implementing them in Python-based simulations and analyses.\nWork with Molecular Structures – Learn how to manipulate and analyze molecular structures using RDKit and ASE.\nPerform Quantum Chemistry Calculations – Use Psi4 and ORCA to run molecular simulations and analyze energy states.\nConduct Molecular Dynamics Simulations – Explore LAMMPS to simulate atomic and molecular interactions over time.\nAutomate Chemical Data Analysis – Utilize NumPy and Pandas for handling large datasets and extracting meaningful insights.\nVisualize Chemical Data & Structures – Create informative plots and molecular visualizations with Matplotlib, Seaborn, and Py3Dmol.\nOptimize Workflows – Write Python scripts to automate common computational chemistry tasks, improving efficiency and reproducibility.\nIntegrate with External Chemistry Software – Learn how to use Python to interface with external tools for advanced chemical modeling and simulations.\n\nBy the end of this journey, you’ll not only gain technical skills in Python for computational chemistry but also develop a deeper intuition for the theoretical foundations that drive these simulations.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-do-i-get-started",
    "href": "index.html#how-do-i-get-started",
    "title": "Python for Chemistry",
    "section": "How Do I Get Started?",
    "text": "How Do I Get Started?\nGetting started with Python for computational chemistry is easy! Follow these steps begin your learning journey.\n\nBeginner Level\n\nPython Basics: Ensure you’re comfortable with Python fundamentals\nChemistry Concepts: Review relevant chemistry principles\nMath Concepts: Review relevant mathematical principles\nData Handling: Learn NumPy, Pandas for scientific data manipulation\nVisualization: Master Matplotlib for plotting results\n\n\n\nIntermediate Level\n\nMolecular Manipulation: Dive deeper into RDKit\nQuantum Chemistry Basics: Understand computational methods\nIntegration with Quantum Chemistry Software: Learn to interface with programs like Gaussian, GAMESS, or Psi4\n\n\n\nAdvanced Level\n\nCustom Method Development: Create your own computational routines\nMachine Learning for Chemistry: Apply ML techniques to chemical problems\nHigh-Performance Computing: Scale your calculations to larger systems",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#recommended-resources",
    "href": "index.html#recommended-resources",
    "title": "Python for Chemistry",
    "section": "Recommended Resources",
    "text": "Recommended Resources\n\nBooks\n\n“Python for Computational Science and Engineering” by Hans Fangohr\n“Computational Chemistry Using Python” by Shiv Upadhyay\n“Introduction to Computational Chemistry” by Frank Jensen\n\n\n\nOnline Courses\n\nedX: “Principles of Chemical Science”\nPython For Everybody by Charles Severance\n\n\n\nCommunities\n\nChemistry Stack Exchange",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "Python for Chemistry",
    "section": "Next Steps",
    "text": "Next Steps\nOnce you’ve set up your environment and run your first script, explore my tutorials section to learn more about computational chemistry.\nReady to start your computational chemistry journey with Python? Let’s dive in!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "Notebooks/Analytical_Chemistry/01.Data_Preprocessing_and_chemometrics.html",
    "href": "Notebooks/Analytical_Chemistry/01.Data_Preprocessing_and_chemometrics.html",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "format: html: embed-resources: false fig-width: 9 fig-height: 6 html-math-method: method: mathjax url: “https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js” jupyter: python3 code-fold: true code-overflow: wrap",
    "crumbs": [
      "Analytical Chemistry",
      "Data Preprocessing and Chemometrics"
    ]
  },
  {
    "objectID": "Notebooks/Analytical_Chemistry/03.Titremetry.html",
    "href": "Notebooks/Analytical_Chemistry/03.Titremetry.html",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "format: html: embed-resources: false fig-width: 9 fig-height: 6 html-math-method: method: mathjax url: “https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js” jupyter: python3 code-fold: true code-overflow: wrap",
    "crumbs": [
      "Analytical Chemistry",
      "Titremetry"
    ]
  },
  {
    "objectID": "Notebooks/Analytical_Chemistry/05.Chromatography.html",
    "href": "Notebooks/Analytical_Chemistry/05.Chromatography.html",
    "title": "Python for Computational Chemistry",
    "section": "",
    "text": "format: html: embed-resources: false fig-width: 9 fig-height: 6 html-math-method: method: mathjax url: “https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js” jupyter: python3 code-fold: true code-overflow: wrap",
    "crumbs": [
      "Analytical Chemistry",
      "Chromatography"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/01.Vectors_and_Matrices.html",
    "href": "Notebooks/Linear_Algebra/01.Vectors_and_Matrices.html",
    "title": "Vectors and Matrices",
    "section": "",
    "text": "A vector is a mathematical object that possesses both magnitude (or length) and direction. This distinguishes it from a scalar, which has only magnitude. Examples: velocity (speed and direction), force (strength and direction), displacement (distance and direction).\nMagnitude: The magnitude of a vector is its length or size. It’s a scalar quantity and is always non-negative. We denote the magnitude of a vector \\(\\vec{v}\\) as \\(||\\vec{v}||\\) or simply \\(v\\). Direction: The direction of a vector indicates the line of action and sense of the vector. It can be specified using angles relative to a reference axis or by comparing it to another vector.\n\n\n2D Representation: * In a two-dimensional plane (like the xy-plane), a vector is typically represented as an arrow. * The tail of the arrow is the starting point (initial point), and the head of the arrow is the ending point (terminal point). * The length of the arrow represents the magnitude of the vector. * The orientation of the arrow represents the direction of the vector. * Example: A vector representing a wind velocity of 10 m/s blowing northeast.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport math\nfrom IPython.display import display, HTML\nimport manim as mn\nfrom manim import *\nimport plotly.graph_objects as go\nconfig.media_embed = True\n\n\n\n\nCode\ndef plot_2d_vectors(vectors, colors=None):\n    fig, ax = plt.subplots()\n    ax.axhline(0, color='black', linewidth=0.5)\n    ax.axvline(0, color='black', linewidth=0.5)\n    ax.grid()\n    \n    for i, vec in enumerate(vectors):\n        color = colors[i] if colors else 'blue'\n        ax.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, color=color)\n    \n    max_val = np.max(np.abs(vectors)) + 1\n    ax.set_xlim(-max_val, max_val)\n    ax.set_ylim(-max_val, max_val)\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_title('2D Vector Representation')\n    plt.show()\n\nvectors_2d = [np.array([3, 4]), np.array([-2, 5])]\nplot_2d_vectors(vectors_2d, colors=['red', 'green'])\n\n\n\n\n\n\n\n\n\n3D Representation: * In three-dimensional space (like the xyz-space), vectors are still represented as arrows, but now they exist in a three-dimensional coordinate system. * The concepts of magnitude and direction remain the same. * Example: A vector representing the force acting on an object in 3D space.\n\n\nCode\ndef plot_3d_vectors(vectors, colors=None):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.quiver(0, 0, 0, *zip(*vectors), color=colors if colors else 'blue')\n    \n    max_val = np.max(np.abs(vectors)) + 1\n    ax.set_xlim(-max_val, max_val)\n    ax.set_ylim(-max_val, max_val)\n    ax.set_zlim(-max_val, max_val)\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_zlabel('Z-axis')\n    ax.set_title('3D Vector Representation')\n    plt.show()\n\nvectors_3d = [np.array([3, 4, 5]), np.array([-2, 5, 3])]\nplot_3d_vectors(vectors_3d, colors=['red', 'green'])\n\n\n\n\n\n\n\n\n\nn-dimension vector\nThe concept of vectors can be extended beyond 3d to any number of dimensions (n-dimensions). An n-dimensional vector has n components: \\(\\vec{v} = \\langle v_1, v_2, ..., v_n \\rangle\\). Example: Vectors are used to represent data points in high dimensional data sets.\nComponent Form of Vectors (e.g., \\(v=⟨v1​,v2​,v3​⟩\\)).\n\nComponent Form:\n\nVectors can be expressed in component form, which is particularly useful for mathematical operations.\nIn 2D, a vector \\(\\vec{v}\\) can be written as \\(\\vec{v} = \\langle v_1, v_2 \\rangle\\), where \\(v_1\\) and \\(v_2\\) are the components of the vector along the x-axis and y-axis, respectively.\nIn 3D, a vector \\(\\vec{v}\\) can be written as \\(\\vec{v} = \\langle v_1, v_2, v_3 \\rangle\\), where \\(v_1\\), \\(v_2\\), and \\(v_3\\) are the components along the x-axis, y-axis, and z-axis, respectively.\nThe components can be found by projecting the vector onto the coordinate axes.\n\n\nExample: a vector that moves an object 2 units in the x direction, 3 units in the y direction, and -1 units in the z direction would be represented as \\(\\vec{v} = \\langle 2, 3, -1 \\rangle\\).\n\nMagnitude in Component Form:\n\nThe magnitude of a vector in component form can be calculated using the Pythagorean theorem:\n\nIn 2D: \\(||\\vec{v}|| = \\sqrt{v_1^2 + v_2^2}\\)\nIn 3D: \\(||\\vec{v}|| = \\sqrt{v_1^2 + v_2^2 + v_3^2}\\)\n\n\n\n\n\n\n\n\n\n\nA vector with all components equal to zero, meaning it is a vector with zero magnitude and an undefined direction, often represented as (0, 0) in two dimensions or (0, 0, 0) in three dimensions\n\n\n\nA unit vector is fundamentally a vector whose magnitude, or length, is precisely one. Its primary function is to indicate direction within a given space. A common convention to denote that a vector is a unit vector is the use of the “hat” symbol, such as in \\(\\mathbf{\\hat{u}}\\). To obtain a unit vector \\(\\mathbf{\\hat{u}}\\) that shares the same direction as a given vector \\(\\mathbf{v}\\), one must divide the vector \\(\\mathbf{v}\\) by its magnitude, represented as \\(||\\mathbf{v}||\\). This process effectively scales the vector down to a length of one while preserving its original direction. The magnitude of a vector \\(\\mathbf{v} = \\langle v_1, v_2, v_3 \\rangle\\) is calculated using the formula \\(||\\mathbf{v}|| = \\sqrt{v_1^2 + v_2^2 + v_3^2}\\). Example:\n\n\nCode\ndef plot_unit_vector(vector):\n  \"\"\"\n  Plots a vector and its corresponding unit vector.\n\n  Args:\n    vector: A list or tuple representing the vector.\n  \"\"\"\n  magnitude = math.sqrt(sum(x**2 for x in vector))\n\n  if magnitude == 0:\n    print(\"Vector has zero magnitude, cannot plot unit vector.\")\n    return\n\n  unit_vector = [x / magnitude for x in vector]\n\n  plt.figure()\n  plt.quiver(0, 0, vector[0], vector[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Original Vector')\n  plt.quiver(0, 0, unit_vector[0], unit_vector[1], angles='xy', scale_units='xy', scale=1, color='red', label='Unit Vector')\n\n  # Set axis limits to accommodate both vectors\n  max_val = max(abs(vector[0]), abs(vector[1]), abs(unit_vector[0]), abs(unit_vector[1]))\n  plt.xlim([-max_val - 1, max_val + 1])\n  plt.ylim([-max_val - 1, max_val + 1])\n\n  plt.xlabel('X-axis')\n  plt.ylabel('Y-axis')\n  plt.title('Vector and Unit Vector')\n  plt.legend()\n  plt.grid(True)\n  plt.axhline(0, color='black',linewidth=0.5)\n  plt.axvline(0, color='black',linewidth=0.5)\n  plt.show()\n\n# Example usage:\nvector1 = [4, -3] #try to change the values\nplot_unit_vector(vector1)\n\n\n\n\n\n\n\n\n\nIn a three-dimensional Cartesian coordinate system, there are three essential unit vectors: \\(\\mathbf{\\hat{i}} = \\langle 1, 0, 0 \\rangle\\), \\(\\mathbf{\\hat{j}} = \\langle 0, 1, 0 \\rangle\\), and \\(\\mathbf{\\hat{k}} = \\langle 0, 0, 1 \\rangle\\), which correspond to the x, y, and z axes, respectively. These standard unit vectors form the basis for expressing any vector in 3D space as a linear combination. Example: a vector \\(\\mathbf{v} = \\langle a, b, c \\rangle\\) can be expressed as \\(\\mathbf{v} = a\\mathbf{\\hat{i}} + b\\mathbf{\\hat{j}} + c\\mathbf{\\hat{k}}\\).\n\n\n\nA vector representing the position of a point relative to the origin.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_position_vector(vector, origin=(0, 0), label=\"Position Vector\"):\n  \"\"\"Plots a position vector in 2D space with prominent axes and origin.\n\n  Args:\n    vector: A NumPy array or a list representing the vector's components [x, y].\n    origin: A tuple representing the origin point (x, y) of the vector. Defaults to (0, 0).\n    label: A string representing the label for the vector in the plot.\n  \"\"\"\n  vector = np.array(vector)\n  origin = np.array(origin)\n\n  plt.quiver(*origin, *vector, angles='xy', scale_units='xy', scale=1, color='r', label=label)\n\n  # Set plot limits to include the origin and the vector's endpoint\n  max_val = max(abs(vector[0]), abs(vector[1]), abs(origin[0]), abs(origin[1]))\n  plt.xlim(-max_val - 1, max_val + 1)\n  plt.ylim(-max_val - 1, max_val + 1)\n\n  # Make origin (0, 0) prominent\n  plt.plot(0, 0, 'ko', markersize=8)\n\n  # Make x and y axes prominent\n  plt.axhline(0, color='black', linewidth=1.5)  # Thicker x-axis\n  plt.axvline(0, color='black', linewidth=1.5)  # Thicker y-axis\n\n  plt.xlabel(\"X-axis\")\n  plt.ylabel(\"Y-axis\")\n  plt.title(\"Position Vector\")\n  plt.grid(True)\n  plt.legend()\n  plt.show()\n\n# Example usage:\nvector1 = [3, 4]\nplot_position_vector(vector1, label=\"Vector A\")\n\n\n\n\n\n\n\n\n\n\n\n\nRepresents the change in position of an object.\n\n\nCode\ndef plot_displacement_vector(start, end, label=\"Displacement Vector\"):\n  \"\"\"Plots a displacement vector in 2D space with a prominent origin and axes.\n\n  Args:\n    start: A tuple or list representing the starting point (x, y).\n    end: A tuple or list representing the ending point (x, y).\n    label: A string representing the label for the vector in the plot.\n  \"\"\"\n  start = np.array(start)\n  end = np.array(end)\n  displacement = end - start\n\n  plt.quiver(*start, *displacement, angles='xy', scale_units='xy', scale=1, color='b', label=label)\n\n  # Set plot limits to include the start, end, and origin\n  all_points = np.concatenate((start, end, [0, 0])) #include origin for calculating max\n  max_val = max(abs(all_points))\n  plt.xlim(-max_val - 1, max_val + 1)\n  plt.ylim(-max_val - 1, max_val + 1)\n\n  # Make origin (0, 0) prominent\n  plt.plot(0, 0, 'ko', markersize=8)\n\n  # Make x and y axes prominent\n  plt.axhline(0, color='black', linewidth=1.5)\n  plt.axvline(0, color='black', linewidth=1.5)\n\n  plt.xlabel(\"X-axis\")\n  plt.ylabel(\"Y-axis\")\n  plt.title(\"Displacement Vector\")\n  plt.grid(True)\n  plt.legend()\n  plt.show()\n\nstart_point = (1, 2) #try to change the values of both start and end points\nend_point = (4, 6)\nplot_displacement_vector(start_point, end_point, label=\"vector A\")\n\n\n\n\n\n\n\n\n\non 3D space\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3d_displacement_vector(start, end, label=\"Displacement Vector\"):\n    \"\"\"Plots a 3D displacement vector with a prominent origin and axes.\n\n    Args:\n        start: A tuple or list representing the starting point (x, y, z).\n        end: A tuple or list representing the ending point (x, y, z).\n        label: A string representing the label for the vector.\n    \"\"\"\n    start = np.array(start)\n    end = np.array(end)\n    displacement = end - start\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.quiver(*start, *displacement, color='b', arrow_length_ratio=0.1, label=label)\n\n    # Set plot limits to include the start, end, and origin\n    all_points = np.concatenate((start, end, [0, 0, 0]))\n    max_val = max(abs(all_points))\n    ax.set_xlim([-max_val - 1, max_val + 1])\n    ax.set_ylim([-max_val - 1, max_val + 1])\n    ax.set_zlim([-max_val - 1, max_val + 1])\n\n    # Make origin (0, 0, 0) prominent\n    ax.scatter(0, 0, 0, color='k', s=50)\n\n    # Make x, y, and z axes prominent\n    ax.plot([-max_val - 1, max_val + 1], [0, 0], [0, 0], color='black', linewidth=1.5)  # x-axis\n    ax.plot([0, 0], [-max_val - 1, max_val + 1], [0, 0], color='black', linewidth=1.5)  # y-axis\n    ax.plot([0, 0], [0, 0], [-max_val - 1, max_val + 1], color='black', linewidth=1.5)  # z-axis\n\n    ax.set_xlabel(\"X-axis\")\n    ax.set_ylabel(\"Y-axis\")\n    ax.set_zlabel(\"Z-axis\")\n    ax.set_title(\"3D Displacement Vector\")\n    ax.legend()\n    plt.show()\n\n# Example usage:\n\nstart_point1 = (0,0,0)\nend_point1 = (1,4,3)\nplot_3d_displacement_vector(start_point1, end_point1, label = \"Displacement A\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollinear vectors are vectors that are parallel to the same line, regardless of their magnitude or direction. This means they can point in the same direction, opposite directions, or have different lengths. A crucial condition for two vectors to be collinear is that one vector must be a scalar multiple of the other. In other words, if vectors a and b are collinear, then there exists a scalar ‘k’ such that a = kb. In three dimensional space, another way to determine if vectors are collinear, is to take the cross product of the two vectors. If the cross product equals the zero vector, then the vectors are collinear.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef are_vectors_collinear(vectors, visualize=True):\n    \"\"\"\n    Check if all n-dimensional vectors in the input list are collinear and visualize them.\n    \n    Args:\n        vectors: List of numpy arrays or lists representing vectors.\n                 All vectors must have the same dimension.\n        visualize: Boolean to determine whether to create a visualization.\n    \n    Returns:\n        bool: True if all vectors are collinear, False otherwise.\n    \"\"\"\n    # Convert all vectors to numpy arrays if they aren't already\n    vectors = [np.array(v, dtype=float) for v in vectors]\n    \n    # Check that all vectors have the same dimension\n    dimensions = [v.shape[0] for v in vectors]\n    if len(set(dimensions)) &gt; 1:\n        raise ValueError(\"All vectors must have the same dimension\")\n    \n    dimension = dimensions[0]\n    if dimension not in [2, 3] and visualize:\n        print(f\"Warning: Cannot visualize {dimension}-dimensional vectors. Visualization skipped.\")\n        visualize = False\n    \n    # If there are fewer than 2 vectors, they are trivially collinear\n    if len(vectors) &lt; 2:\n        result = True\n    else:\n        # Find the first non-zero vector to use as reference\n        ref_vector = None\n        for v in vectors:\n            if np.any(v != 0):  # Check if vector is not all zeros\n                ref_vector = v\n                break\n        \n        # If all vectors are zero vectors, they are collinear\n        if ref_vector is None:\n            result = True\n        else:\n            # Calculate the magnitude of the reference vector\n            ref_magnitude = np.linalg.norm(ref_vector)\n            \n            # Check if each vector is a scalar multiple of the reference vector\n            result = True\n            for v in vectors:\n                # Skip zero vectors as they're collinear with any other vector\n                if np.all(v == 0):\n                    continue\n                    \n                # Calculate the magnitude of the current vector\n                v_magnitude = np.linalg.norm(v)\n                \n                # Calculate the cosine of the angle between vectors using dot product\n                cos_angle = np.dot(ref_vector, v) / (ref_magnitude * v_magnitude)\n                \n                # Allow for floating point errors with np.isclose\n                if not np.isclose(abs(cos_angle), 1.0):\n                    result = False\n                    break\n    \n    # Create a visualization if requested and vectors are 2D or 3D\n    if visualize:\n        plot_vectors(vectors, result)\n    \n    return result\n\ndef plot_vectors(vectors, are_collinear):\n    \"\"\"\n    Plot the vectors in 2D or 3D space.\n    \n    Args:\n        vectors: List of numpy arrays representing vectors.\n        are_collinear: Boolean indicating whether the vectors are collinear.\n    \"\"\"\n    # Determine dimensionality\n    dim = vectors[0].shape[0]\n    \n    # Set up colors for different vectors\n    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n    \n    # Create a figure\n    fig = plt.figure(figsize=(10, 8))\n    \n    # Choose between 2D and 3D plotting\n    if dim == 2:\n        ax = fig.add_subplot(111)\n        \n        # Plot each vector\n        for i, v in enumerate(vectors):\n            color = colors[i % len(colors)]\n            # Plot the vector from origin\n            ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color=color, \n                     label=f'Vector {i+1} {v}')\n            \n        # Set plot limits to ensure all vectors are visible\n        max_magnitude = max([np.linalg.norm(v) for v in vectors if np.any(v != 0)], default=1)\n        lim = max_magnitude * 1.2  # Add some padding\n        ax.set_xlim([-lim, lim])\n        ax.set_ylim([-lim, lim])\n        \n        # Add features with more prominent axes\n        ax.grid(True)\n        ax.axhline(y=0, color='k', linestyle='-', alpha=0.7, linewidth=2)\n        ax.axvline(x=0, color='k', linestyle='-', alpha=0.7, linewidth=2)\n        ax.set_xlabel('X', fontsize=14, fontweight='bold')\n        ax.set_ylabel('Y', fontsize=14, fontweight='bold')\n        \n        # Add tick marks with larger font\n        ax.tick_params(axis='both', which='major', labelsize=12)\n        \n    elif dim == 3:\n        ax = fig.add_subplot(111, projection='3d')\n        \n        # Plot each vector\n        for i, v in enumerate(vectors):\n            color = colors[i % len(colors)]\n            # Plot the vector from origin\n            ax.quiver(0, 0, 0, v[0], v[1], v[2], color=color, label=f'Vector {i+1} {v}')\n            \n        # Set plot limits\n        max_magnitude = max([np.linalg.norm(v) for v in vectors if np.any(v != 0)], default=1)\n        lim = max_magnitude * 1.2  # Add some padding\n        ax.set_xlim([-lim, lim])\n        ax.set_ylim([-lim, lim])\n        ax.set_zlim([-lim, lim])\n        \n        # Add features with more prominent axes\n        ax.plot([-max_magnitude - 1, max_magnitude + 1], [0, 0], [0, 0], color='black', linewidth=1.5)  # x-axis\n        ax.plot([0, 0], [-max_magnitude - 1, max_magnitude + 1], [0, 0], color='black', linewidth=1.5)  # y-axis\n        ax.plot([0, 0], [0, 0], [-max_magnitude - 1, max_magnitude + 1], color='black', linewidth=1.5)  # z-axis\n        ax.set_xlabel('X', fontsize=14, fontweight='bold')\n        ax.set_ylabel('Y', fontsize=14, fontweight='bold')\n        ax.set_zlabel('Z', fontsize=14, fontweight='bold')\n        \n        # Add tick marks with larger font\n        ax.tick_params(axis='both', which='major', labelsize=12)\n        \n        # Make the pane and grid more visible\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n        ax.xaxis.pane.set_edgecolor('black')\n        ax.yaxis.pane.set_edgecolor('black')\n        ax.zaxis.pane.set_edgecolor('black')\n        ax.grid(True, linestyle='-', linewidth=0.8, alpha=0.6)\n    \n    # Add title based on collinearity result with larger font\n    if are_collinear:\n        title = \"Vectors are Collinear\"\n    else:\n        title = \"Vectors are NOT Collinear\"\n    plt.title(title, fontsize=16, fontweight='bold')\n    \n    # Add legend\n    plt.legend()\n    \n    # Show plot\n    plt.tight_layout()\n    plt.show()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example 1: Collinear vectors (2D)\n    print(\"\\nExample 1: 2D Collinear Vectors\")\n    v1 = [1, 2]\n    v2 = [2, 4] \n    v3 = [-3, -6]\n    result = are_vectors_collinear([v1, v2, v3])\n    print(f\"Are vectors collinear? {result}\")\n    \n    # Example 2: Collinear vectors (3D)\n    print(\"\\nExample 2: 3D Collinear Vectors\")\n    v4 = [1, 2, 3]\n    v5 = [2, 4, 6]\n    v6 = [-0.5, -1, -1.5]\n    result = are_vectors_collinear([v4, v5, v6])\n    print(f\"Are vectors collinear? {result}\")\n\n\n\nExample 1: 2D Collinear Vectors\n\n\n\n\n\n\n\n\n\nAre vectors collinear? True\n\nExample 2: 3D Collinear Vectors\n\n\n\n\n\n\n\n\n\nAre vectors collinear? True\n\n\n\n\n\nVectors that lie in the same plane. They are a set of vectors that lie in the same plane. In three-dimensional space, any two vectors are inherently coplanar, as they define a plane. However, the concept becomes more relevant when dealing with three or more vectors. A key characteristic of coplanar vectors is their linear dependence; three or more vectors are coplanar if and only if one of the vectors can be expressed as a linear combination of the others. Mathematically, given vectors a, b, and c, their coplanarity can be determined by calculating the determinant of the matrix formed by their components; a zero determinant confirms coplanarity.\nExample\n\nConsider the vectors \\(\\vec{a} = \\hat{i} + \\hat{j}\\), \\(\\vec{b} = \\hat{j} + \\hat{k}\\), and \\(\\vec{c} = \\hat{i} + 2\\hat{j} + \\hat{k}\\).\nTo check if they are coplanar, we can calculate the scalar triple product:\n\n\\[\n\\begin{vmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 2 & 1 \\end{vmatrix} = 1(1 - 2) - 1(0 - 1) + 0(0 - 1) = -1 + 1 + 0 = 0\n\\]\n\nSince the scalar triple product is zero, the vectors \\(\\vec{a}\\), \\(\\vec{b}\\), and \\(\\vec{c}\\) are coplanar.\n\n\n\nCode\ndef are_coplanar(vectors):\n    \"\"\"\n    Checks if a list of vectors are coplanar using the scalar triple product.\n\n    Args:\n        vectors: A list of NumPy arrays representing vectors.\n\n    Returns:\n        True if the vectors are coplanar, False otherwise.\n    \"\"\"\n    if len(vectors) &lt; 3:\n        # Any two vectors are always coplanar.\n        return True\n\n    if len(vectors) &gt; 3:\n        # Check if a vector is a linear combination of the previous two\n        for i in range(2, len(vectors)):\n            a = vectors[0]\n            b = vectors[1]\n            c = vectors[i]\n            scalar_triple_product = np.dot(a, np.cross(b, c))\n            if scalar_triple_product != 0:\n                return False\n        return True\n\n    a, b, c = vectors[0], vectors[1], vectors[2]\n    scalar_triple_product = np.dot(a, np.cross(b, c))\n    return scalar_triple_product == 0\n\ndef plot_vectors(vectors, coplanar):\n    \"\"\"\n    Plots the given vectors in 3D space.\n\n    Args:\n        vectors: A list of NumPy arrays representing vectors.\n        coplanar: Boolean indicating if the vectors are coplanar.\n    \"\"\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    origin = np.array([0, 0, 0])\n    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']  # Colors for the vectors\n\n    for i, v in enumerate(vectors):\n        ax.quiver(*origin, *v, color=colors[i % len(colors)], arrow_length_ratio=0.1)\n        ax.text(*v, f'v{i+1}', color='black')\n\n    max_range = np.max(np.abs(np.concatenate(vectors))) * 1.2  # Adjust plot limits\n    ax.set_xlim([-max_range, max_range])\n    ax.set_ylim([-max_range, max_range])\n    ax.set_zlim([-max_range, max_range])\n\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_title(f'Vectors (Coplanar: {coplanar})')\n    plt.show()\n\n\nvector1 = np.array([1, 1, 0])\nvector2 = np.array([0, 1, 1])\nvector3 = np.array([1, 2, 1])\nvector4 = np.array([2, 2, 0]) #linear combination\n\nvectors_coplanar = [vector1, vector2, vector3]\nvectors_coplanar2 = [vector1, vector2, vector4]\nvectors_non_coplanar = [vector1, vector2, np.array([1, 0, 0])]\n\n# you can check every combinations of vector and check if they are coplanar or not\nvectors_more_than_3 = [np.array([1,1,0]), np.array([0,1,1]), np.array([1,2,1]), np.array([2,3,1])]\ncoplanar4 = are_coplanar(vectors_more_than_3)\nplot_vectors(vectors_more_than_3, coplanar4)\n\n\n\n\n\n\n\n\n\n\n\n\nVectors that have the same magnitude and direction.\n\n\n\nVectors with the same magnitude but opposite direction.\n\n\n\n\n\n\nA vector that is independent of its initial position. It is a vector whose point of application is not fixed but magnitude and direction. Same vector can become free or fixed depending on scenario. Like if a force F is applied on a table which has rotational and translational motion. If we are Interested in Translational motion then we consider force F as free vector because we can move it to the centre of mass of the table in solving the problem.\n\n\nCode\n%%manim -qm -v WARNING FreeVectorScene\n\nclass FreeVectorScene(Scene):\n    def construct(self):\n        vector = np.array([2, 1, 0])\n        arrow_color = BLUE\n\n        positions = [\n            np.array([-3, -1, 0]),\n            np.array([-1, 1, 0]),\n            np.array([1, -2, 0]),\n            np.array([3, 1, 0])\n        ]\n\n        arrows = [Arrow(start=pos, end=pos + vector, color=arrow_color) for pos in positions]\n        labels = [MathTex(r\"\\vec{v}\").next_to(arrow, UP) for arrow in arrows]\n\n        for arrow, label in zip(arrows, labels):\n            self.play(GrowArrow(arrow), Write(label))\n            self.wait(0.5)\n\n        self.wait(2)\n\n\n\nManim Community v0.19.0\n\n\n\n\n                                                                          \n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\n\nA vector that is defined with a fixed starting point.\nExample * Displacement Vector: An arrow showing how far and in what direction something moved from a starting point. * Position Vector: An arrow showing the location of a point relative to a reference point. * Moment of force: An arrow representing the turning effect of a force around a specific point.\n\n\nCode\n%%manim -qm -v WARNING BoundVector\nclass BoundVector(Scene):\n    def construct(self):\n        # Define the origin point\n        origin = Dot(ORIGIN, color=WHITE)\n        origin_label = MathTex(\"origin\").next_to(origin, DOWN)\n        \n        # Define the bound vector\n        vector = Arrow(ORIGIN, [2, 1, 0], buff=0, color=BLUE)\n        vector_label = MathTex(r\"\\vec{v}\").next_to(vector, UP)\n        \n        # Animate the elements\n        self.play(FadeIn(origin), Write(origin_label))\n        self.play(GrowArrow(vector), Write(vector_label))\n        \n        # Hold the final state\n        self.wait(2)\n\n\nManim Community v0.19.0\n\n\n\n\n                                                                                     \n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\n\nA vector that can be moved along its line of action without changing its effect. It combines a vector quantity (like force or moment) with a line of application (or line of action).\n\nExamples: Forces acting on a rigid body are often represented as sliding vectors because their effect (e.g., causing translation or rotation) is independent of the specific point of application along the line of force\n\n\nCode\n%%manim -qm -v WARNING SlidingVector\n\nclass SlidingVector(Scene):\n    def construct(self):\n        # Define the rigid body (a simple rectangle)\n        body = Rectangle(width=2, height=1, color=WHITE).move_to(ORIGIN)\n        \n        # Define force vectors\n        force1 = Arrow(start=LEFT, end=LEFT + RIGHT, color=RED, buff=0).shift(UP * 0.5)\n        force2 = Arrow(start=RIGHT, end=RIGHT + LEFT, color=GREEN, buff=0).shift(DOWN * 0.5)\n        \n        # Labels for forces\n        label1 = MathTex(r\"\\vec{F}_1\").next_to(force1, UP)\n        label2 = MathTex(r\"\\vec{F}_2\").next_to(force2, DOWN)\n        \n        # Grouping elements\n        force_group = VGroup(body, force1, force2, label1, label2)\n        \n        # Animate the forces acting on the body\n        self.play(Create(body))\n        self.play(Create(force1), Create(force2))\n        \n        # Apply movement (simulate body motion due to forces)\n        self.play(force_group.animate.shift(RIGHT * 2 + UP * 0.5), run_time=2)\n        \n        # Hold position\n        self.wait(1)\n\n\n\nManim Community v0.19.0\n\n\n\n\n                                                                                             \n\n\n\n \n Your browser does not support the video tag.\n \n\n\nConsider two equal and opposite forces acting on a rigid body along the same line—for instance, pulling at the two opposite ends of a rigid rod. Since these forces are aligned, their effects cancel out, meaning the rod remains in equilibrium.\nThis demonstrates an important principle: if a force is slid along its own line of action, its overall effect on the rigid body remains unchanged. However, if the point of application of the force is moved off this line—while keeping its magnitude, direction, and sense unchanged—the force’s effect on the rigid body will change. This is because forces applied at different points can introduce rotational effects (torques), which influence how the body moves or rotates.\n\n\n\n\n\n\nRepresents the velocity of an object in a specific direction.\n\n\n\nRepresents the rate of change of velocity.\n\n\n\nRepresents the force applied to an object in a specific direction.\n\n\n\nRepresents mass times velocity, indicating the motion of an object.\n\n\n\nRepresents the electric field intensity at a point.\n\n\n\nRepresents the direction and strength of a magnetic field.\n\n\n\n\n\n\nA vector represented as a single row of elements.\nExample: \\[\n\\mathbf{v} = \\begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\end{bmatrix}\n\\] ##### Column Vector A vector represented as a single column of elements.  Example: \\[\n\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}\n\\]\n\n\n\n\n\n\nVectors that remain in the same direction after a linear transformation.\n\\[\nA \\mathbf{v} = \\lambda \\mathbf{v}\n\\]\nwhere: - $ A $ is a square matrix, - $ $ is an eigenvalue, - $ $ is the corresponding eigenvector.\n\n\nCode\n%%manim -qm -v WARNING EigenvectorExample\n\nclass EigenvectorExample(Scene):\n    def construct(self):\n        # Set up axes\n        axes = Axes(\n            x_range=[-1, 3],\n            y_range=[-1, 3],\n            tips=True,\n            axis_config={\"include_numbers\": False},\n        ).add_coordinates()\n\n        self.play(Create(axes))\n\n        # Original vector x = [1, 1]\n        vec_x = Arrow(\n            start=axes.c2p(0, 0),\n            end=axes.c2p(1, 1),\n            buff=0,\n            color=BLUE\n        )\n        x_label = MathTex(r\"\\vec{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\").next_to(vec_x, LEFT)\n\n        self.play(GrowArrow(vec_x), Write(x_label))\n        self.wait()\n\n        # Transformed vector A x = [2, 2]\n        vec_Ax = Arrow(\n            start=axes.c2p(0, 0),\n            end=axes.c2p(2, 2),\n            buff=0,\n            color=RED\n        )\n\n        # Animate the scaling\n        self.play(Transform(vec_x, vec_Ax), run_time=2)\n        self.wait()\n\n        # Equation and lambda\n        equation = MathTex(\n            r\"A\\vec{x} = \\lambda \\vec{x} = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\"\n        ).to_edge(UP)\n        lambda_val = MathTex(r\"\\lambda = 2\").next_to(equation, DOWN)\n\n        self.play(Write(equation), Write(lambda_val))\n        self.wait()\n\n\nManim Community v0.19.0\n\n\n\n\n                                                                                                                                                               \n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\n\nRepresents the rate and direction of change of a scalar field.\nExample: Temperature Distribution is an example of gradient vector, with the equation\n\\[\n\\phi(x, y, z) = \\text{temperature at } (x, y, z)\n\\]\nThen:\n\\[\n\\vec{F}(x, y, z) = \\nabla \\phi = \\left( \\frac{\\partial \\phi}{\\partial x}, \\frac{\\partial \\phi}{\\partial y}, \\frac{\\partial \\phi}{\\partial z} \\right)\n\\]\nThis gives you the direction of fastest increase in temperature.\n\n\nCode\n# Define the temperature scalar field: a Gaussian \"heat source\"\ndef temperature(x, y):\n    return np.exp(-((x - 1.5)**2 + (y - 1.5)**2)) + 0.5 * np.exp(-((x + 1.5)**2 + (y + 1.5)**2))\n\n# Create a grid of (x, y) points\nx = np.linspace(-5, 5, 40)\ny = np.linspace(-5, 5, 40)\nX, Y = np.meshgrid(x, y)\n\n# Evaluate the scalar temperature field\nZ = temperature(X, Y)\n\n# Compute the gradient (∇T)\ndT_dx, dT_dy = np.gradient(Z, x, y)\n\n# Plotting\nplt.figure(figsize=(8, 6))\nplt.contourf(X, Y, Z, cmap='plasma', levels=30)  # Temperature map\nplt.colorbar(label='Temperature')\n\n# Overlay gradient vectors\nplt.quiver(X, Y, dT_dx, dT_dy, color='white', scale=50, width=0.003)\n\nplt.title(\"Temperature Field and Its Gradient\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nA vector perpendicular to a surface or curve.\n\n\n\nA vector that touches a curve at a single point and points in the direction of the curve.\n\n\n\n\n\n\n\n\\[\n   \\mathbf{A} + \\mathbf{B} = (A_x + B_x, A_y + B_y, A_z + B_z)\n   \\]\n- Performed component-wise.\n\n\n\n\\[\n   \\mathbf{A} - \\mathbf{B} = (A_x - B_x, A_y - B_y, A_z - B_z)\n   \\]\n- Similar to addition but subtracts components.\n\n\n\n\\[\n   k\\mathbf{A} = (kA_x, kA_y, kA_z)\n   \\]\n- Scales the vector by a constant factor.\n\n\n\n\\[\n   \\mathbf{A} \\cdot \\mathbf{B} = A_x B_x + A_y B_y + A_z B_z\n   \\]\n- Can also be expressed as:\n\\[\n   \\mathbf{A} \\cdot \\mathbf{B} = |\\mathbf{A}| |\\mathbf{B}| \\cos\\theta\n   \\]\n- Produces a scalar value.\n- Used to determine the angle between vectors.\n\n\n\n\\[\n   \\mathbf{A} \\times \\mathbf{B} =\n   \\begin{vmatrix}\n   \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n   A_x & A_y & A_z \\\\\n   B_x & B_y & B_z\n   \\end{vmatrix}\n   \\]\n- Produces a new vector perpendicular to both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\).\n- Magnitude:\n\\[\n   |\\mathbf{A} \\times \\mathbf{B}| = |\\mathbf{A}| |\\mathbf{B}| \\sin\\theta\n   \\]\n- Used in torque, angular momentum, and physics applications.\n\n\n\n\nScalar projection:\n\\[\n   \\text{proj}_{\\mathbf{B}} \\mathbf{A} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{|\\mathbf{B}|}\n   \\]\n\nVector projection:\n\\[\n   \\text{Proj}_{\\mathbf{B}} \\mathbf{A} = \\left( \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{|\\mathbf{B}|^2} \\right) \\mathbf{B}\n   \\]\n\n\n\n\n\\[\n   |\\mathbf{A}| = \\sqrt{A_x^2 + A_y^2 + A_z^2}\n   \\]\n- Represents the length of the vector.\n\n\n\n\\[\n   \\hat{\\mathbf{A}} = \\frac{\\mathbf{A}}{|\\mathbf{A}|}\n   \\]\n- A vector of magnitude 1 in the same direction as \\(\\mathbf{A}\\).\n\n\n\n\\[\n   \\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z} \\right)\n   \\]\n- Represents the rate of change of a scalar field.\n\n\n\n\\[\n   \\nabla \\cdot \\mathbf{A} = \\frac{\\partial A_x}{\\partial x} + \\frac{\\partial A_y}{\\partial y} + \\frac{\\partial A_z}{\\partial z}\n   \\]\n- Measures the “spreading out” of a vector field.\n\n\n\n\\[\n   \\nabla \\times \\mathbf{A} =\n   \\begin{vmatrix}\n   \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n   \\frac{\\partial}{\\partial x} & \\frac{\\partial}{\\partial y} & \\frac{\\partial}{\\partial z} \\\\\n   A_x & A_y & A_z\n   \\end{vmatrix}\n   \\]\n- Measures the rotational tendency of a vector field.\n\nLaplacian\n\\[\n   \\nabla^2 f = \\nabla \\cdot \\nabla f\n   \\]\n\n\n\nA second-order differential operator that measures the rate of change of a function’s gradient.\n\n\n\n\n\n\nRotation of Vectors (Matrix multiplication)\n\n\n\nA vector \\(\\mathbf{A}\\) can be rotated using a transformation matrix.\n\n\nVector Integration\n\n\n\nUsed to compute work done by a force field.\n\n\nVector Differentiation\n\n\n\nMeasures the rate of change of vector functions.\n\n\n\n\nA vector space (or linear space) is a fundamental concept in linear algebra and is the set of vectors that can be added together and multiplied (“scaled”) by numbers, called scalars, in such a way that certain properties hold.\nA vector space over a field $ F $ is a set $ V $ equipped with two operations: - Vector addition: A binary operation $ + : V V V $ that combines two vectors to produce another vector. - Scalar multiplication: A binary operation $ : F V V $ that combines a scalar (an element from field $ F $) and a vector from $ V $, producing another vector.\nThe vector space must satisfy the following properties (axioms):\n\n\nLet $ V $ be a set of vectors, and $ F $ be a field of scalars. For any vectors $ u, v, w V $ and scalars $ a, b F $, the following properties must hold:\n\nClosure under addition: For any $ u, v V $, $ u + v V $.\nClosure under scalar multiplication: For any $ a F $ and $ v V $, $ a v V $.\nCommutativity of addition: $ u + v = v + u $ for all $ u, v V $.\nAssociativity of addition: $ (u + v) + w = u + (v + w) $ for all $ u, v, w V $.\nExistence of additive identity: There exists a zero vector $ 0 V $ such that $ v + 0 = v $ for all $ v V $.\nExistence of additive inverses: For every vector $ v V $, there exists $ -v V $ such that $ v + (-v) = 0 $.\nDistributivity of scalar multiplication over vector addition: $ a (v + w) = a v + a w $ for all $ a F $ and $ v, w V $.\nDistributivity of scalar multiplication over scalar addition: $ (a + b) v = a v + b v $ for all $ a, b F $ and $ v V $.\nCompatibility of scalar multiplication with field multiplication: $ a (b v) = (a b) v $ for all $ a, b F $ and $ v V $.\nExistence of multiplicative identity: $ 1 v = v $ for all $ v V $, where $ 1 $ is the multiplicative identity in the field $ F $.\n\n\n\n\n\nEuclidean Space $ ^n $: The set of all $ n $-dimensional vectors with real-number components.\nFunction Spaces: The set of all continuous functions defined on a closed interval $ [a, b] $ is a vector space.\nPolynomial Spaces: The set of all polynomials with real coefficients, denoted $ [x] $, forms a vector space.\nMatrices: The set of all $ m n $ matrices with real entries is a vector space.\n\n\n\n\n\n\nA subspace is a subset of a vector space that is itself a vector space. For a subset $ W $ of a vector space $ V $ to be a subspace, it must satisfy the following: - $ W $ is closed under vector addition. - $ W $ is closed under scalar multiplication. - $ W $ contains the zero vector.\n\n\n\nA set of vectors $ {v_1, v_2, , v_k} $ in a vector space $ V $ is said to be linearly independent if the only solution to the equation \\[\na_1 v_1 + a_2 v_2 + \\dots + a_k v_k = 0\n\\] is $ a_1 = a_2 = = a_k = 0 $.\nIf there is a nontrivial solution (i.e., some coefficients are nonzero), the vectors are said to be linearly dependent.\n\n\n\nThe span of a set of vectors $ {v_1, v_2, , v_k} $ is the set of all possible linear combinations of those vectors: \\[\n\\text{span}(v_1, v_2, \\dots, v_k) = \\{ a_1 v_1 + a_2 v_2 + \\dots + a_k v_k \\mid a_1, a_2, \\dots, a_k \\in F \\}\n\\] The span forms a subspace of $ V $.\n\n\n\nA basis of a vector space $ V $ is a set of linearly independent vectors in $ V $ that span $ V $. Every vector in $ V $ can be uniquely written as a linear combination of the basis vectors.\n\n\n\nThe dimension of a vector space $ V $ is the number of vectors in a basis of $ V $. A vector space can be finite-dimensional or infinite-dimensional, depending on the number of basis vectors.\n\n\n\nOnce a basis $ {v_1, v_2, , v_n} $ is chosen, any vector $ v V $ can be written uniquely as: \\[\nv = a_1 v_1 + a_2 v_2 + \\dots + a_n v_n\n\\] The scalars $ a_1, a_2, , a_n $ are called the coordinates of $ v $ relative to the basis $ {v_1, v_2, , v_n} $.",
    "crumbs": [
      "Linear Algebra",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/01.Vectors_and_Matrices.html#vectors",
    "href": "Notebooks/Linear_Algebra/01.Vectors_and_Matrices.html#vectors",
    "title": "Vectors and Matrices",
    "section": "",
    "text": "A vector is a mathematical object that possesses both magnitude (or length) and direction. This distinguishes it from a scalar, which has only magnitude. Examples: velocity (speed and direction), force (strength and direction), displacement (distance and direction).\nMagnitude: The magnitude of a vector is its length or size. It’s a scalar quantity and is always non-negative. We denote the magnitude of a vector \\(\\vec{v}\\) as \\(||\\vec{v}||\\) or simply \\(v\\). Direction: The direction of a vector indicates the line of action and sense of the vector. It can be specified using angles relative to a reference axis or by comparing it to another vector.\n\n\n2D Representation: * In a two-dimensional plane (like the xy-plane), a vector is typically represented as an arrow. * The tail of the arrow is the starting point (initial point), and the head of the arrow is the ending point (terminal point). * The length of the arrow represents the magnitude of the vector. * The orientation of the arrow represents the direction of the vector. * Example: A vector representing a wind velocity of 10 m/s blowing northeast.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport math\nfrom IPython.display import display, HTML\nimport manim as mn\nfrom manim import *\nimport plotly.graph_objects as go\nconfig.media_embed = True\n\n\n\n\nCode\ndef plot_2d_vectors(vectors, colors=None):\n    fig, ax = plt.subplots()\n    ax.axhline(0, color='black', linewidth=0.5)\n    ax.axvline(0, color='black', linewidth=0.5)\n    ax.grid()\n    \n    for i, vec in enumerate(vectors):\n        color = colors[i] if colors else 'blue'\n        ax.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, color=color)\n    \n    max_val = np.max(np.abs(vectors)) + 1\n    ax.set_xlim(-max_val, max_val)\n    ax.set_ylim(-max_val, max_val)\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_title('2D Vector Representation')\n    plt.show()\n\nvectors_2d = [np.array([3, 4]), np.array([-2, 5])]\nplot_2d_vectors(vectors_2d, colors=['red', 'green'])\n\n\n\n\n\n\n\n\n\n3D Representation: * In three-dimensional space (like the xyz-space), vectors are still represented as arrows, but now they exist in a three-dimensional coordinate system. * The concepts of magnitude and direction remain the same. * Example: A vector representing the force acting on an object in 3D space.\n\n\nCode\ndef plot_3d_vectors(vectors, colors=None):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.quiver(0, 0, 0, *zip(*vectors), color=colors if colors else 'blue')\n    \n    max_val = np.max(np.abs(vectors)) + 1\n    ax.set_xlim(-max_val, max_val)\n    ax.set_ylim(-max_val, max_val)\n    ax.set_zlim(-max_val, max_val)\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_zlabel('Z-axis')\n    ax.set_title('3D Vector Representation')\n    plt.show()\n\nvectors_3d = [np.array([3, 4, 5]), np.array([-2, 5, 3])]\nplot_3d_vectors(vectors_3d, colors=['red', 'green'])\n\n\n\n\n\n\n\n\n\nn-dimension vector\nThe concept of vectors can be extended beyond 3d to any number of dimensions (n-dimensions). An n-dimensional vector has n components: \\(\\vec{v} = \\langle v_1, v_2, ..., v_n \\rangle\\). Example: Vectors are used to represent data points in high dimensional data sets.\nComponent Form of Vectors (e.g., \\(v=⟨v1​,v2​,v3​⟩\\)).\n\nComponent Form:\n\nVectors can be expressed in component form, which is particularly useful for mathematical operations.\nIn 2D, a vector \\(\\vec{v}\\) can be written as \\(\\vec{v} = \\langle v_1, v_2 \\rangle\\), where \\(v_1\\) and \\(v_2\\) are the components of the vector along the x-axis and y-axis, respectively.\nIn 3D, a vector \\(\\vec{v}\\) can be written as \\(\\vec{v} = \\langle v_1, v_2, v_3 \\rangle\\), where \\(v_1\\), \\(v_2\\), and \\(v_3\\) are the components along the x-axis, y-axis, and z-axis, respectively.\nThe components can be found by projecting the vector onto the coordinate axes.\n\n\nExample: a vector that moves an object 2 units in the x direction, 3 units in the y direction, and -1 units in the z direction would be represented as \\(\\vec{v} = \\langle 2, 3, -1 \\rangle\\).\n\nMagnitude in Component Form:\n\nThe magnitude of a vector in component form can be calculated using the Pythagorean theorem:\n\nIn 2D: \\(||\\vec{v}|| = \\sqrt{v_1^2 + v_2^2}\\)\nIn 3D: \\(||\\vec{v}|| = \\sqrt{v_1^2 + v_2^2 + v_3^2}\\)\n\n\n\n\n\n\n\n\n\n\nA vector with all components equal to zero, meaning it is a vector with zero magnitude and an undefined direction, often represented as (0, 0) in two dimensions or (0, 0, 0) in three dimensions\n\n\n\nA unit vector is fundamentally a vector whose magnitude, or length, is precisely one. Its primary function is to indicate direction within a given space. A common convention to denote that a vector is a unit vector is the use of the “hat” symbol, such as in \\(\\mathbf{\\hat{u}}\\). To obtain a unit vector \\(\\mathbf{\\hat{u}}\\) that shares the same direction as a given vector \\(\\mathbf{v}\\), one must divide the vector \\(\\mathbf{v}\\) by its magnitude, represented as \\(||\\mathbf{v}||\\). This process effectively scales the vector down to a length of one while preserving its original direction. The magnitude of a vector \\(\\mathbf{v} = \\langle v_1, v_2, v_3 \\rangle\\) is calculated using the formula \\(||\\mathbf{v}|| = \\sqrt{v_1^2 + v_2^2 + v_3^2}\\). Example:\n\n\nCode\ndef plot_unit_vector(vector):\n  \"\"\"\n  Plots a vector and its corresponding unit vector.\n\n  Args:\n    vector: A list or tuple representing the vector.\n  \"\"\"\n  magnitude = math.sqrt(sum(x**2 for x in vector))\n\n  if magnitude == 0:\n    print(\"Vector has zero magnitude, cannot plot unit vector.\")\n    return\n\n  unit_vector = [x / magnitude for x in vector]\n\n  plt.figure()\n  plt.quiver(0, 0, vector[0], vector[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Original Vector')\n  plt.quiver(0, 0, unit_vector[0], unit_vector[1], angles='xy', scale_units='xy', scale=1, color='red', label='Unit Vector')\n\n  # Set axis limits to accommodate both vectors\n  max_val = max(abs(vector[0]), abs(vector[1]), abs(unit_vector[0]), abs(unit_vector[1]))\n  plt.xlim([-max_val - 1, max_val + 1])\n  plt.ylim([-max_val - 1, max_val + 1])\n\n  plt.xlabel('X-axis')\n  plt.ylabel('Y-axis')\n  plt.title('Vector and Unit Vector')\n  plt.legend()\n  plt.grid(True)\n  plt.axhline(0, color='black',linewidth=0.5)\n  plt.axvline(0, color='black',linewidth=0.5)\n  plt.show()\n\n# Example usage:\nvector1 = [4, -3] #try to change the values\nplot_unit_vector(vector1)\n\n\n\n\n\n\n\n\n\nIn a three-dimensional Cartesian coordinate system, there are three essential unit vectors: \\(\\mathbf{\\hat{i}} = \\langle 1, 0, 0 \\rangle\\), \\(\\mathbf{\\hat{j}} = \\langle 0, 1, 0 \\rangle\\), and \\(\\mathbf{\\hat{k}} = \\langle 0, 0, 1 \\rangle\\), which correspond to the x, y, and z axes, respectively. These standard unit vectors form the basis for expressing any vector in 3D space as a linear combination. Example: a vector \\(\\mathbf{v} = \\langle a, b, c \\rangle\\) can be expressed as \\(\\mathbf{v} = a\\mathbf{\\hat{i}} + b\\mathbf{\\hat{j}} + c\\mathbf{\\hat{k}}\\).\n\n\n\nA vector representing the position of a point relative to the origin.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_position_vector(vector, origin=(0, 0), label=\"Position Vector\"):\n  \"\"\"Plots a position vector in 2D space with prominent axes and origin.\n\n  Args:\n    vector: A NumPy array or a list representing the vector's components [x, y].\n    origin: A tuple representing the origin point (x, y) of the vector. Defaults to (0, 0).\n    label: A string representing the label for the vector in the plot.\n  \"\"\"\n  vector = np.array(vector)\n  origin = np.array(origin)\n\n  plt.quiver(*origin, *vector, angles='xy', scale_units='xy', scale=1, color='r', label=label)\n\n  # Set plot limits to include the origin and the vector's endpoint\n  max_val = max(abs(vector[0]), abs(vector[1]), abs(origin[0]), abs(origin[1]))\n  plt.xlim(-max_val - 1, max_val + 1)\n  plt.ylim(-max_val - 1, max_val + 1)\n\n  # Make origin (0, 0) prominent\n  plt.plot(0, 0, 'ko', markersize=8)\n\n  # Make x and y axes prominent\n  plt.axhline(0, color='black', linewidth=1.5)  # Thicker x-axis\n  plt.axvline(0, color='black', linewidth=1.5)  # Thicker y-axis\n\n  plt.xlabel(\"X-axis\")\n  plt.ylabel(\"Y-axis\")\n  plt.title(\"Position Vector\")\n  plt.grid(True)\n  plt.legend()\n  plt.show()\n\n# Example usage:\nvector1 = [3, 4]\nplot_position_vector(vector1, label=\"Vector A\")\n\n\n\n\n\n\n\n\n\n\n\n\nRepresents the change in position of an object.\n\n\nCode\ndef plot_displacement_vector(start, end, label=\"Displacement Vector\"):\n  \"\"\"Plots a displacement vector in 2D space with a prominent origin and axes.\n\n  Args:\n    start: A tuple or list representing the starting point (x, y).\n    end: A tuple or list representing the ending point (x, y).\n    label: A string representing the label for the vector in the plot.\n  \"\"\"\n  start = np.array(start)\n  end = np.array(end)\n  displacement = end - start\n\n  plt.quiver(*start, *displacement, angles='xy', scale_units='xy', scale=1, color='b', label=label)\n\n  # Set plot limits to include the start, end, and origin\n  all_points = np.concatenate((start, end, [0, 0])) #include origin for calculating max\n  max_val = max(abs(all_points))\n  plt.xlim(-max_val - 1, max_val + 1)\n  plt.ylim(-max_val - 1, max_val + 1)\n\n  # Make origin (0, 0) prominent\n  plt.plot(0, 0, 'ko', markersize=8)\n\n  # Make x and y axes prominent\n  plt.axhline(0, color='black', linewidth=1.5)\n  plt.axvline(0, color='black', linewidth=1.5)\n\n  plt.xlabel(\"X-axis\")\n  plt.ylabel(\"Y-axis\")\n  plt.title(\"Displacement Vector\")\n  plt.grid(True)\n  plt.legend()\n  plt.show()\n\nstart_point = (1, 2) #try to change the values of both start and end points\nend_point = (4, 6)\nplot_displacement_vector(start_point, end_point, label=\"vector A\")\n\n\n\n\n\n\n\n\n\non 3D space\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3d_displacement_vector(start, end, label=\"Displacement Vector\"):\n    \"\"\"Plots a 3D displacement vector with a prominent origin and axes.\n\n    Args:\n        start: A tuple or list representing the starting point (x, y, z).\n        end: A tuple or list representing the ending point (x, y, z).\n        label: A string representing the label for the vector.\n    \"\"\"\n    start = np.array(start)\n    end = np.array(end)\n    displacement = end - start\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.quiver(*start, *displacement, color='b', arrow_length_ratio=0.1, label=label)\n\n    # Set plot limits to include the start, end, and origin\n    all_points = np.concatenate((start, end, [0, 0, 0]))\n    max_val = max(abs(all_points))\n    ax.set_xlim([-max_val - 1, max_val + 1])\n    ax.set_ylim([-max_val - 1, max_val + 1])\n    ax.set_zlim([-max_val - 1, max_val + 1])\n\n    # Make origin (0, 0, 0) prominent\n    ax.scatter(0, 0, 0, color='k', s=50)\n\n    # Make x, y, and z axes prominent\n    ax.plot([-max_val - 1, max_val + 1], [0, 0], [0, 0], color='black', linewidth=1.5)  # x-axis\n    ax.plot([0, 0], [-max_val - 1, max_val + 1], [0, 0], color='black', linewidth=1.5)  # y-axis\n    ax.plot([0, 0], [0, 0], [-max_val - 1, max_val + 1], color='black', linewidth=1.5)  # z-axis\n\n    ax.set_xlabel(\"X-axis\")\n    ax.set_ylabel(\"Y-axis\")\n    ax.set_zlabel(\"Z-axis\")\n    ax.set_title(\"3D Displacement Vector\")\n    ax.legend()\n    plt.show()\n\n# Example usage:\n\nstart_point1 = (0,0,0)\nend_point1 = (1,4,3)\nplot_3d_displacement_vector(start_point1, end_point1, label = \"Displacement A\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollinear vectors are vectors that are parallel to the same line, regardless of their magnitude or direction. This means they can point in the same direction, opposite directions, or have different lengths. A crucial condition for two vectors to be collinear is that one vector must be a scalar multiple of the other. In other words, if vectors a and b are collinear, then there exists a scalar ‘k’ such that a = kb. In three dimensional space, another way to determine if vectors are collinear, is to take the cross product of the two vectors. If the cross product equals the zero vector, then the vectors are collinear.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef are_vectors_collinear(vectors, visualize=True):\n    \"\"\"\n    Check if all n-dimensional vectors in the input list are collinear and visualize them.\n    \n    Args:\n        vectors: List of numpy arrays or lists representing vectors.\n                 All vectors must have the same dimension.\n        visualize: Boolean to determine whether to create a visualization.\n    \n    Returns:\n        bool: True if all vectors are collinear, False otherwise.\n    \"\"\"\n    # Convert all vectors to numpy arrays if they aren't already\n    vectors = [np.array(v, dtype=float) for v in vectors]\n    \n    # Check that all vectors have the same dimension\n    dimensions = [v.shape[0] for v in vectors]\n    if len(set(dimensions)) &gt; 1:\n        raise ValueError(\"All vectors must have the same dimension\")\n    \n    dimension = dimensions[0]\n    if dimension not in [2, 3] and visualize:\n        print(f\"Warning: Cannot visualize {dimension}-dimensional vectors. Visualization skipped.\")\n        visualize = False\n    \n    # If there are fewer than 2 vectors, they are trivially collinear\n    if len(vectors) &lt; 2:\n        result = True\n    else:\n        # Find the first non-zero vector to use as reference\n        ref_vector = None\n        for v in vectors:\n            if np.any(v != 0):  # Check if vector is not all zeros\n                ref_vector = v\n                break\n        \n        # If all vectors are zero vectors, they are collinear\n        if ref_vector is None:\n            result = True\n        else:\n            # Calculate the magnitude of the reference vector\n            ref_magnitude = np.linalg.norm(ref_vector)\n            \n            # Check if each vector is a scalar multiple of the reference vector\n            result = True\n            for v in vectors:\n                # Skip zero vectors as they're collinear with any other vector\n                if np.all(v == 0):\n                    continue\n                    \n                # Calculate the magnitude of the current vector\n                v_magnitude = np.linalg.norm(v)\n                \n                # Calculate the cosine of the angle between vectors using dot product\n                cos_angle = np.dot(ref_vector, v) / (ref_magnitude * v_magnitude)\n                \n                # Allow for floating point errors with np.isclose\n                if not np.isclose(abs(cos_angle), 1.0):\n                    result = False\n                    break\n    \n    # Create a visualization if requested and vectors are 2D or 3D\n    if visualize:\n        plot_vectors(vectors, result)\n    \n    return result\n\ndef plot_vectors(vectors, are_collinear):\n    \"\"\"\n    Plot the vectors in 2D or 3D space.\n    \n    Args:\n        vectors: List of numpy arrays representing vectors.\n        are_collinear: Boolean indicating whether the vectors are collinear.\n    \"\"\"\n    # Determine dimensionality\n    dim = vectors[0].shape[0]\n    \n    # Set up colors for different vectors\n    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n    \n    # Create a figure\n    fig = plt.figure(figsize=(10, 8))\n    \n    # Choose between 2D and 3D plotting\n    if dim == 2:\n        ax = fig.add_subplot(111)\n        \n        # Plot each vector\n        for i, v in enumerate(vectors):\n            color = colors[i % len(colors)]\n            # Plot the vector from origin\n            ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color=color, \n                     label=f'Vector {i+1} {v}')\n            \n        # Set plot limits to ensure all vectors are visible\n        max_magnitude = max([np.linalg.norm(v) for v in vectors if np.any(v != 0)], default=1)\n        lim = max_magnitude * 1.2  # Add some padding\n        ax.set_xlim([-lim, lim])\n        ax.set_ylim([-lim, lim])\n        \n        # Add features with more prominent axes\n        ax.grid(True)\n        ax.axhline(y=0, color='k', linestyle='-', alpha=0.7, linewidth=2)\n        ax.axvline(x=0, color='k', linestyle='-', alpha=0.7, linewidth=2)\n        ax.set_xlabel('X', fontsize=14, fontweight='bold')\n        ax.set_ylabel('Y', fontsize=14, fontweight='bold')\n        \n        # Add tick marks with larger font\n        ax.tick_params(axis='both', which='major', labelsize=12)\n        \n    elif dim == 3:\n        ax = fig.add_subplot(111, projection='3d')\n        \n        # Plot each vector\n        for i, v in enumerate(vectors):\n            color = colors[i % len(colors)]\n            # Plot the vector from origin\n            ax.quiver(0, 0, 0, v[0], v[1], v[2], color=color, label=f'Vector {i+1} {v}')\n            \n        # Set plot limits\n        max_magnitude = max([np.linalg.norm(v) for v in vectors if np.any(v != 0)], default=1)\n        lim = max_magnitude * 1.2  # Add some padding\n        ax.set_xlim([-lim, lim])\n        ax.set_ylim([-lim, lim])\n        ax.set_zlim([-lim, lim])\n        \n        # Add features with more prominent axes\n        ax.plot([-max_magnitude - 1, max_magnitude + 1], [0, 0], [0, 0], color='black', linewidth=1.5)  # x-axis\n        ax.plot([0, 0], [-max_magnitude - 1, max_magnitude + 1], [0, 0], color='black', linewidth=1.5)  # y-axis\n        ax.plot([0, 0], [0, 0], [-max_magnitude - 1, max_magnitude + 1], color='black', linewidth=1.5)  # z-axis\n        ax.set_xlabel('X', fontsize=14, fontweight='bold')\n        ax.set_ylabel('Y', fontsize=14, fontweight='bold')\n        ax.set_zlabel('Z', fontsize=14, fontweight='bold')\n        \n        # Add tick marks with larger font\n        ax.tick_params(axis='both', which='major', labelsize=12)\n        \n        # Make the pane and grid more visible\n        ax.xaxis.pane.fill = False\n        ax.yaxis.pane.fill = False\n        ax.zaxis.pane.fill = False\n        ax.xaxis.pane.set_edgecolor('black')\n        ax.yaxis.pane.set_edgecolor('black')\n        ax.zaxis.pane.set_edgecolor('black')\n        ax.grid(True, linestyle='-', linewidth=0.8, alpha=0.6)\n    \n    # Add title based on collinearity result with larger font\n    if are_collinear:\n        title = \"Vectors are Collinear\"\n    else:\n        title = \"Vectors are NOT Collinear\"\n    plt.title(title, fontsize=16, fontweight='bold')\n    \n    # Add legend\n    plt.legend()\n    \n    # Show plot\n    plt.tight_layout()\n    plt.show()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example 1: Collinear vectors (2D)\n    print(\"\\nExample 1: 2D Collinear Vectors\")\n    v1 = [1, 2]\n    v2 = [2, 4] \n    v3 = [-3, -6]\n    result = are_vectors_collinear([v1, v2, v3])\n    print(f\"Are vectors collinear? {result}\")\n    \n    # Example 2: Collinear vectors (3D)\n    print(\"\\nExample 2: 3D Collinear Vectors\")\n    v4 = [1, 2, 3]\n    v5 = [2, 4, 6]\n    v6 = [-0.5, -1, -1.5]\n    result = are_vectors_collinear([v4, v5, v6])\n    print(f\"Are vectors collinear? {result}\")\n\n\n\nExample 1: 2D Collinear Vectors\n\n\n\n\n\n\n\n\n\nAre vectors collinear? True\n\nExample 2: 3D Collinear Vectors\n\n\n\n\n\n\n\n\n\nAre vectors collinear? True\n\n\n\n\n\nVectors that lie in the same plane. They are a set of vectors that lie in the same plane. In three-dimensional space, any two vectors are inherently coplanar, as they define a plane. However, the concept becomes more relevant when dealing with three or more vectors. A key characteristic of coplanar vectors is their linear dependence; three or more vectors are coplanar if and only if one of the vectors can be expressed as a linear combination of the others. Mathematically, given vectors a, b, and c, their coplanarity can be determined by calculating the determinant of the matrix formed by their components; a zero determinant confirms coplanarity.\nExample\n\nConsider the vectors \\(\\vec{a} = \\hat{i} + \\hat{j}\\), \\(\\vec{b} = \\hat{j} + \\hat{k}\\), and \\(\\vec{c} = \\hat{i} + 2\\hat{j} + \\hat{k}\\).\nTo check if they are coplanar, we can calculate the scalar triple product:\n\n\\[\n\\begin{vmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 2 & 1 \\end{vmatrix} = 1(1 - 2) - 1(0 - 1) + 0(0 - 1) = -1 + 1 + 0 = 0\n\\]\n\nSince the scalar triple product is zero, the vectors \\(\\vec{a}\\), \\(\\vec{b}\\), and \\(\\vec{c}\\) are coplanar.\n\n\n\nCode\ndef are_coplanar(vectors):\n    \"\"\"\n    Checks if a list of vectors are coplanar using the scalar triple product.\n\n    Args:\n        vectors: A list of NumPy arrays representing vectors.\n\n    Returns:\n        True if the vectors are coplanar, False otherwise.\n    \"\"\"\n    if len(vectors) &lt; 3:\n        # Any two vectors are always coplanar.\n        return True\n\n    if len(vectors) &gt; 3:\n        # Check if a vector is a linear combination of the previous two\n        for i in range(2, len(vectors)):\n            a = vectors[0]\n            b = vectors[1]\n            c = vectors[i]\n            scalar_triple_product = np.dot(a, np.cross(b, c))\n            if scalar_triple_product != 0:\n                return False\n        return True\n\n    a, b, c = vectors[0], vectors[1], vectors[2]\n    scalar_triple_product = np.dot(a, np.cross(b, c))\n    return scalar_triple_product == 0\n\ndef plot_vectors(vectors, coplanar):\n    \"\"\"\n    Plots the given vectors in 3D space.\n\n    Args:\n        vectors: A list of NumPy arrays representing vectors.\n        coplanar: Boolean indicating if the vectors are coplanar.\n    \"\"\"\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    origin = np.array([0, 0, 0])\n    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']  # Colors for the vectors\n\n    for i, v in enumerate(vectors):\n        ax.quiver(*origin, *v, color=colors[i % len(colors)], arrow_length_ratio=0.1)\n        ax.text(*v, f'v{i+1}', color='black')\n\n    max_range = np.max(np.abs(np.concatenate(vectors))) * 1.2  # Adjust plot limits\n    ax.set_xlim([-max_range, max_range])\n    ax.set_ylim([-max_range, max_range])\n    ax.set_zlim([-max_range, max_range])\n\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_title(f'Vectors (Coplanar: {coplanar})')\n    plt.show()\n\n\nvector1 = np.array([1, 1, 0])\nvector2 = np.array([0, 1, 1])\nvector3 = np.array([1, 2, 1])\nvector4 = np.array([2, 2, 0]) #linear combination\n\nvectors_coplanar = [vector1, vector2, vector3]\nvectors_coplanar2 = [vector1, vector2, vector4]\nvectors_non_coplanar = [vector1, vector2, np.array([1, 0, 0])]\n\n# you can check every combinations of vector and check if they are coplanar or not\nvectors_more_than_3 = [np.array([1,1,0]), np.array([0,1,1]), np.array([1,2,1]), np.array([2,3,1])]\ncoplanar4 = are_coplanar(vectors_more_than_3)\nplot_vectors(vectors_more_than_3, coplanar4)\n\n\n\n\n\n\n\n\n\n\n\n\nVectors that have the same magnitude and direction.\n\n\n\nVectors with the same magnitude but opposite direction.\n\n\n\n\n\n\nA vector that is independent of its initial position. It is a vector whose point of application is not fixed but magnitude and direction. Same vector can become free or fixed depending on scenario. Like if a force F is applied on a table which has rotational and translational motion. If we are Interested in Translational motion then we consider force F as free vector because we can move it to the centre of mass of the table in solving the problem.\n\n\nCode\n%%manim -qm -v WARNING FreeVectorScene\n\nclass FreeVectorScene(Scene):\n    def construct(self):\n        vector = np.array([2, 1, 0])\n        arrow_color = BLUE\n\n        positions = [\n            np.array([-3, -1, 0]),\n            np.array([-1, 1, 0]),\n            np.array([1, -2, 0]),\n            np.array([3, 1, 0])\n        ]\n\n        arrows = [Arrow(start=pos, end=pos + vector, color=arrow_color) for pos in positions]\n        labels = [MathTex(r\"\\vec{v}\").next_to(arrow, UP) for arrow in arrows]\n\n        for arrow, label in zip(arrows, labels):\n            self.play(GrowArrow(arrow), Write(label))\n            self.wait(0.5)\n\n        self.wait(2)\n\n\n\nManim Community v0.19.0\n\n\n\n\n                                                                          \n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\n\nA vector that is defined with a fixed starting point.\nExample * Displacement Vector: An arrow showing how far and in what direction something moved from a starting point. * Position Vector: An arrow showing the location of a point relative to a reference point. * Moment of force: An arrow representing the turning effect of a force around a specific point.\n\n\nCode\n%%manim -qm -v WARNING BoundVector\nclass BoundVector(Scene):\n    def construct(self):\n        # Define the origin point\n        origin = Dot(ORIGIN, color=WHITE)\n        origin_label = MathTex(\"origin\").next_to(origin, DOWN)\n        \n        # Define the bound vector\n        vector = Arrow(ORIGIN, [2, 1, 0], buff=0, color=BLUE)\n        vector_label = MathTex(r\"\\vec{v}\").next_to(vector, UP)\n        \n        # Animate the elements\n        self.play(FadeIn(origin), Write(origin_label))\n        self.play(GrowArrow(vector), Write(vector_label))\n        \n        # Hold the final state\n        self.wait(2)\n\n\nManim Community v0.19.0\n\n\n\n\n                                                                                     \n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\n\nA vector that can be moved along its line of action without changing its effect. It combines a vector quantity (like force or moment) with a line of application (or line of action).\n\nExamples: Forces acting on a rigid body are often represented as sliding vectors because their effect (e.g., causing translation or rotation) is independent of the specific point of application along the line of force\n\n\nCode\n%%manim -qm -v WARNING SlidingVector\n\nclass SlidingVector(Scene):\n    def construct(self):\n        # Define the rigid body (a simple rectangle)\n        body = Rectangle(width=2, height=1, color=WHITE).move_to(ORIGIN)\n        \n        # Define force vectors\n        force1 = Arrow(start=LEFT, end=LEFT + RIGHT, color=RED, buff=0).shift(UP * 0.5)\n        force2 = Arrow(start=RIGHT, end=RIGHT + LEFT, color=GREEN, buff=0).shift(DOWN * 0.5)\n        \n        # Labels for forces\n        label1 = MathTex(r\"\\vec{F}_1\").next_to(force1, UP)\n        label2 = MathTex(r\"\\vec{F}_2\").next_to(force2, DOWN)\n        \n        # Grouping elements\n        force_group = VGroup(body, force1, force2, label1, label2)\n        \n        # Animate the forces acting on the body\n        self.play(Create(body))\n        self.play(Create(force1), Create(force2))\n        \n        # Apply movement (simulate body motion due to forces)\n        self.play(force_group.animate.shift(RIGHT * 2 + UP * 0.5), run_time=2)\n        \n        # Hold position\n        self.wait(1)\n\n\n\nManim Community v0.19.0\n\n\n\n\n                                                                                             \n\n\n\n \n Your browser does not support the video tag.\n \n\n\nConsider two equal and opposite forces acting on a rigid body along the same line—for instance, pulling at the two opposite ends of a rigid rod. Since these forces are aligned, their effects cancel out, meaning the rod remains in equilibrium.\nThis demonstrates an important principle: if a force is slid along its own line of action, its overall effect on the rigid body remains unchanged. However, if the point of application of the force is moved off this line—while keeping its magnitude, direction, and sense unchanged—the force’s effect on the rigid body will change. This is because forces applied at different points can introduce rotational effects (torques), which influence how the body moves or rotates.\n\n\n\n\n\n\nRepresents the velocity of an object in a specific direction.\n\n\n\nRepresents the rate of change of velocity.\n\n\n\nRepresents the force applied to an object in a specific direction.\n\n\n\nRepresents mass times velocity, indicating the motion of an object.\n\n\n\nRepresents the electric field intensity at a point.\n\n\n\nRepresents the direction and strength of a magnetic field.\n\n\n\n\n\n\nA vector represented as a single row of elements.\nExample: \\[\n\\mathbf{v} = \\begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\end{bmatrix}\n\\] ##### Column Vector A vector represented as a single column of elements.  Example: \\[\n\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}\n\\]\n\n\n\n\n\n\nVectors that remain in the same direction after a linear transformation.\n\\[\nA \\mathbf{v} = \\lambda \\mathbf{v}\n\\]\nwhere: - $ A $ is a square matrix, - $ $ is an eigenvalue, - $ $ is the corresponding eigenvector.\n\n\nCode\n%%manim -qm -v WARNING EigenvectorExample\n\nclass EigenvectorExample(Scene):\n    def construct(self):\n        # Set up axes\n        axes = Axes(\n            x_range=[-1, 3],\n            y_range=[-1, 3],\n            tips=True,\n            axis_config={\"include_numbers\": False},\n        ).add_coordinates()\n\n        self.play(Create(axes))\n\n        # Original vector x = [1, 1]\n        vec_x = Arrow(\n            start=axes.c2p(0, 0),\n            end=axes.c2p(1, 1),\n            buff=0,\n            color=BLUE\n        )\n        x_label = MathTex(r\"\\vec{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\").next_to(vec_x, LEFT)\n\n        self.play(GrowArrow(vec_x), Write(x_label))\n        self.wait()\n\n        # Transformed vector A x = [2, 2]\n        vec_Ax = Arrow(\n            start=axes.c2p(0, 0),\n            end=axes.c2p(2, 2),\n            buff=0,\n            color=RED\n        )\n\n        # Animate the scaling\n        self.play(Transform(vec_x, vec_Ax), run_time=2)\n        self.wait()\n\n        # Equation and lambda\n        equation = MathTex(\n            r\"A\\vec{x} = \\lambda \\vec{x} = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\"\n        ).to_edge(UP)\n        lambda_val = MathTex(r\"\\lambda = 2\").next_to(equation, DOWN)\n\n        self.play(Write(equation), Write(lambda_val))\n        self.wait()\n\n\nManim Community v0.19.0\n\n\n\n\n                                                                                                                                                               \n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\n\nRepresents the rate and direction of change of a scalar field.\nExample: Temperature Distribution is an example of gradient vector, with the equation\n\\[\n\\phi(x, y, z) = \\text{temperature at } (x, y, z)\n\\]\nThen:\n\\[\n\\vec{F}(x, y, z) = \\nabla \\phi = \\left( \\frac{\\partial \\phi}{\\partial x}, \\frac{\\partial \\phi}{\\partial y}, \\frac{\\partial \\phi}{\\partial z} \\right)\n\\]\nThis gives you the direction of fastest increase in temperature.\n\n\nCode\n# Define the temperature scalar field: a Gaussian \"heat source\"\ndef temperature(x, y):\n    return np.exp(-((x - 1.5)**2 + (y - 1.5)**2)) + 0.5 * np.exp(-((x + 1.5)**2 + (y + 1.5)**2))\n\n# Create a grid of (x, y) points\nx = np.linspace(-5, 5, 40)\ny = np.linspace(-5, 5, 40)\nX, Y = np.meshgrid(x, y)\n\n# Evaluate the scalar temperature field\nZ = temperature(X, Y)\n\n# Compute the gradient (∇T)\ndT_dx, dT_dy = np.gradient(Z, x, y)\n\n# Plotting\nplt.figure(figsize=(8, 6))\nplt.contourf(X, Y, Z, cmap='plasma', levels=30)  # Temperature map\nplt.colorbar(label='Temperature')\n\n# Overlay gradient vectors\nplt.quiver(X, Y, dT_dx, dT_dy, color='white', scale=50, width=0.003)\n\nplt.title(\"Temperature Field and Its Gradient\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nA vector perpendicular to a surface or curve.\n\n\n\nA vector that touches a curve at a single point and points in the direction of the curve.\n\n\n\n\n\n\n\n\\[\n   \\mathbf{A} + \\mathbf{B} = (A_x + B_x, A_y + B_y, A_z + B_z)\n   \\]\n- Performed component-wise.\n\n\n\n\\[\n   \\mathbf{A} - \\mathbf{B} = (A_x - B_x, A_y - B_y, A_z - B_z)\n   \\]\n- Similar to addition but subtracts components.\n\n\n\n\\[\n   k\\mathbf{A} = (kA_x, kA_y, kA_z)\n   \\]\n- Scales the vector by a constant factor.\n\n\n\n\\[\n   \\mathbf{A} \\cdot \\mathbf{B} = A_x B_x + A_y B_y + A_z B_z\n   \\]\n- Can also be expressed as:\n\\[\n   \\mathbf{A} \\cdot \\mathbf{B} = |\\mathbf{A}| |\\mathbf{B}| \\cos\\theta\n   \\]\n- Produces a scalar value.\n- Used to determine the angle between vectors.\n\n\n\n\\[\n   \\mathbf{A} \\times \\mathbf{B} =\n   \\begin{vmatrix}\n   \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n   A_x & A_y & A_z \\\\\n   B_x & B_y & B_z\n   \\end{vmatrix}\n   \\]\n- Produces a new vector perpendicular to both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\).\n- Magnitude:\n\\[\n   |\\mathbf{A} \\times \\mathbf{B}| = |\\mathbf{A}| |\\mathbf{B}| \\sin\\theta\n   \\]\n- Used in torque, angular momentum, and physics applications.\n\n\n\n\nScalar projection:\n\\[\n   \\text{proj}_{\\mathbf{B}} \\mathbf{A} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{|\\mathbf{B}|}\n   \\]\n\nVector projection:\n\\[\n   \\text{Proj}_{\\mathbf{B}} \\mathbf{A} = \\left( \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{|\\mathbf{B}|^2} \\right) \\mathbf{B}\n   \\]\n\n\n\n\n\\[\n   |\\mathbf{A}| = \\sqrt{A_x^2 + A_y^2 + A_z^2}\n   \\]\n- Represents the length of the vector.\n\n\n\n\\[\n   \\hat{\\mathbf{A}} = \\frac{\\mathbf{A}}{|\\mathbf{A}|}\n   \\]\n- A vector of magnitude 1 in the same direction as \\(\\mathbf{A}\\).\n\n\n\n\\[\n   \\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z} \\right)\n   \\]\n- Represents the rate of change of a scalar field.\n\n\n\n\\[\n   \\nabla \\cdot \\mathbf{A} = \\frac{\\partial A_x}{\\partial x} + \\frac{\\partial A_y}{\\partial y} + \\frac{\\partial A_z}{\\partial z}\n   \\]\n- Measures the “spreading out” of a vector field.\n\n\n\n\\[\n   \\nabla \\times \\mathbf{A} =\n   \\begin{vmatrix}\n   \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n   \\frac{\\partial}{\\partial x} & \\frac{\\partial}{\\partial y} & \\frac{\\partial}{\\partial z} \\\\\n   A_x & A_y & A_z\n   \\end{vmatrix}\n   \\]\n- Measures the rotational tendency of a vector field.\n\nLaplacian\n\\[\n   \\nabla^2 f = \\nabla \\cdot \\nabla f\n   \\]\n\n\n\nA second-order differential operator that measures the rate of change of a function’s gradient.\n\n\n\n\n\n\nRotation of Vectors (Matrix multiplication)\n\n\n\nA vector \\(\\mathbf{A}\\) can be rotated using a transformation matrix.\n\n\nVector Integration\n\n\n\nUsed to compute work done by a force field.\n\n\nVector Differentiation\n\n\n\nMeasures the rate of change of vector functions.\n\n\n\n\nA vector space (or linear space) is a fundamental concept in linear algebra and is the set of vectors that can be added together and multiplied (“scaled”) by numbers, called scalars, in such a way that certain properties hold.\nA vector space over a field $ F $ is a set $ V $ equipped with two operations: - Vector addition: A binary operation $ + : V V V $ that combines two vectors to produce another vector. - Scalar multiplication: A binary operation $ : F V V $ that combines a scalar (an element from field $ F $) and a vector from $ V $, producing another vector.\nThe vector space must satisfy the following properties (axioms):\n\n\nLet $ V $ be a set of vectors, and $ F $ be a field of scalars. For any vectors $ u, v, w V $ and scalars $ a, b F $, the following properties must hold:\n\nClosure under addition: For any $ u, v V $, $ u + v V $.\nClosure under scalar multiplication: For any $ a F $ and $ v V $, $ a v V $.\nCommutativity of addition: $ u + v = v + u $ for all $ u, v V $.\nAssociativity of addition: $ (u + v) + w = u + (v + w) $ for all $ u, v, w V $.\nExistence of additive identity: There exists a zero vector $ 0 V $ such that $ v + 0 = v $ for all $ v V $.\nExistence of additive inverses: For every vector $ v V $, there exists $ -v V $ such that $ v + (-v) = 0 $.\nDistributivity of scalar multiplication over vector addition: $ a (v + w) = a v + a w $ for all $ a F $ and $ v, w V $.\nDistributivity of scalar multiplication over scalar addition: $ (a + b) v = a v + b v $ for all $ a, b F $ and $ v V $.\nCompatibility of scalar multiplication with field multiplication: $ a (b v) = (a b) v $ for all $ a, b F $ and $ v V $.\nExistence of multiplicative identity: $ 1 v = v $ for all $ v V $, where $ 1 $ is the multiplicative identity in the field $ F $.\n\n\n\n\n\nEuclidean Space $ ^n $: The set of all $ n $-dimensional vectors with real-number components.\nFunction Spaces: The set of all continuous functions defined on a closed interval $ [a, b] $ is a vector space.\nPolynomial Spaces: The set of all polynomials with real coefficients, denoted $ [x] $, forms a vector space.\nMatrices: The set of all $ m n $ matrices with real entries is a vector space.\n\n\n\n\n\n\nA subspace is a subset of a vector space that is itself a vector space. For a subset $ W $ of a vector space $ V $ to be a subspace, it must satisfy the following: - $ W $ is closed under vector addition. - $ W $ is closed under scalar multiplication. - $ W $ contains the zero vector.\n\n\n\nA set of vectors $ {v_1, v_2, , v_k} $ in a vector space $ V $ is said to be linearly independent if the only solution to the equation \\[\na_1 v_1 + a_2 v_2 + \\dots + a_k v_k = 0\n\\] is $ a_1 = a_2 = = a_k = 0 $.\nIf there is a nontrivial solution (i.e., some coefficients are nonzero), the vectors are said to be linearly dependent.\n\n\n\nThe span of a set of vectors $ {v_1, v_2, , v_k} $ is the set of all possible linear combinations of those vectors: \\[\n\\text{span}(v_1, v_2, \\dots, v_k) = \\{ a_1 v_1 + a_2 v_2 + \\dots + a_k v_k \\mid a_1, a_2, \\dots, a_k \\in F \\}\n\\] The span forms a subspace of $ V $.\n\n\n\nA basis of a vector space $ V $ is a set of linearly independent vectors in $ V $ that span $ V $. Every vector in $ V $ can be uniquely written as a linear combination of the basis vectors.\n\n\n\nThe dimension of a vector space $ V $ is the number of vectors in a basis of $ V $. A vector space can be finite-dimensional or infinite-dimensional, depending on the number of basis vectors.\n\n\n\nOnce a basis $ {v_1, v_2, , v_n} $ is chosen, any vector $ v V $ can be written uniquely as: \\[\nv = a_1 v_1 + a_2 v_2 + \\dots + a_n v_n\n\\] The scalars $ a_1, a_2, , a_n $ are called the coordinates of $ v $ relative to the basis $ {v_1, v_2, , v_n} $.",
    "crumbs": [
      "Linear Algebra",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/01.Vectors_and_Matrices.html#matrices",
    "href": "Notebooks/Linear_Algebra/01.Vectors_and_Matrices.html#matrices",
    "title": "Vectors and Matrices",
    "section": "Matrices",
    "text": "Matrices\n\nBasic Concepts\n\n\nTypes of Matrix\n\n\nMatrix Operations",
    "crumbs": [
      "Linear Algebra",
      "Vectors and Matrices"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/03.Computational_approaches_to_linear_systems.html",
    "href": "Notebooks/Linear_Algebra/03.Computational_approaches_to_linear_systems.html",
    "title": "Computational approaches",
    "section": "",
    "text": "Code\nimport numpy as np\nfrom scipy.linalg import solve, lu, qr, cholesky, eig, svd\nfrom scipy.sparse import csr_matrix\nimport scipy.sparse.linalg as spla\nfrom scipy.sparse.linalg import spsolve\nfrom scipy.sparse.linalg import cg  # Conjugate Gradient",
    "crumbs": [
      "Linear Algebra",
      "Computational approaches to Linear Systems"
    ]
  },
  {
    "objectID": "Notebooks/Linear_Algebra/03.Computational_approaches_to_linear_systems.html#numpy-and-scipy-linear-algebra-solvers",
    "href": "Notebooks/Linear_Algebra/03.Computational_approaches_to_linear_systems.html#numpy-and-scipy-linear-algebra-solvers",
    "title": "Computational approaches",
    "section": "NumPy and SciPy linear algebra solvers",
    "text": "NumPy and SciPy linear algebra solvers\n\nNumPy\nNumPy provides basic linear algebra routines through \\(\\texttt{numpy.linalg}\\). These functions internally use LAPACK and BLAS for efficiency.\n\n1. Solving a system of linear equations\nGiven the system:\n\\[ Ax = b \\]\n\n\nCode\nA = np.array([[3, 2], [1, 4]])\nB = np.array([5, 6])\nx = np.linalg.solve(A, B)  # Solves Ax = B\nprint(x)\n\n\n[0.8 1.3]\n\n\nThis function uses an efficient algorithm (Gaussian elimination with partial pivoting) to find x. Refer to this link for more information.\n\n\n2. Computing the inverse of a matrix\n\\[ A^{-1} \\]\n\n\nCode\nA_inv = np.linalg.inv(A) # Inverse of A\nx = np.dot(A_inv, B) # Solves Ax = B\nprint(x)\n\n\n[0.8 1.3]\n\n\nNote that the matrix must non-singular \\((\\det(A) \\neq 0)\\) for this to work. Click the link for more information.\n\n\n3. Computing the determinant\nThe determinant of a square matrix A is a scalar value that provides important information about the matrix. It is denoted as: \\[ \\det(A) \\]\n\n\nCode\ndet_A = np.linalg.det(A) # Determinant of A\n# Check if the determinant is zero\nif det_A == 0:\n    print(\"The system has no unique solution (determinant is zero).\")\nelse:\n    x = np.linalg.solve(A, B) # Solves Ax = B\n    print(x)\n\n\n[0.8 1.3]\n\n\nA nonzero determinant means A is invertible, while det(A) = 0 means A is singular (not invertible).\n\n\n4. Eigenvalues and Eigenvectors\nFor a square matrix ( \\(A\\) ), the eigenvalues (( \\(\\lambda\\) )) and corresponding eigenvectors (( \\(v\\) )) satisfy the equation:\n\\[\nA \\cdot v = \\lambda \\cdot v\n\\]\nwhere:\n\n( \\(\\lambda\\) ) (lambda) is a scalar (eigenvalue),\n( \\(v\\) ) is a nonzero vector (eigenvector).\n\n\n\nCode\n# Eigenvalues and eigenvectors\neigenvalues, eigenvectors  = np.linalg.eig(A) # compute eigenvalues and eigenvectors\n\n# Diagonal matrix of eigenvalues\nD = np.diag(eigenvalues)\n\n# Compute P inverse\nP_inv = np.linalg.inv(eigenvectors)\n\n# Transform B into the eigenbasis\nB_prime = P_inv @ B\n\n# Solve for Y in DY = B'\nY = np.linalg.solve(D, B_prime)\n\n# Compute the final solution X = P Y\nX = eigenvectors @ Y\n\n# Print the result\nprint(\"Solution X:\", X)\n\n\nSolution X: [0.8 1.3]\n\n\nELI5: Eigenvectors “point” in the same direction as most of your data, and eigenvalues tell you how strongly your data points that direction.\n\n\n5. Singular Value Decomposition (SVD)\nSingular Value Decomposition (SVD) is a method to break down a complex matrix into simpler components. It is similar to taking a blurry image and separating it into clear building blocks.\nFor any matrix ( \\(A\\) ), SVD expresses it as:\n\\[ A = U \\Sigma V^T \\]\nWhere: - ( \\(U\\) ) (Left singular vectors) → Contains important patterns from the original data. - ( \\(\\Sigma\\) ) (Singular values) → A diagonal matrix with values indicating the importance of each pattern. - ( \\(V^{T}\\) ) (Right singular vectors) → Contains another set of patterns that, when combined with ( \\(\\Sigma\\) ), recreate the original data.\nAnalogy: Imagine you have a large playlist of songs. SVD breaks it down into: - ( \\(U\\)) → The general themes of music (e.g., rock, jazz, pop). - ( \\(\\Sigma\\) ) → How strongly each theme appears in the playlist. - ( \\(V^{T}\\) ) → The details of each song, arranged by themes.\n\n\nCode\n# Compute Singular Value Decomposition (SVD)\nU, S, V = np.linalg.svd(A)\n\n# Convert S into a diagonal matrix\nS_diag = np.diag(S)\n\n# Compute the pseudo-inverse of S\nS_inv = np.linalg.inv(S_diag)\n\n# Compute the pseudo-inverse of A using SVD\nA_pinv = V.T @ S_inv @ U.T # @ means matrix multiplication and .T means transpose\n\n# Solve for X using the pseudo-inverse\nX = A_pinv @ B\n\n# Print the result\nprint(\"Solution X:\", X)\n\n\nSolution X: [0.8 1.3]\n\n\n\n\n\nSciPy\nSciPy extends NumPy’s linear algebra capabilities with additional matrix decompositions, advanced solvers, and performance optimizations. Key Advantages of SciPy over NumPy: - Uses LAPACK and ATLAS, but with extra options for optimization. - Supports sparse matrices via . - Includes additional decomposition methods.\n\n1. Solving a linear system (\\(\\texttt{scipy.linalg.solve}\\))\nSame as \\(\\texttt{numpy.linalg.solve}\\) but often faster and more stable.\n\n\nCode\nx = solve(A, B)  # More efficient than np.linalg.solve for large matrices\nprint(x)\n\n\n[0.8 1.3]\n\n\n\n\n2. LU Decomposition\n\\[ PA = LU \\]\n\n\nCode\nP, L, U = lu(A)  # P: Permutation, L: Lower triangular, U: Upper triangular\n\n# Compute Pb\nPb = np.dot(P, B)\n\n# Solve Ly = Pb using forward substitution\ny = np.linalg.solve(L, Pb)\n\n# Solve Ux = y using backward substitution\nx = np.linalg.solve(U, y)\nx\n\n\narray([0.8, 1.3])\n\n\n\n\n3. QR Decomposition\n\\[ A = QR \\]\n\n\nCode\nQ, R = qr(A)\n# Compute Q^T B\nQTB = np.dot(Q.T, B)\n\n# Solve Rx = Q^T B using backward substitution\nx = np.linalg.solve(R, QTB)\nx\n\n\narray([0.8, 1.3])\n\n\n\n\n4. Cholesky Decomposition (for positive definite matrices)\nCholesky Decomposition is a matrix factorization technique used for symmetric positive definite matrices. Given a symmetric positive definite matrix ( A ), it can be decomposed as:\n\\[\nA = LL^T\n\\]\nwhere: - ( \\(L\\) ) is a lower triangular matrix with positive diagonal entries. - ( \\(L^{T}\\) ) is the transpose of ( L ), making the decomposition efficient and numerically stable.\nWhy Use Cholesky Decomposition? 1. Efficient Computation: It is about twice as fast as LU decomposition for solving linear systems. 2. Numerical Stability: Since it is applied only to positive definite matrices, it avoids issues related to pivoting. 3. Applications: - Solving linear systems ( \\(Ax = b\\) ) efficiently. - Optimization problems (e.g., least squares, Kalman filters). - Monte Carlo simulations (e.g., generating correlated random variables). Helpful in Quantum Mechanical simulations.\nAlgorithm for Cholesky Decomposition Given a symmetric positive definite matrix ( \\(A\\) ), the elements of ( \\(L\\) ) are computed as:\n\nDiagonal elements:\n\n\\[\nL_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2}\n\\]\n\nOff-diagonal elements:\n\n\\[\nL_{ij} = \\frac{1}{L_{ii}} \\left( A_{ij} - \\sum_{k=1}^{i-1} L_{ik} L_{jk} \\right), \\quad j &gt; i\n\\]\nTo solve for ( x ) in the equation:\n\\[ Ax = B \\]\nusing Cholesky decomposition, follow these steps:\nSteps for Solving ( \\(Ax = B\\) ) Using Cholesky Decomposition\n\nCheck if ( \\(A\\) ) is symmetric positive definite\n\n( \\(A\\) ) must be symmetric (( \\(A = A^{T}\\) )).\nAll its eigenvalues must be positive.\n\nPerform Cholesky Decomposition\n\nDecompose ( \\(A\\) ) as ( \\(A = LL^{T}\\) ), where ( \\(L\\) ) is a lower triangular matrix.\n\nSolve for ( \\(y\\) ) in ( \\(Ly = B\\) ) (using forward substitution).\nSolve for ( \\(x\\) ) in ( \\(L^{T} x = y\\) ) (using backward substitution).\n\n\n\nCode\nL = cholesky(A, lower=True)\n# Solve Ly = B using forward substitution\ny = np.linalg.solve(L, B)\n\n# Solve L^T x = y using backward substitution\nx = np.linalg.solve(L.T, y)\nx\n\n\narray([1.27272727, 1.18181818])\n\n\nThe answer is different, issue coming from the fact that Cholesky decomposition only works for symmetric positive definite (SPD) matrices and the example above is NOT symmetric.\n\n\n\nExample\nConsider the matrix:\n\\[\nA =\n\\begin{bmatrix}\n4 & 12 & -16 \\\\\n12 & 37 & -43 \\\\\n-16 & -43 & 98\n\\end{bmatrix}\n\\]\nApplying Cholesky decomposition, we get:\n\\[\nL =\n\\begin{bmatrix}\n2 & 0 & 0 \\\\\n6 & 1 & 0 \\\\\n-8 & 5 & 3\n\\end{bmatrix}\n\\]\nwhere ( \\(L^{T}\\) ) is its transpose.\n\n\nCode\nC = np.array([\n    [4, 12, -16],\n    [12, 37, -43],\n    [-16, -43, 98]\n])\nL = cholesky(C, lower=True)\nL\n\n\narray([[ 2.,  0.,  0.],\n       [ 6.,  1.,  0.],\n       [-8.,  5.,  3.]])\n\n\nand if you want to solve for x, assuming that B \\[\nB =\n\\begin{bmatrix}\n2  \\\\\n6 \\\\\n-8\n\\end{bmatrix}\n\\]\n\n\nCode\n# Define vector B\nD = np.array([2, 6, -8]) # B is being already above\n\n# Perform Cholesky decomposition: A = L * L^T\nL = cholesky(C, lower=True)\n\n# Solve for y in L * y = B using forward substitution\ny = np.linalg.solve(L, D)\n\n# Solve for x in L^T * x = y using backward substitution\nx = np.linalg.solve(L.T, y)\n\nprint(\"Solution x:\", x)\n\n\nSolution x: [0.5 0.  0. ]\n\n\n\n5. Eigenvalue Problems (\\(\\texttt{scipy.linalg.eig}\\))\nELI5, think of a spring: - if you pull it straight, it stretches along its length—that’s like an eigenvector (a special direction). - How much it stretches (double, triple, or shrinks) is the eigenvalue (a number describing the effect).\n\n\nCode\neigvals, eigvecs = eig(A)\n\n# Compute V^(-1) * B\nV_inv_B = np.linalg.solve(eigvecs, B)\n\n# Solve D * y = V_inv_B (since D is diagonal, just divide)\ny = V_inv_B / eigvals\n\n# Compute final solution x = V * y\nx = np.dot(eigvecs, y).real\nx\n\n\narray([0.8, 1.3])\n\n\nAnother example, array C from the example above\n\n\nCode\neigvals_c, eigvecs_c = eig(C)\n\n# Compute V^(-1) * B\nV_inv_D = np.linalg.solve(eigvecs_c, D)\n\n# Solve D * y = V_inv_B (since D is diagonal, just divide)\ny = V_inv_D / eigvals_c\n\n# Compute final solution x = V * y  \nx = np.dot(eigvecs_c, y).real.round(2) # real to remove imaginary part and round to 2 decimal places\nx\n\n\narray([ 0.5, -0. ,  0. ])\n\n\n\n\n6. Singular Value Decomposition (SVD)\nSingular Value Decomposition (SVD) is a way to break down a big, complicated matrix into smaller, simpler parts. For any matrix ( \\(A\\) ), SVD expresses it as: \\[\nA = U \\Sigma V^T\n\\] Where: - ( \\(U\\) ) (Left singular vectors) → Contains important patterns from the original data. - ( \\(\\Sigma\\) ) (Singular values) → A diagonal matrix with values indicating the importance of each pattern. - ( \\(V^{T}\\) ) (Right singular vectors) → Contains another set of patterns that, when combined with ( ), recreate the original data.\nAnalogy: Imagine you have a large playlist of songs. SVD breaks it down into: - ( \\(U\\) ) → The general themes of music (e.g., rock, jazz, pop). - ( \\(\\Sigma\\) ) → How strongly each theme appears in the playlist. - ( \\(V^{T}\\) ) → The details of each song, arranged by themes.\n\n\nCode\nU, S, V = svd(A)\n# Compute the pseudo-inverse of Sigma\nS_inv = np.diag(1 / S)\n\n# Solve for x using the SVD components\nx = V.T @ S_inv @ U.T @ B\n\nprint(\"Solution x:\", x)\n\n\nSolution x: [0.8 1.3]\n\n\nOther example:\n\n\nCode\nU1, S1, V1 = svd(C)\n# Compute the pseudo-inverse of Sigma\nS_inv1 = np.diag(1 / S1)\n\n# Solve for x using the SVD components\nx1 = V1.T @ S_inv1 @ U1.T @ D\n\nprint(\"Solution x:\", x1.real.round(2))\n\n\nSolution x: [ 0.5  0.  -0. ]\n\n\n\n\n\nSparse Linear Algebra (\\(\\texttt{scipy.sparse.linalg}\\))\nFor large, sparse matrices, SciPy provides \\(\\texttt{scipy.sparse.linalg}\\), which is optimized for efficiency. if you don’t know, A sparse matrix is a matrix in which most of the elements are zero. It is the opposite of a dense matrix, where most elements are nonzero.\n\n1. Solving a sparse linear system\n\\[ Ax = b \\] where \\(A\\) is a sparse matrix. for example:\n\n\nCode\n# sparse matrix A (5x5)\nE = csr_matrix([\n    [4, 1, 0, 0, 0],\n    [1, 4, 1, 0, 0],\n    [0, 1, 4, 1, 0],\n    [0, 0, 1, 4, 1],\n    [0, 0, 0, 1, 3]\n])\n\n# right-hand side vector b\nF = np.array([1, 2, 3, 4, 5])\n\n# Solve Ax = b\nx = spla.spsolve(E, F)\n\n# Print the solution\nprint(\"Solution x:\", x)\n\n\nSolution x: [0.16987741 0.32049037 0.54816112 0.48686515 1.50437828]\n\n\n\n\nCode\nA_sparse = csc_matrix(A)  # Convert A to sparse format\nx = spsolve(A_sparse, B)\n\n\n\n\n2. Iterative solvers (Conjugate Gradient, GMRES)\nIterative solvers like Conjugate Gradient (CG) and Generalized Minimal Residual (GMRES) are widely used for solving large, sparse linear systems of the form:\n\\[\nAx = b\n\\]\nwhere ( \\(A\\) ) is a matrix, ( \\(x\\) ) is the unknown vector, and ( \\(b\\) ) is a given right-hand-side vector. These solvers are particularly useful when direct methods (like LU decomposition) are too expensive in terms of memory and computation.\nComparison: CG vs. GMRES\n\n\n\n\n\n\n\n\nFeature\nConjugate Gradient (CG)\nGMRES\n\n\n\n\nMatrix Type\nSymmetric Positive Definite\nGeneral (Non-Symmetric, Non-SPD)\n\n\nStorage\nLow (only need a few vectors)\nHigh (grows with iterations)\n\n\nConvergence\nFaster for well-conditioned SPD matrices\nCan stagnate without restarts\n\n\nComputational Cost\nCheaper per iteration\nMore expensive due to orthogonalization\n\n\nRestarting Needed?\nNo\nYes (for large problems, GMRES(m))\n\n\n\n\n\nCode\n# sparse matrix A (5x5)\nE = csr_matrix([\n    [4, 1, 0, 0, 0],\n    [1, 4, 1, 0, 0],\n    [0, 1, 4, 1, 0],\n    [0, 0, 1, 4, 1],\n    [0, 0, 0, 1, 3]\n])\n\n# right-hand side vector b\nF = np.array([1, 2, 3, 4, 5])\n\n# Solve\n\nx, info = cg(E, F)\n# Print the solution\nprint(\"Solution x:\", x)\nprint(\"CG Info:\", info) \n\n\nSolution x: [0.16987741 0.32049037 0.54816112 0.48686515 1.50437828]\nCG Info: 0\n\n\ninfo=0 means that the solution converged successfully.",
    "crumbs": [
      "Linear Algebra",
      "Computational approaches to Linear Systems"
    ]
  }
]