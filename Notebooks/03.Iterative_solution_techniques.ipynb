{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "    fig-width: 9\n",
    "    fig-height: 6\n",
    "jupyter: python3\n",
    "code-fold: true\n",
    "code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Solution Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterative solution techniques** are methods used to approximate solutions to mathematical problems, especially when direct (analytical) solutions are difficult or impossible to obtain. In this context, these methods are called **Relaxation Methods**, as they are used for solving systems of equations, including nonlinear systems. These methods work by iteratively refining an initial guess until a satisfactory solution is reached. Below are some key iterative solution techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Jacobi Method**\n",
    "- Updates each variable using values from the **previous iteration**.\n",
    "- Simple but may converge slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Gauss-Seidel Method**\n",
    "- Uses the **most recent updated values** within the same iteration.\n",
    "- Typically converges faster than Jacobi for diagonally dominant matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Successive Over-Relaxation (SOR)**\n",
    "- Extends Gauss-Seidel by introducing a **relaxation factor** $ \\omega $:\n",
    "  - $ \\omega > 1 $: Over-relaxation (accelerates convergence).\n",
    "  - $ \\omega < 1 $: Under-relaxation (can improve stability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Symmetric Successive Over-Relaxation (SSOR)**\n",
    "- Applies SOR **forward and backward** in each iteration.\n",
    "- Often used as a **preconditioner** in iterative methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Accelerated Over-Relaxation (AOR)**\n",
    "- Generalizes SOR by using **two parameters** for additional flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. **Chebyshev Semi-Iterative Method**\n",
    "- Uses **Chebyshev polynomials** to accelerate basic iterative methods (e.g., Jacobi).\n",
    "- Requires knowledge of eigenvalue bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. **Richardson Iteration**\n",
    "- Simplest method; updates the solution by scaling the residual with a **relaxation parameter** $ \\alpha $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Summary Table\n",
    "\n",
    "| **Method**                      | **Relaxation Parameter(s)**        | **Key Feature**                               |\n",
    "|---------------------------------|-----------------------------------|-----------------------------------------------|\n",
    "| Jacobi                          | None                              | Simple, independent updates                   |\n",
    "| Gauss-Seidel                    | None                              | Uses latest updates immediately               |\n",
    "| Successive Over-Relaxation (SOR)| $ \\omega $                        | Tunable parameter accelerates convergence     |\n",
    "| Symmetric SOR (SSOR)            | $ \\omega $                        | Symmetric application, preconditioning        |\n",
    "| Accelerated Over-Relaxation (AOR)| Two parameters                    | Flexible convergence control                  |\n",
    "| Chebyshev Method                | None (Polynomial-based)           | Acceleration via Chebyshev polynomials        |\n",
    "| Richardson Iteration            | $ \\alpha $                        | Simple residual scaling                       |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
