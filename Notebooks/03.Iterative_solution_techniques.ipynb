{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "    fig-width: 9\n",
    "    fig-height: 6\n",
    "jupyter: python3\n",
    "code-fold: true\n",
    "code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Solution Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative solution techniques are methods used to approximate solutions to mathematical problems, especially when direct (analytical) solutions are difficult or impossible to obtain. These methods work by iteratively refining an initial guess until a satisfactory solution is reached. Below are some key iterative solution techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fixed-Point Iteration\n",
    "- **Concept:** Given a function \\( $f(x)$ \\), rewrite the equation in the form \\( $x = g(x)$ \\) and use the iterative formula:\n",
    "  $$\n",
    "  x_{n+1} = g(x_n)\n",
    "  $$\n",
    "- **Application:** Solving nonlinear equations.\n",
    "- **Convergence:** If \\( $|g'(x)| < 1$ \\) in the neighborhood of the root, the method converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Newton-Raphson Method\n",
    "- **Concept:** Uses the derivative of a function to refine guesses toward the root:\n",
    "  $$\n",
    "  x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
    "  $$\n",
    "- **Application:** Root-finding problems.\n",
    "- **Convergence:** Quadratic convergence, meaning it converges very quickly when close to the root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Secant Method\n",
    "- **Concept:** A derivative-free alternative to Newton-Raphson:\n",
    "  $$\n",
    "  x_{n+1} = x_n - f(x_n) \\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}\n",
    "  $$\n",
    "- **Application:** Root-finding problems where derivatives are difficult to compute.\n",
    "- **Convergence:** Superlinear but not as fast as Newtonâ€™s method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Jacobi Method\n",
    "- **Concept:** Solves systems of linear equations by iterating:\n",
    "  $$\n",
    "  x_i^{(k+1)} = \\frac{b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)}}{a_{ii}}\n",
    "  $$\n",
    "- **Application:** Large systems of linear equations.\n",
    "- **Convergence:** Requires a diagonally dominant or symmetric positive definite matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gauss-Seidel Method\n",
    "- **Concept:** Like the Jacobi method but uses updated values immediately:\n",
    "  $$\n",
    "  x_i^{(k+1)} = \\frac{b_i - \\sum_{j < i} a_{ij} x_j^{(k+1)} - \\sum_{j > i} a_{ij} x_j^{(k)}}{a_{ii}}\n",
    "  $$\n",
    "- **Application:** Faster than Jacobi for solving linear systems.\n",
    "- **Convergence:** Same conditions as Jacobi but usually converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Successive Over-Relaxation (SOR)\n",
    "- **Concept:** A modified Gauss-Seidel method introducing a relaxation factor \\( $\\omega$ \\):\n",
    "  $$\n",
    "  x_i^{(k+1)} = (1 - \\omega)x_i^{(k)} + \\omega \\left( \\frac{b_i - \\sum_{j < i} a_{ij} x_j^{(k+1)} - \\sum_{j > i} a_{ij} x_j^{(k)}}{a_{ii}} \\right)\n",
    "  $$\n",
    "- **Application:** Enhances convergence speed for linear system solvers.\n",
    "- **Convergence:** Best for optimally chosen \\( $\\omega$ \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gradient Descent\n",
    "- **Concept:** Minimizes functions iteratively using gradients:\n",
    "  $$\n",
    "  x_{n+1} = x_n - \\alpha \\nabla f(x_n)\n",
    "  $$\n",
    "- **Application:** Optimization problems, machine learning.\n",
    "- **Convergence:** Dependent on step size \\( $\\alpha$ \\), sensitive to function curvature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conjugate Gradient Method\n",
    "- **Concept:** Solves linear systems by minimizing a quadratic function:\n",
    "  $$\n",
    "  x_{n+1} = x_n + \\alpha p_n\n",
    "  $$\n",
    "- **Application:** Large sparse symmetric positive definite matrices.\n",
    "- **Convergence:** Faster than gradient descent for well-conditioned problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Iterative Solution Techniques in Computational Chemistry\n",
    "\n",
    "In **computational chemistry**, several iterative solution techniques are extensively used to solve equations related to **quantum mechanics, molecular dynamics, and statistical mechanics**. The most commonly used methods include:\n",
    "\n",
    "## 1. Self-Consistent Field (SCF) Method  \n",
    "- **Uses:** Solving the **Hartree-Fock (HF) equations** and **Density Functional Theory (DFT)** calculations.  \n",
    "- **Technique:**  \n",
    "  - Based on **Fixed-Point Iteration**: The wavefunction and electron density are updated iteratively until self-consistency is achieved.\n",
    "  - Involves solving the **Fock matrix** iteratively to obtain molecular orbitals.  \n",
    "- **Related Methods:**  \n",
    "  - DIIS (Direct Inversion in the Iterative Subspace) accelerates SCF convergence.  \n",
    "  - Pulayâ€™s method is another acceleration technique.  \n",
    "\n",
    "## 2. Newton-Raphson Method (Quasi-Newton Methods)\n",
    "- **Uses:**  \n",
    "  - Geometry optimization (finding minimum energy structures).  \n",
    "  - Finding transition states in reaction mechanisms.  \n",
    "- **Technique:**  \n",
    "  - Iteratively updates atomic coordinates using gradients and Hessians (second derivatives of energy).  \n",
    "  - Faster convergence but requires Hessian computation.  \n",
    "\n",
    "## 3. Conjugate Gradient (CG) Method\n",
    "- **Uses:**  \n",
    "  - Solving large linear systems in **DFT calculations**.  \n",
    "  - Optimization of molecular geometries (for large systems).  \n",
    "- **Technique:**  \n",
    "  - A more efficient alternative to simple gradient descent.  \n",
    "  - Works well for **sparse matrices** in large-scale simulations.  \n",
    "\n",
    "## 4. Gauss-Seidel & Successive Over-Relaxation (SOR)\n",
    "- **Uses:**  \n",
    "  - Solving the **Poisson equation** for electrostatics in molecular simulations.  \n",
    "  - Iteratively refining solutions in **ab initio molecular dynamics**.  \n",
    "- **Technique:**  \n",
    "  - Efficient for **grid-based methods** in electrostatics and charge distributions.  \n",
    "\n",
    "## 5. Multigrid Methods\n",
    "- **Uses:**  \n",
    "  - Solving differential equations in **quantum mechanics** and **molecular dynamics**.  \n",
    "  - Used in Poisson-Boltzmann solvers for biomolecular electrostatics.  \n",
    "- **Technique:**  \n",
    "  - Solves problems at multiple scales (coarse to fine grid levels) to improve convergence.  \n",
    "\n",
    "## 6. Gradient-Based Optimization (Gradient Descent & Variants)\n",
    "- **Uses:**  \n",
    "  - **Molecular docking** and **protein-ligand interactions**.  \n",
    "  - Training **machine learning models** for chemistry applications.  \n",
    "- **Technique:**  \n",
    "  - Moves in the direction of the **negative gradient** of energy or loss function.  \n",
    "  - Variants like **stochastic gradient descent (SGD)** and **Adam optimizer** are used in cheminformatics.  \n",
    "\n",
    "## Which Method is Best?\n",
    "| **Application** | **Best Method** |\n",
    "|----------------|----------------|\n",
    "| **Hartree-Fock & DFT calculations** | SCF + DIIS/Pulay acceleration |\n",
    "| **Geometry Optimization** | Newton-Raphson, Quasi-Newton, CG |\n",
    "| **Large-scale systems (10,000+ atoms)** | Conjugate Gradient, Multigrid |\n",
    "| **Molecular Dynamics (force field-based simulations)** | Gauss-Seidel, SOR |\n",
    "| **Deep Learning in Chemistry** | Gradient Descent-based optimizers |\n",
    "\n",
    "Would you like a more detailed explanation of any method in a computational chemistry context? ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
