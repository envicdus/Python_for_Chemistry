{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "    fig-width: 9\n",
    "    fig-height: 6\n",
    "    html-math-method: katex\n",
    "jupyter: python3\n",
    "code-fold: true\n",
    "code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Solution Techniques(Relaxation Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterative solution techniques** are methods used to approximate solutions to mathematical problems, especially when direct (analytical) solutions are difficult or impossible to obtain. In this context, these methods are called **Relaxation Methods**, as they are used for solving systems of equations, including nonlinear systems. These methods work by iteratively refining an initial guess until a satisfactory solution is reached. Before we begin, I'll encourage everyone who will read this to skip the example if your not fond of number as you will see a lot of equations and numbers here for my examples as I want to be thorough (sorry ðŸ˜…). Below are some key iterative solution techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Jacobi Method**\n",
    "The Jacobi method is an iterative algorithm used to approximate the solution of a system of linear equations. It's particularly useful for diagonally dominant systems. Here's a breakdown of the method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\displaystyle\n",
    "\\begin{align*}\n",
    "a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\n",
    "a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n",
    "\\vdots \\hspace{1cm} \\vdots \\hspace{2cm} \\vdots \\\\\n",
    "a_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n &= b_n\n",
    "\\end{align*}\n",
    "$$\n",
    "In matrix form, this can be written as:\n",
    "\n",
    "$$Ax = b$$\n",
    "\n",
    "where $A$ is an $n \\times n$ matrix, $x$ is an $n \\times 1$ vector of unknowns, and $b$ is an $n \\times 1$ vector of constants.\n",
    "\n",
    "### Formula\n",
    "\n",
    "The Jacobi method rewrites each equation to solve for one variable in terms of the others. For the $i$-th equation, we solve for $x_i$:\n",
    "\n",
    "$$x_i = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j=1, j\\neq i}^{n} a_{ij}x_j\\right)$$\n",
    "\n",
    "In iterative form, the formula becomes:\n",
    "\n",
    "$$x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j=1, j\\neq i}^{n} a_{ij}x_j^{(k)}\\right)$$\n",
    "\n",
    "where $x_i^{(k)}$ represents the value of $x_i$ at the $k$-th iteration.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1.  $\\textbf{Initialization:}$ Choose an initial guess for the solution vector $x^{(0)} = (x_1^{(0)}, x_2^{(0)}, \\dots, x_n^{(0)})$.\n",
    "2.  $\\textbf{Iteration:}$ For $k = 0, 1, 2, \\dots$ until convergence:\n",
    "    * For each $i = 1, 2, \\dots, n$:\n",
    "        $$x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j=1, j\\neq i}^{n} a_{ij}x_j^{(k)}\\right)$$\n",
    "3.  $\\textbf{Convergence:}$ Check for convergence. This can be done by comparing the difference between successive iterations:\n",
    "    $$\\|x^{(k+1)} - x^{(k)}\\| < \\text{tolerance}$$\n",
    "    where $\\text{tolerance}$ is a small positive number. Alternatively, check the absolute value of the difference of each variable between iterations: $|x_i^{(k+1)}-x_i^{(k)}| < \\text{tolerance}$ for all i.\n",
    "4.  $\\textbf{Output:}$ The final approximation $x^{(k+1)}$ is the approximate solution.\n",
    "\n",
    "### Matrix Form\n",
    "\n",
    "The matrix $A$ can be decomposed into:\n",
    "\n",
    "* $D$: a diagonal matrix containing the diagonal elements of $A$.\n",
    "* $L$: a lower triangular matrix with the negative of the off-diagonal elements below the main diagonal.\n",
    "* $U$: an upper triangular matrix with the negative of the off-diagonal elements above the main diagonal.\n",
    "\n",
    "Then, $A = D - L - U$. The iterative formula becomes:\n",
    "\n",
    "$$x^{(k+1)} = D^{-1}(L + U)x^{(k)} + D^{-1}b$$\n",
    "\n",
    "Or:\n",
    "\n",
    "$$x^{(k+1)} = Tx^{(k)} + c$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $T = D^{-1}(L + U)$ is the iteration matrix.\n",
    "* $c = D^{-1}b$.\n",
    "\n",
    "### Convergence\n",
    "\n",
    "The Jacobi method converges if the matrix $A$ is strictly diagonally dominant. A matrix is strictly diagonally dominant if:\n",
    "\n",
    "$$|a_{ii}| > \\sum_{j=1, j\\neq i}^{n} |a_{ij}|$$\n",
    "\n",
    "for all $i = 1, 2, \\dots, n$.\n",
    "\n",
    "Convergence is also guaranteed if the spectral radius of the iteration matrix $T$ is less than 1, i.e., $\\rho(T) < 1$.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Simple to understand and implement.\n",
    "* Relatively easy to parallelize.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Can be slow to converge, especially for large systems.\n",
    "* Convergence is not guaranteed for all systems.\n",
    "* The method requires storage of two vectors, the old values, and the new values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "$$\n",
    "\\begin{align*}\n",
    "10x_1 - 2x_2 - x_3 - x_4 &= 3 \\\\\n",
    "-2x_1 + 10x_2 - x_3 - x_4 &= 15 \\\\\n",
    "-x_1 - x_2 + 10x_3 - 2x_4 &= 27 \\\\\n",
    "-x_1 - x_2 - 2x_3 + 10x_4 &= -9\n",
    "\\end{align*}\n",
    "$$\n",
    "In matrix form, $Ax = b$, where:\n",
    "\n",
    "$$ A = \\begin{pmatrix} 10 & -2 & -1 & -1 \\\\ -2 & 10 & -1 & -1 \\\\ -1 & -1 & 10 & -2 \\\\ -1 & -1 & -2 & 10 \\end{pmatrix}, \\quad x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{pmatrix} $$\n",
    "\n",
    "This system is diagonally dominant, so the Jacobi method should converge.\n",
    "\n",
    "**Solution**<br>\n",
    "We rewrite each equation to solve for one variable:\n",
    "\n",
    "\\begin{align*}\n",
    "x_1^{(k+1)} &= \\frac{1}{10}(3 + 2x_2^{(k)} + x_3^{(k)} + x_4^{(k)}) \\\\\n",
    "x_2^{(k+1)} &= \\frac{1}{10}(15 + 2x_1^{(k)} + x_3^{(k)} + x_4^{(k)}) \\\\\n",
    "x_3^{(k+1)} &= \\frac{1}{10}(27 + x_1^{(k)} + x_2^{(k)} + 2x_4^{(k)}) \\\\\n",
    "x_4^{(k+1)} &= \\frac{1}{10}(-9 + x_1^{(k)} + x_2^{(k)} + 2x_3^{(k)})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "Let's use an initial guess of $x^{(0)} = (0, 0, 0, 0)$. We'll perform a few iterations:\n",
    "\n",
    "**Iteration 1:**\n",
    "\n",
    "\\begin{align*}\n",
    "x_1^{(1)} &= \\frac{1}{10}(3) = 0.3 \\\\\n",
    "x_2^{(1)} &= \\frac{1}{10}(15) = 1.5 \\\\\n",
    "x_3^{(1)} &= \\frac{1}{10}(27) = 2.7 \\\\\n",
    "x_4^{(1)} &= \\frac{1}{10}(-9) = -0.9\n",
    "\\end{align*}\n",
    "\n",
    "$x^{(1)} = (0.3, 1.5, 2.7, -0.9)$\n",
    "\n",
    "**Iteration 2:**\n",
    "\n",
    "\\begin{align*}\n",
    "x_1^{(2)} &= \\frac{1}{10}(3 + 2(1.5) + 2.7 + (-0.9)) = \\frac{1}{10}(7.8) = 0.78 \\\\\n",
    "x_2^{(2)} &= \\frac{1}{10}(15 + 2(0.3) + 2.7 + (-0.9)) = \\frac{1}{10}(17.4) = 1.74 \\\\\n",
    "x_3^{(2)} &= \\frac{1}{10}(27 + 0.3 + 1.5 + 2(-0.9)) = \\frac{1}{10}(27.9) = 2.79 \\\\\n",
    "x_4^{(2)} &= \\frac{1}{10}(-9 + 0.3 + 1.5 + 2(2.7)) = \\frac{1}{10}(-9 + 7.2) = -0.18\n",
    "\\end{align*}\n",
    "\n",
    "$x^{(2)} = (0.78, 1.74, 2.79, -0.18)$\n",
    "\n",
    "**Iteration 3:**\n",
    "\n",
    "\\begin{align*}\n",
    "x_1^{(3)} &= \\frac{1}{10}(3 + 2(1.74) + 2.79 + (-0.18)) = 0.909 \\\\\n",
    "x_2^{(3)} &= \\frac{1}{10}(15 + 2(0.78) + 2.79 + (-0.18)) = 1.917 \\\\\n",
    "x_3^{(3)} &= \\frac{1}{10}(27 + 0.78 + 1.74 + 2(-0.18)) = 2.916 \\\\\n",
    "x_4^{(3)} &= \\frac{1}{10}(-9 + 0.78 + 1.74 + 2(2.79)) = 0.01\n",
    "\\end{align*}\n",
    "\n",
    "$x^{(3)} = (0.909, 1.917, 2.916, 0.01)$\n",
    "\n",
    "As it may take up more space, we'll just skip to **Iteration 10:**\n",
    "\n",
    "So after 10 iterations we get<br>\n",
    "$x^{(10)} = (0.99991, 1.99992, 2.99993, 0.00002)$\n",
    "\n",
    "**Exact Solution:**\n",
    "The exact solution to the system is $x = (1, 2, 3, 0)$.\n",
    "\n",
    "As you can see, the convergence is slow, but the values are approaching the solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how it will be shown in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate solution: [ 9.99999356e-01  1.99999936e+00  2.99999936e+00 -6.44235264e-07]\n",
      "Iterations: 16\n",
      "A*x = [ 2.99999613 14.99999613 26.99999613 -9.00000387]\n",
      "b = [ 3 15 27 -9]\n"
     ]
    }
   ],
   "source": [
    "def jacobi(A, b, x0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Jacobi method for solving Ax = b.\n",
    "\n",
    "    Args:\n",
    "        A: Coefficient matrix (numpy array).\n",
    "        b: Right-hand side vector (numpy array).\n",
    "        x0: Initial guess (numpy array).\n",
    "        tol: Tolerance for convergence.\n",
    "        max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        x: Approximate solution (numpy array).\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(b)\n",
    "    x = x0.copy()\n",
    "    x_new = np.zeros(n)\n",
    "    iteration = 0\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        iteration = k + 1 #track iterations\n",
    "        for i in range(n):\n",
    "            s1 = np.dot(A[i, :i], x[:i])\n",
    "            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n",
    "            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n",
    "\n",
    "        if np.linalg.norm(x_new - x, ord=np.inf) < tol:\n",
    "            return x_new, iteration\n",
    "\n",
    "        x[:] = x_new\n",
    "    return x_new, iteration # returns the value after max_iter reached.\n",
    "\n",
    "# Example usage\n",
    "A = np.array([[10, -2, -1, -1],\n",
    "              [-2, 10, -1, -1],\n",
    "              [-1, -1, 10, -2],\n",
    "              [-1, -1, -2, 10]])\n",
    "\n",
    "b = np.array([3, 15, 27, -9])\n",
    "x0 = np.zeros(4)\n",
    "\n",
    "solution, iterations = jacobi(A, b, x0)\n",
    "print(\"Approximate solution:\", solution)\n",
    "print(\"Iterations:\", iterations)\n",
    "\n",
    "# Check the solution more precisely\n",
    "Ax = np.dot(A, solution)\n",
    "print(\"A*x =\", Ax)\n",
    "print(\"b =\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python code to solve this has a tolerance of $1 \\times 10^{-6}$, which takes about 16 iterations to solve the system.\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "- Updates each variable using values from the **previous iteration**.\n",
    "- Simple but may converge slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Gauss-Seidel Method**\n",
    "\n",
    "The Gauss-Seidel method is an iterative technique used to solve a system of linear equations. It is an improvement over the Jacobi method, as it uses the most recently computed values of the unknowns in subsequent calculations.\n",
    "\n",
    "\n",
    "Consider a system of $n$ linear equations with $n$ unknowns:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\n",
    "a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n",
    "\\vdots \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad & \\vdots \\\\\n",
    "a_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n &= b_n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In matrix form, this can be written as:\n",
    "\n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "\n",
    "where $A$ is the coefficient matrix, $x$ is the vector of unknowns, and $b$ is the vector of constants.\n",
    "\n",
    "### Formula\n",
    "\n",
    "To apply the Gauss-Seidel method, we solve each equation for the corresponding unknown:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1 &= \\frac{1}{a_{11}} \\left( b_1 - a_{12}x_2 - a_{13}x_3 - \\cdots - a_{1n}x_n \\right) \\\\\n",
    "x_2 &= \\frac{1}{a_{22}} \\left( b_2 - a_{21}x_1 - a_{23}x_3 - \\cdots - a_{2n}x_n \\right) \\\\\n",
    "\\vdots \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad & \\vdots \\\\\n",
    "x_n &= \\frac{1}{a_{nn}} \\left( b_n - a_{n1}x_1 - a_{n2}x_2 - \\cdots - a_{n,n-1}x_{n-1} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The iterative formula for the Gauss-Seidel method is:\n",
    "\n",
    "$$\n",
    "x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $x_i^{(k)}$ is the $i$-th component of the solution vector at the $k$-th iteration.\n",
    "* $x_i^{(k+1)}$ is the $i$-th component of the solution vector at the $(k+1)$-th iteration.\n",
    "* The key difference from Jacobi is that we use the most recent values $x_j^{(k+1)}$ for $j < i$ as soon as they are computed.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1.  Choose an initial guess $x^{(0)}$.\n",
    "2.  For $k = 0, 1, 2, \\dots$ until convergence:\n",
    "    * For $i = 1, 2, \\dots, n$:\n",
    "        * Calculate $x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \\right)$.\n",
    "3.  Check for convergence using a suitable criterion, such as:\n",
    "    * $||x^{(k+1)} - x^{(k)}|| < \\epsilon$, where $\\epsilon$ is a small tolerance.\n",
    "    * $||Ax^{(k+1)} - b|| < \\epsilon$.\n",
    "\n",
    "### Convergence\n",
    "\n",
    "The Gauss-Seidel method converges if the matrix $A$ is:\n",
    "\n",
    "* Strictly diagonally dominant: $|a_{ii}| > \\sum_{j=1, j \\neq i}^{n} |a_{ij}|$ for all $i$.\n",
    "* Symmetric positive definite.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Generally converges faster than the Jacobi method.\n",
    "* Uses less memory than some direct methods.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* Convergence is not guaranteed for all matrices.\n",
    "* Can be slower than direct methods for small systems.\n",
    "* The order in which equations are solved can affect convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "we will use the same example as above, but we will solve the system of linear equations using the Gauss-Seidel method:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "10x_1 - 2x_2 - x_3 - x_4 &= 3 \\\\\n",
    "-2x_1 + 10x_2 - x_3 - x_4 &= 15 \\\\\n",
    "-x_1 - x_2 + 10x_3 - 2x_4 &= 27 \\\\\n",
    "-x_1 - x_2 - 2x_3 + 10x_4 &= -9\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Iterative Formulas\n",
    "\n",
    "First, we solve each equation for the corresponding unknown:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1 &= \\frac{1}{10}(3 + 2x_2 + x_3 + x_4) \\\\\n",
    "x_2 &= \\frac{1}{10}(15 + 2x_1 + x_3 + x_4) \\\\\n",
    "x_3 &= \\frac{1}{10}(27 + x_1 + x_2 + 2x_4) \\\\\n",
    "x_4 &= \\frac{1}{10}(-9 + x_1 + x_2 + 2x_3)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Iterations\n",
    "\n",
    "We start with an initial guess of $x^{(0)} = (0, 0, 0, 0)$.\n",
    "\n",
    "**Iteration 1:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1^{(1)} &= \\frac{1}{10}(3 + 2(0) + 0 + 0) = 0.3 \\\\\n",
    "x_2^{(1)} &= \\frac{1}{10}(15 + 2(0.3) + 0 + 0) = 1.56 \\\\\n",
    "x_3^{(1)} &= \\frac{1}{10}(27 + 0.3 + 1.56 + 2(0)) = 2.886 \\\\\n",
    "x_4^{(1)} &= \\frac{1}{10}(-9 + 0.3 + 1.56 + 2(2.886)) = -0.1368\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$x^{(1)} = (0.3, 1.56, 2.886, -0.1368)$\n",
    "\n",
    "**Iteration 2:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1^{(2)} &= \\frac{1}{10}(3 + 2(1.56) + 2.886 - 0.1368) = 0.88692 \\\\\n",
    "x_2^{(2)} &= \\frac{1}{10}(15 + 2(0.88692) + 2.886 - 0.1368) = 1.952304 \\\\\n",
    "x_3^{(2)} &= \\frac{1}{10}(27 + 0.88692 + 1.952304 + 2(-0.1368)) = 2.9698624 \\\\\n",
    "x_4^{(2)} &= \\frac{1}{10}(-9 + 0.88692 + 1.952304 + 2(2.9698624)) = 0.084894928\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$x^{(2)} = (0.88692, 1.952304, 2.9698624, 0.084894928)$\n",
    "\n",
    "**Iteration 3:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1^{(3)} &= \\frac{1}{10}(3 + 2(1.952304) + 2.9698624 + 0.084894928) = 0.995936576 \\\\\n",
    "x_2^{(3)} &= \\frac{1}{10}(15 + 2(0.995936576) + 2.9698624 + 0.084894928) = 1.9996590552 \\\\\n",
    "x_3^{(3)} &= \\frac{1}{10}(27 + 0.995936576 + 1.9996590552 + 2(0.084894928)) = 2.99652854864 \\\\\n",
    "x_4^{(3)} &= \\frac{1}{10}(-9 + 0.995936576 + 1.9996590552 + 2(2.99652854864)) = 0.0988652727488 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$x^{(3)} = (0.995936576, 1.9996590552, 2.99652854864, 0.0988652727488)$\n",
    "\n",
    "After a few more iterations, the solution converges to approximately:\n",
    "\n",
    "$$\n",
    "x \\approx (1, 2, 3, 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution:\n",
      "[ 9.99999890e-01  1.99999994e+00  2.99999995e+00 -2.65476061e-08]\n",
      "Number of iterations: 10\n",
      "Verification: Ax-b\n",
      "[-8.99262862e-07 -3.32124884e-07 -2.40979077e-07  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "def gauss_seidel(A, b, x0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Solves a system of linear equations Ax = b using the Gauss-Seidel method.\n",
    "\n",
    "    Args:\n",
    "        A: The coefficient matrix (numpy array).\n",
    "        b: The constant vector (numpy array).\n",
    "        x0: The initial guess vector (numpy array).\n",
    "        max_iter: The maximum number of iterations.\n",
    "        tolerance: The tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the solution vector (numpy array) and the number of iterations,\n",
    "        or (None, number of iterations) if convergence fails.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(b)\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    num_iter = 0\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        num_iter += 1\n",
    "        for i in range(n):\n",
    "            s1 = sum(A[i][j] * x_new[j] for j in range(i))\n",
    "            s2 = sum(A[i][j] * x[j] for j in range(i + 1, n))\n",
    "            x_new[i] = (b[i] - s1 - s2) / A[i][i]\n",
    "\n",
    "        if np.linalg.norm(x_new - x) < tol:\n",
    "            return x_new, num_iter\n",
    "\n",
    "        x = x_new.copy()\n",
    "\n",
    "    return None, num_iter  # Convergence failed\n",
    "\n",
    "# Example system of equations\n",
    "A = np.array([[10, -2, -1, -1],\n",
    "              [-2, 10, -1, -1],\n",
    "              [-1, -1, 10, -2],\n",
    "              [-1, -1, -2, 10]], dtype=float)\n",
    "\n",
    "b = np.array([3, 15, 27, -9], dtype=float)\n",
    "x0 = np.array([0, 0, 0, 0], dtype=float)\n",
    "\n",
    "# Solve using Gauss-Seidel\n",
    "solution, iterations = gauss_seidel(A, b, x0)\n",
    "\n",
    "if solution is not None:\n",
    "    print(\"Solution:\")\n",
    "    print(solution)\n",
    "    print(\"Number of iterations:\", iterations)\n",
    "    print(\"Verification: Ax-b\")\n",
    "    print(np.dot(A,solution)-b)\n",
    "\n",
    "else:\n",
    "    print(\"Gauss-Seidel method did not converge within the given iterations.\")\n",
    "    print(\"Number of iterations performed:\", iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code illustrates the Gauss-Seidel method's efficiency compared to the Jacobi method, as evidenced by the reduced number of iterations for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- **Iterative Solution:** Gauss-Seidel solves linear equation systems by iteratively refining an initial guess.\n",
    "- **Immediate Updates:** It improves on the Jacobi method by using newly calculated variable values within the same iteration.\n",
    "- **Convergence Criteria:** Convergence is guaranteed for strictly diagonally dominant or symmetric positive definite matrices.\n",
    "- **Performance:** Generally faster than Jacobi, but convergence is not universally guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Successive Over-Relaxation (SOR)**\n",
    "\n",
    "Successive Over-Relaxation (SOR) is an iterative method used to solve a system of linear equations, particularly large sparse systems. It's a variant of the Gauss-Seidel method, designed to accelerate its convergence.\n",
    "\n",
    "### Formula\n",
    "\n",
    "Consider a system of linear equations:\n",
    "\n",
    "$$Ax = b$$\n",
    "\n",
    "where $A$ is an $n \\times n$ matrix, $x$ is an $n \\times 1$ vector of unknowns, and $b$ is an $n \\times 1$ vector of constants.\n",
    "\n",
    "We can decompose the matrix $A$ into its diagonal ($D$), lower triangular ($L$), and upper triangular ($U$) components:\n",
    "\n",
    "$$A = D - L - U$$\n",
    "\n",
    "\n",
    "The Gauss-Seidel method updates the solution iteratively using the following formula:\n",
    "\n",
    "$$x^{(k+1)} = (D - L)^{-1}Ux^{(k)} + (D - L)^{-1}b$$\n",
    "\n",
    "In component form, this is:\n",
    "\n",
    "$$x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \\right)$$\n",
    "\n",
    "\n",
    "The SOR method introduces a relaxation parameter $\\omega$ to accelerate convergence:\n",
    "\n",
    "$$x^{(k+1)} = (D - \\omega L)^{-1} \\left[ (1 - \\omega)D + \\omega U \\right] x^{(k)} + \\omega (D - \\omega L)^{-1}b$$\n",
    "\n",
    "In component form:\n",
    "\n",
    "$$x_i^{(k+1)} = (1 - \\omega)x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \\right)$$\n",
    "\n",
    "Or, equivalently:\n",
    "\n",
    "$$x_i^{(k+1)} = x_i^{(k)} + \\omega \\left( \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i}^{n} a_{ij}x_j^{(k)} \\right) \\right)$$\n",
    "\n",
    "where $\\omega$ is the relaxation parameter.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "-   If $\\omega = 1$, SOR reduces to the Gauss-Seidel method.\n",
    "-   If $\\omega < 1$, it's called under-relaxation, which can be used to ensure convergence for some systems.\n",
    "-   If $\\omega > 1$, it's called over-relaxation, which aims to accelerate convergence.\n",
    "\n",
    "The optimal value of $\\omega$ depends on the properties of the matrix $A$. For certain classes of matrices (e.g., symmetric positive-definite matrices), an optimal $\\omega$ can be determined analytically. For general matrices, it often requires numerical experimentation.\n",
    "\n",
    "### Convergence\n",
    "\n",
    "The convergence of SOR depends on the choice of $\\omega$ and the properties of the matrix $A$. For SOR to converge, the spectral radius of the iteration matrix must be less than 1.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**<br>\n",
    "-   Can significantly accelerate convergence compared to Gauss-Seidel.\n",
    "-   Relatively simple to implement.\n",
    "\n",
    "**Disadvantages:**<br>\n",
    "-   Finding the optimal $\\omega$ can be challenging.\n",
    "-   Convergence is not guaranteed for all matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Using the example from the previous methods:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "10x_1 - 2x_2 - x_3 - x_4 &= 3 \\\\\n",
    "-2x_1 + 10x_2 - x_3 - x_4 &= 15 \\\\\n",
    "-x_1 - x_2 + 10x_3 - 2x_4 &= 27 \\\\\n",
    "-x_1 - x_2 - 2x_3 + 10x_4 &= -9\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**SOR Formula Recap:**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_1^{(k+1)} &= (1 - \\omega)x_1^{(k)} + \\frac{\\omega}{10}\\left(3 + 2x_2^{(k+1)} + x_3^{(k)} + x_4^{(k)}\\right) \\\\\n",
    "x_2^{(k+1)} &= (1 - \\omega)x_2^{(k)} + \\frac{\\omega}{10}\\left(15 + 2x_1^{(k+1)} + x_3^{(k)} + x_4^{(k)}\\right) \\\\\n",
    "x_3^{(k+1)} &= (1 - \\omega)x_3^{(k)} + \\frac{\\omega}{10}\\left(27 + x_1^{(k+1)} + x_2^{(k+1)} + 2x_4^{(k)}\\right) \\\\\n",
    "x_4^{(k+1)} &= (1 - \\omega)x_4^{(k)} + \\frac{\\omega}{10}\\left(-9 + x_1^{(k+1)} + x_2^{(k+1)} + 2x_3^{(k+1)}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "**Initial Guess:**\n",
    "\n",
    "$$\n",
    "x_1^{(0)} = 0, \\quad x_2^{(0)} = 0, \\quad x_3^{(0)} = 0, \\quad x_4^{(0)} = 0\n",
    "$$\n",
    "\n",
    "Weâ€™ll use **\\($ \\omega = 1.25 $\\)** as the relaxation factor (a common value).\n",
    "\n",
    "#### **Solution**\n",
    "\n",
    "**Iteration 1:**\n",
    "\n",
    "$$\n",
    "x_1^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(3 + 2(0) + 0 + 0) = 0 + 0.125(3) = 0.375\n",
    "$$\n",
    "$$\n",
    "x_2^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(15 + 2(0.375) + 0 + 0) = 1.96875\n",
    "$$\n",
    "$$\n",
    "x_3^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(27 + 0.375 + 1.96875 + 0) = 3.66796875\n",
    "$$\n",
    "$$\n",
    "x_4^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(-9 + 0.375 + 1.96875 + 2(3.66796875)) = 0.0849609375\n",
    "$$\n",
    "\n",
    "**Iteration 1 Result:**\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(1)} = 0.375 \\\\\n",
    "x_2^{(1)} = 1.96875 \\\\\n",
    "x_3^{(1)} = 3.66796875 \\\\\n",
    "x_4^{(1)} = 0.0849609375\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Iteration 2:**\n",
    "\n",
    "\n",
    "$$\n",
    "x_1^{(2)} = (1 - 1.25)(0.375) + 0.125(3 + 2(1.96875) + 3.66796875 + 0.0849609375) = 1.2425537109375\n",
    "$$\n",
    "$$\n",
    "x_2^{(2)} = (1 - 1.25)(1.96875) + 0.125(15 + 2(1.2425537109375) + 3.66796875 + 0.0849609375) = 2.16244189453125\n",
    "$$\n",
    "$$\n",
    "x_3^{(2)} = (1 - 1.25)(3.66796875) + 0.125(27 + 1.2425537109375 + 2.16244189453125 + 2(0.0849609375)) = 2.9048724975585938 \n",
    "$$\n",
    "$$\n",
    "x_4^{(2)} = (1 - 1.25)(0.0849609375) + 0.125(-9 + 1.2425537109375 + 2.16244189453125 + 2(2.9048724975585938)) = 0.005202293396\n",
    "$$\n",
    "\n",
    "**Iteration 2 Result:**\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(2)} â‰ˆ 1.2426 \\\\\n",
    "x_2^{(2)} â‰ˆ 2.1624 \\\\\n",
    "x_3^{(2)} â‰ˆ 2.9049 \\\\\n",
    "x_4^{(2)} â‰ˆ 0.0052\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Iteration 3:**\n",
    "$$\n",
    "x_1^{(3)} = (1 - 1.25)(1.2425537109375) + 0.125(3 + 2(2.16244189453125) + 2.9048724975585938 + 0.005202293396) = 0.9688563947677\n",
    "$$\n",
    "$$\n",
    "x_2^{(3)} = (1 - 1.25)(2.16244189453125) + 0.125(15 + 2(0.9688563947677) + 2.9048724975585938 + 0.005202293396) = 1.9403629738789\n",
    "$$\n",
    "$$\n",
    "x_3^{(3)} = (1 - 1.25)(2.9048724975585938) + 0.125(27 + 0.9688563947677 + 1.9403629738789 + 2(0.005202293396)) = 3.0137349575401\n",
    "$$\n",
    "$$\n",
    "x_4^{(3)} = (1 - 1.25)(0.005202293396) + 0.125(-9 + 0.9688563947677 + 1.9403629738789 + 2(3.0137349575401)) = 0.1153360380873\n",
    "$$\n",
    "\n",
    "**Iteration 3 Result:**\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(3)} â‰ˆ 0.9689 \\\\\n",
    "x_2^{(3)} â‰ˆ 1.9404 \\\\\n",
    "x_3^{(3)} â‰ˆ 3.0137 \\\\\n",
    "x_4^{(3)} â‰ˆ 0.1153\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This will converge eventually to approximately:\n",
    "\n",
    "$$x \\approx (1, 2, 3, 1)^T$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how it will look in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [1.0000000557875983, 2.000000137898985, 2.99999986169086, 3.934596124183633e-08]\n",
      "Iterations: 13\n"
     ]
    }
   ],
   "source": [
    "def sor(A, b, x0, omega, tol=1e-6, max_iter=100):\n",
    "    n = len(b)\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        for i in range(n):\n",
    "            s1 = sum(A[i][j] * x_new[j] for j in range(i))\n",
    "            s2 = sum(A[i][j] * x[j] for j in range(i + 1, n))\n",
    "            x_new[i] = (1 - omega) * x[i] + omega * (b[i] - s1 - s2) / A[i][i]\n",
    "\n",
    "        if max(abs(x_new[i] - x[i]) for i in range(n)) < tol:\n",
    "            return x_new, k + 1\n",
    "\n",
    "        x = x_new.copy()\n",
    "\n",
    "    return x_new, max_iter\n",
    "\n",
    "A = [[10, -2, -1, -1],\n",
    "     [-2, 10, -1, -1],\n",
    "     [-1, -1, 10, -2],\n",
    "     [-1, -1, -2, 10]]\n",
    "\n",
    "b = [3, 15, 27, -9]\n",
    "x0 = [0, 0, 0, 0]\n",
    "omega = 1.25\n",
    "\n",
    "solution, iterations = sor(A, b, x0, omega)\n",
    "print(\"Solution:\", solution)\n",
    "print(\"Iterations:\", iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- **Iterative Refinement:** SOR is an iterative technique used to solve systems of linear equations by progressively refining an initial guess until a solution is reached.\n",
    "- **Relaxation Parameter (Ï‰):** It introduces a relaxation parameter (Ï‰) to accelerate convergence, where values between 1 and 2 (typically) are used to \"over-correct\" the solution at each iteration.\n",
    "- **Sequential Updates:** SOR updates each variable sequentially, using the most recently computed values of other variables within the same iteration, thus incorporating immediate feedback.\n",
    "- **Enhanced Convergence:** By strategically applying the relaxation parameter, SOR aims to achieve faster convergence compared to the Gauss-Seidel method, particularly for certain types of matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. **Symmetric Successive Over-Relaxation (SSOR)**\n",
    "\n",
    "The Symmetric Successive Over-Relaxation (SSOR) method is an iterative technique used to solve systems of linear equations of the form:\n",
    "\n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "\n",
    "where $A$ is a matrix, $x$ is the unknown vector, and $b$ is a known vector.\n",
    "\n",
    "### Matrix Decomposition\n",
    "\n",
    "The matrix $A$ is decomposed into:\n",
    "\n",
    "$$\n",
    "A = D + L + U\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $D$ is the diagonal matrix.\n",
    "- $L$ is the strictly lower triangular matrix.\n",
    "- $U$ is the strictly upper triangular matrix.\n",
    "\n",
    "### SSOR Iteration\n",
    "\n",
    "The SSOR iteration consists of two steps: a forward SOR sweep and a backward SOR sweep.<br>\n",
    "The forward SOR sweep is given by:\n",
    "\n",
    "$$\n",
    "x^{(k+1/2)} = (D + \\omega L)^{-1} \\left[ (1 - \\omega) D - \\omega U \\right] x^{(k)} + \\omega (D + \\omega L)^{-1} b\n",
    "$$\n",
    "\n",
    "The backward SOR sweep is given by:\n",
    "\n",
    "$$\n",
    "x^{(k+1)} = (D + \\omega U)^{-1} \\left[ (1 - \\omega) D - \\omega L \\right] x^{(k+1/2)} + \\omega (D + \\omega U)^{-1} b\n",
    "$$\n",
    "\n",
    "where $\\omega$ is the relaxation parameter.\n",
    "\n",
    "### SSOR Iteration Matrix\n",
    "\n",
    "The SSOR iteration matrix $S_\\omega$ is given by:\n",
    "\n",
    "$$\n",
    "S_\\omega = (D + \\omega U)^{-1} \\left[ (1 - \\omega) D - \\omega L \\right] (D + \\omega L)^{-1} \\left[ (1 - \\omega) D - \\omega U \\right]\n",
    "$$\n",
    "\n",
    "### Preconditioning\n",
    "\n",
    "SSOR is often used as a preconditioner for other iterative methods. The SSOR preconditioned system is:\n",
    "\n",
    "$$\n",
    "M^{-1} A x = M^{-1} b\n",
    "$$\n",
    "\n",
    "where $M$ is the SSOR preconditioner, given by:\n",
    "\n",
    "$$\n",
    "M = \\frac{1}{\\omega(2-\\omega)}(D+\\omega L)D^{-1}(D+\\omega U)\n",
    "$$\n",
    "\n",
    "### Convergence\n",
    "\n",
    "The convergence of SSOR depends on the properties of the matrix $A$ and the relaxation parameter $\\omega$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Using same example:\n",
    "\n",
    "**Given:**\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "10 & -2 & -1 & -1 \\\\\n",
    "-2 & 10 & -1 & -1 \\\\\n",
    "-1 & -1 & 10 & -2 \\\\\n",
    "-1 & -1 & -2 & 10\n",
    "\\end{pmatrix}, \\quad\n",
    "\\mathbf{b} = \\begin{pmatrix}\n",
    "3 \\\\ 15 \\\\ 27 \\\\ -9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Relaxation factor \\($ \\omega = 1.25 $\\)\n",
    "\n",
    "Initial guess:  \n",
    "$$\n",
    "\\mathbf{x}^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**General formula for SOR:**<br>\n",
    "\n",
    "$$\n",
    "x_i^{(k+1)} = (1 - \\omega)x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j < i} a_{ij}x_j^{(k+1)} - \\sum_{j > i} a_{ij}x_j^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "SSOR:\n",
    "1. **Forward sweep (1 â†’ 4)**\n",
    "2. **Backward sweep (4 â†’ 1)**\n",
    "\n",
    "**Solution:**<br>\n",
    "\n",
    "\n",
    "**Iteration 1:**<br>\n",
    "\n",
    "**Forward Sweep:**\n",
    "\n",
    "$$\n",
    "x_1^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(3 + 2(0) + 0 + 0) = 0 + 0.125(3) = 0.375\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(15 + 2(0.375) + 0 + 0) = 0 + 0.125(15 + 0.75) = 1.96875\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(27 + 0.375 + 1.96875 + 0) = 0 + 0.125(29.34375) = 3.66796875\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_4^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(-9 + 0.375 + 1.96875 + 2(3.66796875)) = 0 + 0.125(0.6796875) = 0.0849609375\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Backward Sweep:**\n",
    "\n",
    "$$\n",
    "x_4^{(1)} = (1 - 1.25)(0.0849609375) + \\frac{1.25}{10}(-9 + 0.375 + 1.96875 + 2(3.66796875)) = 0.0849609375\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3^{(1)} = (1 - 1.25)(3.66796875) + \\frac{1.25}{10}(27 + 0.375 + 1.96875 + 2(0.0849609375)) = 2.764892578125\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{(1)} = (1 - 1.25)(1.96875) + \\frac{1.25}{10}(15 + 2(0.375) + 2.764892578125 + 0.0849609375) = 1.8377685546875\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_1^{(1)} = (1 - 1.25)(0.375) + \\frac{1.25}{10}(3 + 2(1.8377685546875) + 2.764892578125 + 0.0849609375) = 1.096466064453125\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Iteration 1 Result:**\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(1)} = 1.096466064453125 \\\\\n",
    "x_2^{(1)} = 1.8377685546875 \\\\\n",
    "x_3^{(1)} = 2.764892578125 \\\\\n",
    "x_4^{(1)} = 0.0849609375\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Iteration 2:**<br>\n",
    "\n",
    "**Forward Sweep:**\n",
    "\n",
    "$$\n",
    "x_1^{(2)} = (1 - 1.25)(1.096466064453125) + \\frac{1.25}{10}(3 + 2(1.8377685546875) + 2.764892578125 + 0.0849609375) = 1.675684928894043\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{(2)} = (1 - 1.25)(1.8377685546875) + \\frac{1.25}{10}(15 + 2(1.675684928894043) + 2.764892578125 + 0.0849609375) = 2.466885447502136\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3^{(2)} = (1 - 1.25)(2.764892578125) + \\frac{1.25}{10}(27 + 1.675684928894043 + 2.466885447502136 + 2(0.0849609375)) = 3.241812765598297\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_4^{(2)} = (1 - 1.25)(0.0849609375) + \\frac{1.25}{10}(-9 + 1.675684928894043 + 2.466885447502136 + 2(3.241812765598297)) = 0.9704861893057823\n",
    "$$\n",
    "\n",
    "\n",
    "**Backward Sweep:**\n",
    "\n",
    "$$\n",
    "x_4^{(2)} = 0.9704861893057823\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3^{(2)} = (1 - 1.25)(3.241812765598297) + \\frac{1.25}{10}(27 + 1.675684928894043 + 2.466885447502136 + 2(0.9704861893057823)) = 3.170768279492855\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{(2)} = (1 - 1.25)(2.466885447502136) + \\frac{1.25}{10}(15 + 2(1.675684928894043) + 3.170768279492855 + 0.9704861893057823) = 2.344833124428034\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_1^{(2)} = (1 - 1.25)(1.675684928894043) + \\frac{1.25}{10}(3 + 2(2.344833124428034) + 3.170768279492855 + 0.9704861893057823) = 1.6019394582211971\n",
    "$$\n",
    "\n",
    "**Iteration 2 Result:**\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(2)} = 1.6019394582211971 \\\\\n",
    "x_2^{(2)} = 2.344833124428034 \\\\\n",
    "x_3^{(2)} = 3.170768279492855 \\\\\n",
    "x_4^{(2)} = 0.9704861893057823\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "**Iteration 3:**<br>\n",
    "\n",
    "**Forward Sweep:**\n",
    "\n",
    "$$\n",
    "x_1^{(3)} = (1 - 1.25)(1.6019394582211971) + \\frac{1.25}{10}(3 + 2(2.344833124428034) + 3.170768279492855 + 0.9704861893057823) = 1.6020043636265095\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{(3)} = (1 - 1.25)(2.344833124428034) + \\frac{1.25}{10}(15 + 2(1.6020043636265095) + 3.170768279492855 + 0.9704861893057823) = 2.3439992931822126\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3^{(3)} = (1 - 1.25)(3.170768279492855) + \\frac{1.25}{10}(27 + 1.6020043636265095 + 2.3439992931822126 + 2(0.9704861893057823)) = 3.1700007750166693\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_4^{(3)} = (1 - 1.25)(0.9704861893057823) + \\frac{1.25}{10}(-9 + 1.6020043636265095 + 2.3439992931822126 + 2(3.1700007750166693)) = 0.9600021139037759\n",
    "$$\n",
    "\n",
    "**Backward Sweep:**\n",
    "\n",
    "$$\n",
    "x_4^{(3)} = 0.9600021139037759\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3^{(3)} = (1 - 1.25)(3.1700007750166693) + \\frac{1.25}{10}(27 + 1.6020043636265095 + 2.3439992931822126 + 2(0.9600021139037759)) = 3.170000002953419\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{(3)} = (1 - 1.25)(2.3439992931822126) + \\frac{1.25}{10}(15 + 2(1.6020043636265095) + 3.170000002953419 + 0.9600021139037759) = 2.3440000004163684\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_1^{(3)} = (1 - 1.25)(1.6020043636265095) + \\frac{1.25}{10}(3 + 2(2.3440000004163684) + 3.170000002953419 + 0.9600021139037759) = 1.6020000000535157\n",
    "$$\n",
    "\n",
    "**Iteration 3 Result:**\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(3)} = 1.6020000000535157 \\\\\n",
    "x_2^{(3)} = 2.3440000004163684 \\\\\n",
    "x_3^{(3)} = 3.170000002953419 \\\\\n",
    "x_4^{(3)} = 0.9600021139037759\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Which will converge eventually to:\n",
    "$$x \\approx (1, 2, 3, 1)^T$$\n",
    "\n",
    "We will code this in Python and check how many iterations it takes to converge:\n",
    "\n",
    "**Python Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 9 iterations\n",
      "Solution: [ 1.00000001e+00  1.99999998e+00  2.99999999e+00 -1.16701435e-08]\n"
     ]
    }
   ],
   "source": [
    "# Define matrix A and vector b\n",
    "A = np.array([\n",
    "    [10, -2, -1, -1],\n",
    "    [-2, 10, -1, -1],\n",
    "    [-1, -1, 10, -2],\n",
    "    [-1, -1, -2, 10]\n",
    "], dtype=float)\n",
    "\n",
    "b = np.array([3, 15, 27, -9], dtype=float)\n",
    "\n",
    "# Parameters\n",
    "omega = 1.25  # Relaxation factor (can tweak between 1 < omega < 2 for convergence)\n",
    "tol = 1e-6   # Convergence tolerance\n",
    "max_iterations = 1000\n",
    "\n",
    "# Initial guess\n",
    "x = np.zeros(len(b))\n",
    "\n",
    "# Precompute matrices\n",
    "D = np.diag(np.diag(A))\n",
    "L = np.tril(A, -1)\n",
    "U = np.triu(A, 1)\n",
    "\n",
    "# SSOR iterative process\n",
    "for iteration in range(max_iterations):\n",
    "    x_old = x.copy()\n",
    "\n",
    "    # Forward SOR\n",
    "    for i in range(len(A)):\n",
    "        sum1 = np.dot(A[i, :i], x[:i])\n",
    "        sum2 = np.dot(A[i, i+1:], x_old[i+1:])\n",
    "        x[i] = (1 - omega) * x_old[i] + (omega / A[i, i]) * (b[i] - sum1 - sum2)\n",
    "\n",
    "    # Backward SOR\n",
    "    for i in reversed(range(len(A))):\n",
    "        sum1 = np.dot(A[i, :i], x[:i])\n",
    "        sum2 = np.dot(A[i, i+1:], x[i+1:])\n",
    "        x[i] = (1 - omega) * x[i] + (omega / A[i, i]) * (b[i] - sum1 - sum2)\n",
    "\n",
    "    # Check convergence\n",
    "    if np.linalg.norm(x - x_old, ord=np.inf) < tol:\n",
    "        print(f\"Converged in {iteration + 1} iterations\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Did not converge within the maximum number of iterations\")\n",
    "\n",
    "print(\"Solution:\", x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Accelerated Over-Relaxation (AOR)**\n",
    "- Generalizes SOR by using **two parameters** for additional flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Accelerated Over-Relaxation (AOR) method is an iterative technique used to solve linear systems of equations $Ax = b$ by introducing two relaxation parameters, $\\omega$ and $\\gamma$, to accelerate convergence beyond the Successive Over-Relaxation (SOR) method.\n",
    "\n",
    "### Formula\n",
    "Given the decomposition $A = D - L - U$, where $D$ is diagonal, $L$ is lower triangular, and $U$ is upper triangular, the AOR iteration is:\n",
    "\n",
    "$$x^{(k+1)} = (D - \\omega \\gamma L)^{-1} [(\\omega \\gamma U + (1 - \\omega \\gamma) D) x^{(k)} + \\omega b]$$\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "$$x^{(k+1)} = x^{(k)} + \\omega (D - \\gamma L)^{-1} (b - Ax^{(k)})$$\n",
    "\n",
    "where $\\omega$ and $\\gamma$ are relaxation parameters.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. **Initialize** $x^{(0)}$.\n",
    "2. **Choose** relaxation parameters $\\omega$ and $\\gamma$.\n",
    "3. **Iterate** for $k = 0, 1, 2, \\dots$ until convergence:\n",
    "    1. Update:\n",
    "    \n",
    "        $$\n",
    "        x^{(k+1)} = (D - \\omega \\gamma L)^{-1} \\left[ (\\omega \\gamma U + (1 - \\omega \\gamma) D) x^{(k)} + \\omega b \\right]\n",
    "        $$\n",
    "    \n",
    "    2. Check convergence criterion:\n",
    "    \n",
    "        $$\n",
    "        \\| x^{(k+1)} - x^{(k)} \\| < \\text{tolerance}\n",
    "        $$\n",
    "4. **Return** $x^{(k+1)}$.\n",
    "\n",
    "\n",
    "### Convergence\n",
    "The AOR method converges if and only if the spectral radius of the iteration matrix $T_{\\omega, \\gamma}$ is less than 1:\n",
    "\n",
    "$$\\rho(T_{\\omega, \\gamma}) < 1$$\n",
    "\n",
    "where $T_{\\omega, \\gamma} = (D - \\omega \\gamma L)^{-1} (\\omega \\gamma U + (1 - \\omega \\gamma) D)$. The optimal values of $\\omega$ and $\\gamma$ are problem-dependent.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "#### Advantages\n",
    "- Potentially faster convergence than Gauss-Seidel and SOR with optimal $\\omega$ and $\\gamma$.\n",
    "- Increased flexibility with two relaxation parameters.\n",
    "\n",
    "\n",
    "#### Disadvantages\n",
    "- **Optimal** $\\omega$ and $\\gamma$ are difficult to determine.\n",
    "- **Convergence** is highly sensitive to the choice of $\\omega$ and $\\gamma$.\n",
    "- **Higher computational cost** per iteration compared to Gauss-Seidel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Using the same Example but using AOR to solve the problem\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "**Step 1: Write the system in matrix form**<br>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "10 & -2 & -1 & -1 \\\\\n",
    "-2 & 10 & -1 & -1 \\\\\n",
    "-1 & -1 & 10 & -2 \\\\\n",
    "-1 & -1 & -2 & 10\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3 \\\\ 15 \\\\ 27 \\\\ -9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Denote:\n",
    "- Coefficient matrix: \\($ A $\\)\n",
    "- Solution vector: \\( $ \\mathbf{x} = [x_1, x_2, x_3, x_4]^T $\\)\n",
    "- RHS vector: \\( $\\mathbf{b} = [3, 15, 27, -9]^T $\\)\n",
    "\n",
    "\n",
    "**Step 2: Split matrix A**<br>\n",
    "\n",
    "Split \\($ A = D - L - U $\\), where:\n",
    "- \\($ D $\\) is diagonal,\n",
    "- \\($ L $\\) is strictly lower triangular,\n",
    "- \\($ U $\\) is strictly upper triangular.\n",
    "\n",
    "$$\n",
    "D = \n",
    "\\begin{bmatrix}\n",
    "10 & 0 & 0 & 0 \\\\\n",
    "0 & 10 & 0 & 0 \\\\\n",
    "0 & 0 & 10 & 0 \\\\\n",
    "0 & 0 & 0 & 10\n",
    "\\end{bmatrix}, \\quad\n",
    "L = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "2 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 2 & 0\n",
    "\\end{bmatrix}, \\quad\n",
    "U = \n",
    "\\begin{bmatrix}\n",
    "0 & 2 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 2 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "(Note: Negative signs absorbed accordingly.)\n",
    "\n",
    "\n",
    "\n",
    "**Step 3: AOR formula**<br>\n",
    "\n",
    "The **AOR iteration formula** is:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = (D - \\omega L)^{-1} \\left[ (1 - \\omega) D + \\omega U \\right] \\mathbf{x}^{(k)} + \\omega (D - \\omega L)^{-1} \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($ \\omega $\\) = relaxation factor (commonly \\($ 1 < \\omega < 2 $\\))\n",
    "- Start with an initial guess \\($ \\mathbf{x}^{(0)} = [0, 0, 0, 0]^{T} $\\)\n",
    "\n",
    "\n",
    "\n",
    "**Step 4: Choose parameters**<br>\n",
    "\n",
    "For **SOR (Successive Over-Relaxation)**, \\($ \\omega = 1.25 $\\) is often a good choice, but for **AOR**, there is also a parameter \\($ \\lambda $\\). AOR generalizes SOR and Gauss-Seidel.\n",
    "\n",
    "However, AOR is usually applied in a modified form:\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = (1 - \\lambda) \\mathbf{x}^{(k)} + \\lambda \\cdot \\mathbf{T} \\mathbf{x}^{(k)} + \\mathbf{C}\n",
    "$$\n",
    "\n",
    "To keep it manageable, let's use SOR (as a special case of AOR with \\($ \\lambda = 1 $\\)).\n",
    "\n",
    "**Step 5: Iterative process**<br>\n",
    "\n",
    "**SOR formula for each variable:**\n",
    "$$\n",
    "x_i^{(k+1)} = (1 - \\omega) x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j < i} a_{ij} x_j^{(k+1)} - \\sum_{j > i} a_{ij} x_j^{(k)} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration 1:**<br>\n",
    "\n",
    "$$\n",
    "x_1^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(3 + 2(0) + 0 + 0) = 0 + 0.125(3) = 0.375\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(15 + 2(0.375) + 0 + 0) = 0 + 0.125(15 + 0.75) = 1.96875\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(27 + 0.375 + 1.96875 + 0) = 0 + 0.125(29.34375) = 3.66796875\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_4^{(1)} = (1 - 1.25)(0) + \\frac{1.25}{10}(-9 + 0.375 + 1.96875 + 2(3.66796875)) = 0 + 0.125(0.6796875) = 0.0849609375\n",
    "$$\n",
    "\n",
    "**Iteration 1 Result:**<br>\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(1)} = 0.375 \\\\\n",
    "x_2^{(1)} = 1.96875 \\\\\n",
    "x_3^{(1)} = 3.66796875 \\\\\n",
    "x_4^{(1)} = 0.0849609375\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Iteration 2:**<br>\n",
    "\n",
    "$$\n",
    "x_1^{(2)} = (1 - 1.25)(0.375) + \\frac{1.25}{10}(3 + 2(1.96875) + 1(3.66796875) + 1(0.0849609375)) = 1.242553710938\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{(2)} = (1 - 1.25)(1.96875) + \\frac{1.25}{10}(15 + 2(1.242553710938) + 1(3.66796875) + 1(0.0849609375)) = 2.162561035157\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3^{(2)} = (1 - 1.25)(3.66796875) + \\frac{1.25}{10}(27 + 1(1.242553710938) + 1(2.162561035157) + 2(0.0849609375)) = 3.902415527387\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_4^{(2)} = (1 - 1.25)(0.0849609375) + \\frac{1.25}{10}(-9 + 1(1.242553710938) + 1(2.162561035157) + 2(3.902415527387)) = 0.255702941884\n",
    "$$\n",
    "\n",
    "**Iteration 2 Result:**<br>\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(2)} = 1.242553710938 \\\\\n",
    "x_2^{(2)} = 2.162561035157 \\\\\n",
    "x_3^{(2)} = 3.902415527387 \\\\\n",
    "x_4^{(2)} = 0.255702941884\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## **Iteration 3:**<br>\n",
    "\n",
    "$$\n",
    "x_1^{(3)} = (1 - 1.25)(1.242553710938) + \\frac{1.25}{10}(3 + 2(2.162561035157) + 1(3.902415527387) + 1(0.255702941884)) = 1.124766643465\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{(3)} = (1 - 1.25)(2.162561035157) + \\frac{1.25}{10}(15 + 2(1.124766643465) + 1(3.902415527387) + 1(0.255702941884)) = 2.136316710236\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_3^{(3)} = (1 - 1.25)(3.902415527387) + \\frac{1.25}{10}(27 + 1(1.124766643465) + 1(2.136316710236) + 2(0.255702941884)) = 2.896957722992\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_4^{(3)} = (1 - 1.25)(0.255702941884) + \\frac{1.25}{10}(-9 + 1(1.124766643465) + 1(2.136316710236) + 2(2.896957722992)) = -0.05795038548\n",
    "$$\n",
    "\n",
    "**Iteration 3 Result:**<br>\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(3)} = 1.124766643465 \\\\\n",
    "x_2^{(3)} = 2.136316710236 \\\\\n",
    "x_3^{(3)} = 2.896957722992 \\\\\n",
    "x_4^{(3)} = -0.05795038548\n",
    "\\end{cases}\n",
    "$$\n",
    "This, same as above methods, will converge eventually to approximately:\n",
    "\n",
    "$$x \\approx (1, 2, 3, 1)^T$$\n",
    "\n",
    "We will code this in Python and check how many iterations it takes to converge:\n",
    "\n",
    "**Python Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution after 13 iterations:\n",
      "[ 9.99999533e-01  1.99999963e+00  3.00000045e+00 -1.98849693e-07]\n"
     ]
    }
   ],
   "source": [
    "# Coefficient matrix\n",
    "A = np.array([\n",
    "    [10, -2, -1, -1],\n",
    "    [-2, 10, -1, -1],\n",
    "    [-1, -1, 10, -2],\n",
    "    [-1, -1, -2, 10]\n",
    "], dtype=float)\n",
    "\n",
    "# Right-hand side vector\n",
    "b = np.array([3, 15, 27, -9], dtype=float)\n",
    "\n",
    "# Initial guess\n",
    "x = np.zeros(4)\n",
    "\n",
    "# Parameters\n",
    "omega = 1.25  # Relaxation factor\n",
    "tol = 1e-6    # Tolerance\n",
    "max_iterations = 100\n",
    "\n",
    "# Iteration\n",
    "for iteration in range(max_iterations):\n",
    "    x_new = np.copy(x)\n",
    "    for i in range(4):\n",
    "        sigma = 0\n",
    "        for j in range(4):\n",
    "            if j != i:\n",
    "                sigma += A[i, j] * x_new[j] if j < i else A[i, j] * x[j]\n",
    "        x_new[i] = (1 - omega) * x[i] + (omega / A[i, i]) * (b[i] - sigma)\n",
    "    \n",
    "    # Check convergence\n",
    "    if np.linalg.norm(x_new - x, np.inf) < tol:\n",
    "        break\n",
    "    x = x_new\n",
    "\n",
    "print(f\"Solution after {iteration+1} iterations:\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. **Chebyshev Semi-Iterative Method**\n",
    "\n",
    "\n",
    "The Chebyshev semi-iterative method is an iterative method used to solve linear systems of equations of the form $Ax = b$, where $A$ is a symmetric positive definite matrix. It is a variant of the Richardson method that utilizes Chebyshev polynomials to accelerate convergence. It is particularly effective when the eigenvalues of $A$ are clustered, leading to faster convergence compared to simple iterative methods.\n",
    "\n",
    "### Formula\n",
    "\n",
    "The Chebyshev semi-iterative method generates a sequence of approximations $x_k$ to the solution $x$ using the following recursive formula:\n",
    "\n",
    "$$x_{k+1} = \\omega_{k+1} \\left( \\alpha_k (b - Ax_k) + x_k - x_{k-1} \\right) + x_{k-1}$$\n",
    "\n",
    "where $\\omega_{k+1}$ and $\\alpha_k$ are parameters derived from the Chebyshev polynomials and the spectral radius of the iteration matrix.\n",
    "\n",
    "More precisely, the iteration can be expressed as:\n",
    "\n",
    "$$x_{k+1} = \\rho_k \\left( \\frac{r_k}{\\sigma} \\right) + (1-\\rho_k)x_{k-1}$$\n",
    "\n",
    "where $r_k = b - Ax_k$ is the residual, $\\sigma = \\frac{\\lambda_{max} + \\lambda_{min}}{2}$, and $\\rho_k$ is computed using Chebyshev polynomials:\n",
    "\n",
    "$$\\rho_1 = 1, \\quad \\rho_2 = \\frac{2\\sigma^2}{\\lambda_{max}^2+\\lambda_{min}^2}, \\quad \\rho_{k+1} = \\frac{1}{1 - \\frac{\\rho_k(\\lambda_{max} - \\lambda_{min})^2}{4\\sigma^2}}$$\n",
    "\n",
    "where $\\lambda_{min}$ and $\\lambda_{max}$ are the smallest and largest eigenvalues of $A$, respectively.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1.  **Initialization:**\n",
    "    * Choose an initial guess $x_0$.\n",
    "    * Choose $x_1$ (often $x_1 = x_0 + \\alpha_0 (b-Ax_0)$).\n",
    "    * Compute $\\lambda_{min}$ and $\\lambda_{max}$ (or estimates).\n",
    "    * Compute $\\sigma = \\frac{\\lambda_{max} + \\lambda_{min}}{2}$.\n",
    "    * Compute $\\rho_1 = 1$ and $\\rho_2 = \\frac{2\\sigma^2}{\\lambda_{max}^2+\\lambda_{min}^2}$.\n",
    "\n",
    "2.  **Iteration:**\n",
    "    * For $k = 1, 2, \\dots$ until convergence:\n",
    "        * Compute the residual $r_k = b - Ax_k$.\n",
    "        * Compute $x_{k+1} = \\rho_k \\left( \\frac{r_k}{\\sigma} \\right) + (1-\\rho_k)x_{k-1}$.\n",
    "        * Compute $\\rho_{k+1} = \\frac{1}{1 - \\frac{\\rho_k(\\lambda_{max} - \\lambda_{min})^2}{4\\sigma^2}}$.\n",
    "        * Check for convergence (e.g., $\\|r_k\\| < \\text{tolerance}$).\n",
    "\n",
    "3.  **Output:**\n",
    "    * Return the approximate solution $x_{k+1}$.\n",
    "\n",
    "### Convergence\n",
    "\n",
    "The Chebyshev semi-iterative method converges if $A$ is a symmetric positive definite matrix. The convergence rate depends on the condition number of $A$, defined as $\\kappa(A) = \\frac{\\lambda_{max}}{\\lambda_{min}}$.\n",
    "\n",
    "The convergence rate is roughly proportional to $\\sqrt{\\kappa(A)}$, which is better than the linear convergence rate of simple iterative methods like the Jacobi or Gauss-Seidel methods.\n",
    "\n",
    "The convergence is guaranteed if the eigenvalues of $A$ are real and positive. The method's effectiveness is enhanced when the eigenvalues are clustered, leading to a smaller condition number and faster convergence.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**<br>\n",
    "\n",
    "* **Faster Convergence:** Compared to basic iterative methods, it often converges much faster, especially for well-conditioned systems.\n",
    "* **Effective for Clustered Eigenvalues:** Performs well when the eigenvalues of $A$ are clustered.\n",
    "* **Predictable Convergence:** The convergence rate can be estimated based on the eigenvalues of $A$.\n",
    "\n",
    "**Disadvantages:**<br>\n",
    "\n",
    "* **Requires Eigenvalue Estimates:** Requires estimates of the largest and smallest eigenvalues of $A$, which can be computationally expensive to obtain.\n",
    "* **Sensitivity to Eigenvalue Estimates:** The convergence rate is sensitive to the accuracy of the eigenvalue estimates. Poor estimates can lead to slow or even divergent behavior.\n",
    "* **Not Applicable to General Matrices:** It is primarily designed for symmetric positive definite matrices; it may not converge for general matrices.\n",
    "* **More Complex Implementation:** The recursive formula is more complex than simple iterative methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "using the same given System:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "10x_1 - 2x_2 - x_3 - x_4 &= 3 \\quad (1) \\\\\n",
    "-2x_1 + 10x_2 - x_3 - x_4 &= 15 \\quad (2) \\\\\n",
    "-x_1 - x_2 + 10x_3 - 2x_4 &= 27 \\quad (3) \\\\\n",
    "-x_1 - x_2 - 2x_3 + 10x_4 &= -9 \\quad (4)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Step 1: **Rewrite the system in matrix form:**<br>\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "10 & -2 & -1 & -1 \\\\\n",
    "-2 & 10 & -1 & -1 \\\\\n",
    "-1 & -1 & 10 & -2 \\\\\n",
    "-1 & -1 & -2 & 10\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\mathbf{b} = \\begin{pmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Step 2: **Initialize:**<br>\n",
    "\n",
    "Letâ€™s take:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Step 3: **Chebyshev Semi-Iterative Formula:** <br>\n",
    "\n",
    "For the first iteration, it simplifies to the **Jacobi method**:\n",
    "\n",
    "$$\n",
    "x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "**Iterations**<br>\n",
    "\n",
    "**Iteration 1**\n",
    "\n",
    "$$\n",
    "x_1^{(1)} = \\frac{1}{10} \\left( 3 - (-2)(0) - (-1)(0) - (-1)(0) \\right) = \\frac{3}{10} = 0.3\n",
    "$$\n",
    "$$\n",
    "x_2^{(1)} = \\frac{1}{10} \\left( 15 - (-2)(0) - (-1)(0) - (-1)(0) \\right) = \\frac{15}{10} = 1.5\n",
    "$$\n",
    "$$\n",
    "x_3^{(1)} = \\frac{1}{10} \\left( 27 - (-1)(0) - (-1)(0) - (-2)(0) \\right) = \\frac{27}{10} = 2.7\n",
    "$$\n",
    "$$\n",
    "x_4^{(1)} = \\frac{1}{10} \\left( -9 - (-1)(0) - (-1)(0) - (-2)(0) \\right) = \\frac{-9}{10} = -0.9\n",
    "$$\n",
    "\n",
    "**Iteration 1 Result:**\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(1)} = 0.3 \\\\\n",
    "x_2^{(1)} = 1.5 \\\\\n",
    "x_3^{(1)} = 2.7 \\\\\n",
    "x_4^{(1)} = -0.9\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Iteration 2**\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "x_1^{(2)} &= \\frac{1}{10} \\left( 3 - (-2)(1.5) - (-1)(2.7) - (-1)(-0.9) \\right) \\\\\n",
    "&= \\frac{1}{10} \\left( 3 + 3 + 2.7 - 0.9 \\right) \\\\\n",
    "&= \\frac{1}{10} (7.8) \\\\\n",
    "&= 0.78\n",
    "\\end{align*}\n",
    "\\]\n",
    "\\[\n",
    "\\begin{align*}\n",
    "x_2^{(2)} &= \\frac{1}{10} \\left( 15 - (-2)(0.3) - (-1)(2.7) - (-1)(-0.9) \\right) \\\\\n",
    "&= \\frac{1}{10} \\left( 15 + 0.6 + 2.7 - 0.9 \\right) \\\\\n",
    "&= \\frac{1}{10} (17.4) \\\\\n",
    "&= 1.74\n",
    "\\end{align*}\n",
    "\\]\n",
    "\\[\n",
    "\\begin{align*}\n",
    "x_3^{(2)} &= \\frac{1}{10} \\left( 27 - (-1)(0.3) - (-1)(1.5) - (-2)(-0.9) \\right) \\\\\n",
    "&= \\frac{1}{10} \\left( 27 + 0.3 + 1.5 - 1.8 \\right) \\\\\n",
    "&= \\frac{1}{10} (27.0) \\\\\n",
    "&= 2.7\n",
    "\\end{align*}\n",
    "\\]\n",
    "\\[\n",
    "\\begin{align*}\n",
    "x_4^{(2)} &= \\frac{1}{10} \\left( -9 - (-1)(0.3) - (-1)(1.5) - (-2)(2.7) \\right) \\\\\n",
    "&= \\frac{1}{10} \\left( -9 + 0.3 + 1.5 + 5.4 \\right) \\\\\n",
    "&= \\frac{1}{10} (-1.8) \\\\\n",
    "&= -0.18\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "**Iteration 2 Result:**\n",
    "\n",
    "\\[\n",
    "\\begin{cases}\n",
    "x_1^{(2)} = 0.78 \\\\\n",
    "x_2^{(2)} = 1.74 \\\\\n",
    "x_3^{(2)} = 2.7 \\\\\n",
    "x_4^{(2)} = -0.18\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "\n",
    "## **Iteration 3**\n",
    "\n",
    "Now, use the results from Iteration 2:\n",
    "\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "x_1^{(3)} &= \\frac{1}{10} \\left( 3 - (-2)(1.74) - (-1)(2.7) - (-1)(-0.18) \\right) \\\\\n",
    "&= \\frac{1}{10} \\left( 3 + 3.48 + 2.7 - 0.18 \\right) \\\\\n",
    "&= \\frac{1}{10} (9.0) \\\\\n",
    "&= 0.9\n",
    "\\end{align*}\n",
    "\\]\n",
    "\\[\n",
    "\\begin{align*}\n",
    "x_2^{(3)} &= \\frac{1}{10} \\left( 15 - (-2)(0.78) - (-1)(2.7) - (-1)(-0.18) \\right) \\\\\n",
    "&= \\frac{1}{10} \\left( 15 + 1.56 + 2.7 - 0.18 \\right) \\\\\n",
    "&= \\frac{1}{10} (19.08) \\\\\n",
    "&= 1.908\n",
    "\\end{align*}\n",
    "\\]\n",
    "\\[\n",
    "\\begin{align*}\n",
    "x_3^{(3)} &= \\frac{1}{10} \\left( 27 - (-1)(0.78) - (-1)(1.74) - (-2)(-0.18) \\right) \\\\\n",
    "&= \\frac{1}{10} \\left( 27 + 0.78 + 1.74 - 0.36 \\right) \\\\\n",
    "&= \\frac{1}{10} (29.16) \\\\\n",
    "&= 2.916\n",
    "\\end{align*}\n",
    "\\]\n",
    "\\[\n",
    "\\begin{align*}\n",
    "x_4^{(3)} &= \\frac{1}{10} \\left( -9 - (-1)(0.78) - (-1)(1.74) - (-2)(2.7) \\right) \\\\\n",
    "&= \\frac{1}{10} \\left( -9 + 0.78 + 1.74 + 5.4 \\right) \\\\\n",
    "&= \\frac{1}{10} (-1.08) \\\\\n",
    "&= -0.108\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "**Iteration 3 Result:**\n",
    "\n",
    "\\[\n",
    "\\begin{cases}\n",
    "x_1^{(3)} = 0.9 \\\\\n",
    "x_2^{(3)} = 1.908 \\\\\n",
    "x_3^{(3)} = 2.916 \\\\\n",
    "x_4^{(3)} = -0.108\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "## **Summary of All Iterations:**\n",
    "\n",
    "| Iteration | \\(x_1\\) | \\(x_2\\) | \\(x_3\\) | \\(x_4\\) |\n",
    "|---------|---------|---------|---------|---------|\n",
    "| 1       | 0.3     | 1.5     | 2.7     | -0.9    |\n",
    "| 2       | 0.78    | 1.74    | 2.7     | -0.18   |\n",
    "| 3       | 0.9     | 1.908   | 2.916   | -0.108  |\n",
    "\n",
    "Here is the python code, which I translated from MATLAB code shown in [wikipedia](https://en.wikipedia.org/wiki/Chebyshev_iteration), with revision where it only shows the converged solution and maximum iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [ 9.99999985e-01  1.99999999e+00  3.00000000e+00 -2.27554890e-08], Iterations: 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def SolChebyshev(A, b, x0, iter_num, l_max, l_min, tol=1e-6):\n",
    "    d = (l_max + l_min) / 2\n",
    "    c = (l_max - l_min) / 2\n",
    "    pre_cond = np.eye(A.shape[0])\n",
    "    x = x0.copy()\n",
    "    r = b - A @ x\n",
    "\n",
    "    for i in range(1, iter_num + 1):\n",
    "        z = np.linalg.solve(pre_cond, r)\n",
    "\n",
    "        if i == 1:\n",
    "            p = z\n",
    "            alpha = 1 / d\n",
    "        elif i == 2:\n",
    "            beta = 0.5 * (c * alpha) ** 2\n",
    "            alpha = 1 / (d - beta / alpha)\n",
    "            p = z + beta * p\n",
    "        else:\n",
    "            beta = (c * alpha / 2) ** 2\n",
    "            alpha = 1 / (d - beta / alpha)\n",
    "            p = z + beta * p\n",
    "\n",
    "        x = x + alpha * p\n",
    "        r = b - A @ x\n",
    "\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            break\n",
    "\n",
    "    return x, i  # You can print externally\n",
    "\n",
    "\n",
    "A = np.array([\n",
    "    [10, -2, -1, -1],\n",
    "    [-2, 10, -1, -1],\n",
    "    [-1, -1, 10, -2],\n",
    "    [-1, -1, -2, 10]\n",
    "], dtype=float)\n",
    "\n",
    "b = np.array([3, 15, 27, -9], dtype=float)\n",
    "x0 = np.zeros_like(b)\n",
    "\n",
    "x_sol, num_iters = SolChebyshev(A, b, x0, iter_num=100, l_max=12, l_min=6) # change l_max and l_min for different eigenvalues\n",
    "print(f\"Solution: {x_sol}, Iterations: {num_iters}\")\n",
    "\n",
    "eg = np.linalg.eigvals(A) \n",
    "#print(f\"Eigenvalues: {eg}\") # calculate the actual eigenvalues of the matrix for l_max and l_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. **Richardson Iteration**\n",
    "\n",
    "The Richardson iteration is a basic iterative method used to solve systems of linear equations of the form $Ax = b$, where $A$ is a known matrix, $b$ is a known vector, and $x$ is the unknown vector we aim to find. It refines an initial guess of the solution with each iteration by using the residual.\n",
    "\n",
    "### **Formula:**\n",
    "\n",
    "* The core formula for the Richardson iteration is:\n",
    "    $$x^{(k+1)} = x^{(k)} + \\alpha (b - Ax^{(k)})$$\n",
    "    * Where:\n",
    "        * $x^{(k)}$ is the approximation of the solution at the k-th iteration.\n",
    "        * $\\alpha$ is a scalar parameter (the step size or relaxation parameter) that influences convergence.\n",
    "        * $b - Ax^{(k)}$ is the residual vector, representing the error at the k-th iteration.\n",
    "\n",
    "### **Algorithm:**\n",
    "\n",
    "1.  **Initialization:**\n",
    "    * Choose an initial guess for the solution, $x^{(0)}$.\n",
    "    * Choose a suitable value for the parameter $\\alpha$.\n",
    "    * set k = 0.\n",
    "2.  **Iteration:**\n",
    "    * Calculate the residual: $r^{(k)} = b - Ax^{(k)}$.\n",
    "    * Update the solution: $x^{(k+1)} = x^{(k)} + \\alpha r^{(k)}$.\n",
    "    * increment k, k = k+1.\n",
    "3.  **Convergence Check:**\n",
    "    * Check if the residual $r^{(k)}$ or the difference between successive solutions ($x^{(k+1)} - x^{(k)}$) is below a specified tolerance.\n",
    "    * If the convergence criterion is met, stop. Otherwise, return to step 2.\n",
    "\n",
    "### **Convergence:**\n",
    "\n",
    "* The convergence of the Richardson iteration is highly dependent on the choice of $\\alpha$ and the properties of the matrix A.\n",
    "* For convergence, the eigenvalues of the matrix $(I - \\alpha A)$ must lie within the unit circle.\n",
    "* Finding the optimal $\\alpha$ can be challenging and often requires knowledge of the spectral properties of A.\n",
    "* If the value of alpha is poorly chosen, the method can diverge, or converge very slowly.\n",
    "* The matrix A must be such that the iteration will converge.\n",
    "\n",
    "### **Advantages and Disadvantages:**\n",
    "\n",
    "* **Advantages:**\n",
    "    * Simplicity: The algorithm is relatively easy to understand and implement.\n",
    "    * Basic foundation: It provides a fundamental understanding of iterative methods for solving linear systems.\n",
    "* **Disadvantages:**\n",
    "    * Convergence sensitivity: The convergence is highly sensitive to the choice of $\\alpha$, making it difficult to use in many practical situations.\n",
    "    * Slow convergence: It can converge very slowly, especially for ill-conditioned matrices.\n",
    "    * Limited applicability: It is not as efficient as other iterative methods, such as conjugate gradient or GMRES, for general linear systems.\n",
    "    * Finding the optimal alpha value can be very difficult.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "10x_1 - 2x_2 - x_3 - x_4 &= 3, \\\\\n",
    "-2x_1 + 10x_2 - x_3 - x_4 &= 15, \\\\\n",
    "- x_1 - x_2 + 10x_3 - 2x_4 &= 27, \\\\\n",
    "- x_1 - x_2 - 2x_3 + 10x_4 &= -9.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To solve this same system using the Richardson iteration method, we first express it in matrix form:\n",
    "\n",
    "$$\n",
    "A \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 10 & -2 & -1 & -1 \\\\ -2 & 10 & -1 & -1 \\\\ -1 & -1 & 10 & -2 \\\\ -1 & -1 & -2 & 10 \\end{bmatrix}, \\quad\n",
    "\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Formula:**<br>\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\alpha (\\mathbf{b} - A \\mathbf{x}^{(k)})\n",
    "$$\n",
    "\n",
    "where \\($ \\alpha $\\) is a relaxation parameter.\n",
    "\n",
    "**Choosing \\($ \\alpha $\\)**<br>\n",
    "A good choice for \\($ \\alpha $\\) is:\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{2}{\\lambda_{\\max} + \\lambda_{\\min}}\n",
    "$$\n",
    "\n",
    "where \\($ \\lambda_{\\max} $\\) and \\($ \\lambda_{\\min} $\\) are the largest and smallest eigenvalues of \\($ A $\\). \n",
    "\\\n",
    "I estimated \\($ \\lambda_{\\max} \\approx 12 $\\), \\($ \\lambda_{\\min} \\approx 8 $\\), leading to:\n",
    "    $$\n",
    "    \\alpha = \\frac{2}{12 + 6} = 0.111\n",
    "    $$\n",
    "\n",
    "Though I will $ \\alpha = 0.1$ for ease of typing the solution, but if you want an exact solution use $ \\alpha = 0.111$\n",
    "\n",
    "Here are the first three iterations of the Richardson method:\n",
    "\n",
    "1. **Initial guess**:  \n",
    "   $$\n",
    "   x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   \n",
    "\n",
    "2. **First iteration**:  \n",
    "\n",
    "    1. Compute \\($ r^{(0)} = b - A x^{(0)} $\\):\n",
    "   $$\n",
    "   r^{(0)} = \\begin{bmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{bmatrix}\n",
    "   $$\n",
    "    2. Update \\( x^{(1)} \\):\n",
    "   $$\n",
    "   x^{(1)} = x^{(0)} + 0.1 \\cdot r^{(0)}\n",
    "   $$\n",
    "   $$\n",
    "   x^{(1)} = (0,0,0,0) + 0.1 \\times \\begin{bmatrix} 3 \\\\ 15 \\\\ 27 \\\\ -9 \\end{bmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   x^{(1)} = (0.3, 1.5, 2.7, -0.9).\n",
    "   $$\n",
    "\n",
    "**Iteration 1 Result:**\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(1)} = 0.3 \\\\\n",
    "x_2^{(1)} = 1.5 \\\\\n",
    "x_3^{(1)} = 2.7 \\\\\n",
    "x_4^{(1)} = -0.9\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. **Second iteration**:  \n",
    "   1. Compute \\($ r^{(1)} = b - A x^{(1)} $\\):\n",
    "   $$\n",
    "   r^{(1)} = \\begin{bmatrix} 4.8 \\\\ 2.4 \\\\ 0 \\\\ 7.2 \\end{bmatrix}\n",
    "   $$\n",
    "    2. Update \\( x^{(2)} \\):\n",
    "   $$\n",
    "   x^{(2)} = x^{(1)} + 0.1 \\cdot r^{(1)}\n",
    "   $$\n",
    "   $$\n",
    "   x^{(2)} = (0.3, 1.5, 2.7, -0.9) + 0.1 \\times \\begin{bmatrix} 4.8 \\\\ 2.4 \\\\ 0 \\\\ 7.2 \\end{bmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   x^{(2)} = (0.78, 1.74, 2.7, -0.18).\n",
    "   $$\n",
    "   **Iteration 2 Result:**\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(2)} = 0.78 \\\\\n",
    "x_2^{(2)} = 1.74 \\\\\n",
    "x_3^{(2)} = 2.7 \\\\\n",
    "x_4^{(2)} = -0.18\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "4. **Third iteration**:  \n",
    "   \n",
    "1. Compute \\($ r^{(2)} = b - A x^{(2)} $\\):\n",
    "   $$\n",
    "   r^{(2)} = \\begin{bmatrix} 1.2 \\\\ 1.68 \\\\ 2.16 \\\\ 0.72 \\end{bmatrix}\n",
    "   $$\n",
    "2. Update \\($ x^{(3)} $\\):\n",
    "   $$\n",
    "   x^{(3)} = x^{(2)} + 0.1 \\cdot r^{(2)}\n",
    "   $$\n",
    "   $$\n",
    "   x^{(3)} = (0.78, 1.74, 2.7, -0.18) + 0.1 \\times \\begin{bmatrix} 1.2 \\\\ 1.68 \\\\ 2.16 \\\\ 0.72 \\end{bmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   x^{(3)} = (0.9, 1.908, 2.916, -0.108).\n",
    "   $$\n",
    "\n",
    "**Iteration 3 Result:**\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_1^{(3)} = 0.9 \\\\\n",
    "x_2^{(3)} = 1.908 \\\\\n",
    "x_3^{(3)} = 2.916 \\\\\n",
    "x_4^{(3)} = -0.108\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_max = 12.0\n",
      "lambda_min = 6.0\n",
      "alpha=0.11\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[10, -2, -1, -1],\n",
    "              [-2, 10, -1, -1],\n",
    "              [-1, -1, 10, -2],\n",
    "              [-1, -1, -2, 10]], dtype=float)\n",
    "\n",
    "# Compute the eigenvalues of matrix A\n",
    "eigenvalues = np.linalg.eigvals(A)\n",
    "\n",
    "# Extract Î»max and Î»min\n",
    "lambda_max = np.max(eigenvalues).round(2)\n",
    "lambda_min = np.min(eigenvalues).round(2)\n",
    "\n",
    "print(f\"lambda_max = {lambda_max}\")\n",
    "print(f\"lambda_min = {lambda_min}\")\n",
    "\n",
    "alpha = (2/(lambda_max + lambda_min)).round(2)\n",
    "print(f\"alpha={alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [ 9.99999859e-01  1.99999993e+00  2.99999999e+00 -2.06155935e-07]\n",
      "Iterations: 15\n"
     ]
    }
   ],
   "source": [
    "def richardson_iter(A, b, alpha, x0, max_iter=1000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Solves the linear system Ax = b using the Richardson iteration method.\n",
    "\n",
    "    Args:\n",
    "        A: The coefficient matrix (numpy array).\n",
    "        b: The right-hand side vector (numpy array).\n",
    "        alpha: The relaxation parameter.\n",
    "        x0: The initial guess for the solution vector (numpy array).\n",
    "        max_iter: The maximum number of iterations.\n",
    "        tolerance: The tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the approximate solution vector (numpy array) and the number of iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(b)\n",
    "    x = x0.copy()\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        x_new = x + alpha * (b - np.dot(A, x))\n",
    "        if np.linalg.norm(x_new - x) < tolerance:\n",
    "            return x_new, k + 1  # Return solution and number of iterations\n",
    "        x = x_new\n",
    "\n",
    "    return x, max_iter  # Return solution and max_iter if no convergence\n",
    "\n",
    "# Example usage:\n",
    "A = np.array([\n",
    "    [10, -2, -1, -1],\n",
    "    [-2, 10, -1, -1],\n",
    "    [-1, -1, 10, -2],\n",
    "    [-1, -1, -2, 10]\n",
    "], dtype=float)\n",
    "\n",
    "b = np.array([3, 15, 27, -9], dtype=float)\n",
    "\n",
    "n = len(b)\n",
    "x0 = np.zeros(n)  # Initial guess\n",
    "alpha = 0.111 # using alpha = 0.111 reduces iterations by 2\n",
    "\n",
    "solution, iterations = richardson_iter(A, b, alpha, x0)\n",
    "\n",
    "print(\"Solution:\", solution)\n",
    "print(\"Iterations:\", iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Summary Table\n",
    "\n",
    "| **Method**                      | **Relaxation Parameter(s)**        | **Key Feature**                               |\n",
    "|---------------------------------|-----------------------------------|-----------------------------------------------|\n",
    "| Jacobi                          | None                              | Simple, independent updates                   |\n",
    "| Gauss-Seidel                    | None                              | Uses latest updates immediately               |\n",
    "| Successive Over-Relaxation (SOR)| $ \\omega $                        | Tunable parameter accelerates convergence     |\n",
    "| Symmetric SOR (SSOR)            | $ \\omega $                        | Symmetric application, preconditioning        |\n",
    "| Accelerated Over-Relaxation (AOR)| Two parameters                    | Flexible convergence control                  |\n",
    "| Chebyshev Method                | None (Polynomial-based)           | Acceleration via Chebyshev polynomials        |\n",
    "| Richardson Iteration            | $ \\alpha $                        | Simple residual scaling                       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
