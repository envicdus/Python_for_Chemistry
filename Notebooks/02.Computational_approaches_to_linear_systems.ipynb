{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "    fig-width: 9\n",
    "    fig-height: 6\n",
    "jupyter: python3\n",
    "code-fold: true\n",
    "code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve, lu, qr, cholesky, eig, svd\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse.linalg as spla\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse.linalg import cg  # Conjugate Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy and SciPy linear algebra solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy provides basic linear algebra routines through $\\texttt{numpy.linalg}$. These functions internally use **LAPACK** and **BLAS** for efficiency.\n",
    "\n",
    "\n",
    "#### 1. **Solving a system of linear equations**  \n",
    "   Given the system:\n",
    "\n",
    "   $$ Ax = b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 1.3]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[3, 2], [1, 4]])\n",
    "B = np.array([5, 6])\n",
    "x = np.linalg.solve(A, B)  # Solves Ax = B\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses an efficient algorithm (Gaussian elimination with partial pivoting) to find x. Refer to [this link](https://numpy.org/doc/2.2/reference/generated/numpy.linalg.solve.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Computing the inverse of a matrix**\n",
    "  $$ A^{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 1.3]\n"
     ]
    }
   ],
   "source": [
    "A_inv = np.linalg.inv(A) # Inverse of A\n",
    "x = np.dot(A_inv, B) # Solves Ax = B\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the matrix must non-singular $(\\det(A) \\neq 0)$  for this to work. Click the [link](https://numpy.org/doc/2.2/reference/generated/numpy.linalg.inv.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Computing the determinant**<br>\n",
    "The determinant of a square matrix A is a scalar value that provides important information about the matrix. It is denoted as:\n",
    "   $$ \\det(A) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 1.3]\n"
     ]
    }
   ],
   "source": [
    "det_A = np.linalg.det(A) # Determinant of A\n",
    "# Check if the determinant is zero\n",
    "if det_A == 0:\n",
    "    print(\"The system has no unique solution (determinant is zero).\")\n",
    "else:\n",
    "    x = np.linalg.solve(A, B) # Solves Ax = B\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nonzero determinant means A is invertible, while det(A) = 0 means A is singular (not invertible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Eigenvalues and Eigenvectors**\n",
    "\n",
    "For a square matrix \\( $A$ \\), the eigenvalues (\\( $\\lambda$ \\)) and corresponding eigenvectors (\\( $v$ \\)) satisfy the equation:\n",
    "\n",
    "$$\n",
    "A \\cdot v = \\lambda \\cdot v\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( $\\lambda$ \\) (lambda) is a **scalar** (eigenvalue),\n",
    "- \\( $v$ \\) is a **nonzero vector** (eigenvector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution X: [0.8 1.3]\n"
     ]
    }
   ],
   "source": [
    "# Eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors  = np.linalg.eig(A) # compute eigenvalues and eigenvectors\n",
    "\n",
    "# Diagonal matrix of eigenvalues\n",
    "D = np.diag(eigenvalues)\n",
    "\n",
    "# Compute P inverse\n",
    "P_inv = np.linalg.inv(eigenvectors)\n",
    "\n",
    "# Transform B into the eigenbasis\n",
    "B_prime = P_inv @ B\n",
    "\n",
    "# Solve for Y in DY = B'\n",
    "Y = np.linalg.solve(D, B_prime)\n",
    "\n",
    "# Compute the final solution X = P Y\n",
    "X = eigenvectors @ Y\n",
    "\n",
    "# Print the result\n",
    "print(\"Solution X:\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELI5: Eigenvectors \"point\" in the same direction as most of your data, and eigenvalues tell you how strongly your data points that direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Singular Value Decomposition (SVD)**\n",
    "\n",
    "Singular Value Decomposition (SVD) is a method to break down a complex matrix into simpler components. It is similar to taking a blurry image and separating it into clear building blocks.\n",
    "\n",
    "For any matrix \\( $A$ \\), SVD expresses it as:\n",
    "\n",
    "$$ A = U \\Sigma V^T $$\n",
    "\n",
    "Where:\n",
    "- **\\( $U$ \\)** (Left singular vectors) → Contains important patterns from the original data.\n",
    "- **\\( $\\Sigma$ \\)** (Singular values) → A diagonal matrix with values indicating the importance of each pattern.\n",
    "- **\\( $V^{T}$ \\)** (Right singular vectors) → Contains another set of patterns that, when combined with \\( $\\Sigma$ \\), recreate the original data.\n",
    "\n",
    "Analogy:<br>\n",
    "Imagine you have a large playlist of songs. SVD breaks it down into:\n",
    "- **\\( $U$\\)** → The general themes of music (e.g., rock, jazz, pop).\n",
    "- **\\( $\\Sigma$ \\)** → How strongly each theme appears in the playlist.\n",
    "- **\\( $V^{T}$ \\)** → The details of each song, arranged by themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution X: [0.8 1.3]\n"
     ]
    }
   ],
   "source": [
    "# Compute Singular Value Decomposition (SVD)\n",
    "U, S, V = np.linalg.svd(A)\n",
    "\n",
    "# Convert S into a diagonal matrix\n",
    "S_diag = np.diag(S)\n",
    "\n",
    "# Compute the pseudo-inverse of S\n",
    "S_inv = np.linalg.inv(S_diag)\n",
    "\n",
    "# Compute the pseudo-inverse of A using SVD\n",
    "A_pinv = V.T @ S_inv @ U.T # @ means matrix multiplication and .T means transpose\n",
    "\n",
    "# Solve for X using the pseudo-inverse\n",
    "X = A_pinv @ B\n",
    "\n",
    "# Print the result\n",
    "print(\"Solution X:\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciPy\n",
    "\n",
    "SciPy extends NumPy's linear algebra capabilities with additional **matrix decompositions, advanced solvers, and performance optimizations**.\n",
    "Key Advantages of SciPy over NumPy:<br>\n",
    "- Uses **LAPACK** and **ATLAS**, but with extra options for optimization.\n",
    "- Supports sparse matrices via \\texttt{scipy.sparse.linalg}.\n",
    "- Includes additional decomposition methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Solving a linear system ($\\texttt{scipy.linalg.solve}$)**  \n",
    "   Same as $\\texttt{numpy.linalg.solve}$ but often faster and more stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 1.3]\n"
     ]
    }
   ],
   "source": [
    "x = solve(A, B)  # More efficient than np.linalg.solve for large matrices\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **LU Decomposition**  \n",
    "\n",
    "   $$ PA = LU $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 1.3])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, L, U = lu(A)  # P: Permutation, L: Lower triangular, U: Upper triangular\n",
    "\n",
    "# Compute Pb\n",
    "Pb = np.dot(P, B)\n",
    "\n",
    "# Solve Ly = Pb using forward substitution\n",
    "y = np.linalg.solve(L, Pb)\n",
    "\n",
    "# Solve Ux = y using backward substitution\n",
    "x = np.linalg.solve(U, y)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **QR Decomposition**  \n",
    "\n",
    "   $$ A = QR $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 1.3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q, R = qr(A)\n",
    "# Compute Q^T B\n",
    "QTB = np.dot(Q.T, B)\n",
    "\n",
    "# Solve Rx = Q^T B using backward substitution\n",
    "x = np.linalg.solve(R, QTB)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Cholesky Decomposition (for positive definite matrices)**  \n",
    "\n",
    "Cholesky Decomposition is a matrix factorization technique used for symmetric positive definite matrices. Given a symmetric positive definite matrix \\( A \\), it can be decomposed as:\n",
    "\n",
    "$$\n",
    "A = LL^T\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( $L$ \\) is a **lower triangular matrix** with positive diagonal entries.\n",
    "- \\( $L^{T}$ \\) is the **transpose** of \\( L \\), making the decomposition efficient and numerically stable.<br>\n",
    "\\\n",
    "**Why Use Cholesky Decomposition?**<br>\n",
    "1. **Efficient Computation**: It is about **twice as fast** as LU decomposition for solving linear systems.\n",
    "2. **Numerical Stability**: Since it is applied only to positive definite matrices, it avoids issues related to pivoting.\n",
    "3. **Applications**:\n",
    "   - Solving linear systems \\( $Ax = b$ \\) efficiently.\n",
    "   - Optimization problems (e.g., least squares, Kalman filters).\n",
    "   - Monte Carlo simulations (e.g., generating correlated random variables). Helpful in Quantum Mechanical simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm for Cholesky Decomposition**\n",
    "Given a symmetric positive definite matrix \\( $A$ \\), the elements of \\( $L$ \\) are computed as:\n",
    "\n",
    "1. **Diagonal elements**:\n",
    "\n",
    "$$\n",
    "L_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2}\n",
    "$$\n",
    "\n",
    "2. **Off-diagonal elements**:\n",
    "\n",
    "$$\n",
    "L_{ij} = \\frac{1}{L_{ii}} \\left( A_{ij} - \\sum_{k=1}^{i-1} L_{ik} L_{jk} \\right), \\quad j > i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve for \\( x \\) in the equation:\n",
    "\n",
    "$$ Ax = B $$\n",
    "\n",
    "using Cholesky decomposition, follow these steps:\n",
    "\n",
    "**Steps for Solving \\( $Ax = B$ \\) Using Cholesky Decomposition**\n",
    "\n",
    "1. **Check if \\( $A$ \\) is symmetric positive definite**\n",
    "   - \\( $A$ \\) must be symmetric (\\( $A = A^{T}$ \\)).\n",
    "   - All its eigenvalues must be positive.\n",
    "\n",
    "2. **Perform Cholesky Decomposition**\n",
    "   - Decompose \\( $A$ \\) as \\( $A = LL^{T}$ \\), where \\( $L$ \\) is a lower triangular matrix.\n",
    "\n",
    "3. **Solve for \\( $y$ \\) in \\( $Ly = B$ \\)** (using forward substitution).\n",
    "\n",
    "4. **Solve for \\( $x$ \\) in \\( $L^{T} x = y$ \\)** (using backward substitution).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.27272727, 1.18181818])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = cholesky(A, lower=True)\n",
    "# Solve Ly = B using forward substitution\n",
    "y = np.linalg.solve(L, B)\n",
    "\n",
    "# Solve L^T x = y using backward substitution\n",
    "x = np.linalg.solve(L.T, y)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is different, issue coming from the fact that Cholesky decomposition only works for symmetric positive definite (SPD) matrices and the example above is **NOT** symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**\n",
    "Consider the matrix:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "4 & 12 & -16 \\\\\n",
    "12 & 37 & -43 \\\\\n",
    "-16 & -43 & 98\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying Cholesky decomposition, we get:\n",
    "\n",
    "$$\n",
    "L =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 \\\\\n",
    "6 & 1 & 0 \\\\\n",
    "-8 & 5 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where \\( $L^{T}$ \\) is its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  0.],\n",
       "       [ 6.,  1.,  0.],\n",
       "       [-8.,  5.,  3.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.array([\n",
    "    [4, 12, -16],\n",
    "    [12, 37, -43],\n",
    "    [-16, -43, 98]\n",
    "])\n",
    "L = cholesky(C, lower=True)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if you want to solve for x, assuming that B\n",
    "$$\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "2  \\\\\n",
    "6 \\\\\n",
    "-8\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x: [0.5 0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "# Define vector B\n",
    "D = np.array([2, 6, -8]) # B is being already above\n",
    "\n",
    "# Perform Cholesky decomposition: A = L * L^T\n",
    "L = cholesky(C, lower=True)\n",
    "\n",
    "# Solve for y in L * y = B using forward substitution\n",
    "y = np.linalg.solve(L, D)\n",
    "\n",
    "# Solve for x in L^T * x = y using backward substitution\n",
    "x = np.linalg.solve(L.T, y)\n",
    "\n",
    "print(\"Solution x:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Eigenvalue Problems ($\\texttt{scipy.linalg.eig}$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELI5, think of a spring:\n",
    "- if you pull it straight, it stretches along its length—that’s like an eigenvector (a special direction).\n",
    "- How much it stretches (double, triple, or shrinks) is the eigenvalue (a number describing the effect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 1.3])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals, eigvecs = eig(A)\n",
    "\n",
    "# Compute V^(-1) * B\n",
    "V_inv_B = np.linalg.solve(eigvecs, B)\n",
    "\n",
    "# Solve D * y = V_inv_B (since D is diagonal, just divide)\n",
    "y = V_inv_B / eigvals\n",
    "\n",
    "# Compute final solution x = V * y\n",
    "x = np.dot(eigvecs, y).real\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, array C from the example above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5, -0. ,  0. ])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals_c, eigvecs_c = eig(C)\n",
    "\n",
    "# Compute V^(-1) * B\n",
    "V_inv_D = np.linalg.solve(eigvecs_c, D)\n",
    "\n",
    "# Solve D * y = V_inv_B (since D is diagonal, just divide)\n",
    "y = V_inv_D / eigvals_c\n",
    "\n",
    "# Compute final solution x = V * y  \n",
    "x = np.dot(eigvecs_c, y).real.round(2) # real to remove imaginary part and round to 2 decimal places\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **Singular Value Decomposition (SVD)**  \n",
    "Singular Value Decomposition (SVD) is a way to break down a big, complicated matrix into smaller, simpler parts. For any matrix \\( $A$ \\), SVD expresses it as:\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "Where:\n",
    "- **\\( $U$ \\)** (Left singular vectors) → Contains important patterns from the original data.\n",
    "- **\\( $\\Sigma$ \\)** (Singular values) → A diagonal matrix with values indicating the importance of each pattern.\n",
    "- **\\( $V^{T}$ \\)** (Right singular vectors) → Contains another set of patterns that, when combined with \\( \\Sigma \\), recreate the original data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogy:<br>\n",
    "Imagine you have a large playlist of songs. SVD breaks it down into:\n",
    "- **\\( $U$ \\)** → The general themes of music (e.g., rock, jazz, pop).\n",
    "- **\\( $\\Sigma$ \\)** → How strongly each theme appears in the playlist.\n",
    "- **\\( $V^{T}$ \\)** → The details of each song, arranged by themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x: [0.8 1.3]\n"
     ]
    }
   ],
   "source": [
    "U, S, V = svd(A)\n",
    "# Compute the pseudo-inverse of Sigma\n",
    "S_inv = np.diag(1 / S)\n",
    "\n",
    "# Solve for x using the SVD components\n",
    "x = V.T @ S_inv @ U.T @ B\n",
    "\n",
    "print(\"Solution x:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x: [ 0.5  0.  -0. ]\n"
     ]
    }
   ],
   "source": [
    "U1, S1, V1 = svd(C)\n",
    "# Compute the pseudo-inverse of Sigma\n",
    "S_inv1 = np.diag(1 / S1)\n",
    "\n",
    "# Solve for x using the SVD components\n",
    "x1 = V1.T @ S_inv1 @ U1.T @ D\n",
    "\n",
    "print(\"Solution x:\", x1.real.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Linear Algebra ($\\texttt{scipy.sparse.linalg}$)\n",
    "For large, sparse matrices, SciPy provides $\\texttt{scipy.sparse.linalg}$, which is optimized for efficiency. if you don't know, A sparse matrix is a matrix in which most of the elements are zero. It is the opposite of a dense matrix, where most elements are nonzero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Solving a sparse linear system**  \n",
    "\n",
    "   $$ Ax = b $$\n",
    "where $A$ is a sparse matrix.<br>\n",
    "for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x: [0.16987741 0.32049037 0.54816112 0.48686515 1.50437828]\n"
     ]
    }
   ],
   "source": [
    "# sparse matrix A (5x5)\n",
    "E = csr_matrix([\n",
    "    [4, 1, 0, 0, 0],\n",
    "    [1, 4, 1, 0, 0],\n",
    "    [0, 1, 4, 1, 0],\n",
    "    [0, 0, 1, 4, 1],\n",
    "    [0, 0, 0, 1, 3]\n",
    "])\n",
    "\n",
    "# right-hand side vector b\n",
    "F = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Solve Ax = b\n",
    "x = spla.spsolve(E, F)\n",
    "\n",
    "# Print the solution\n",
    "print(\"Solution x:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sparse = csc_matrix(A)  # Convert A to sparse format\n",
    "x = spsolve(A_sparse, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Iterative solvers (Conjugate Gradient, GMRES)**  \n",
    "Iterative solvers like **Conjugate Gradient (CG)** and **Generalized Minimal Residual (GMRES)** are widely used for solving large, sparse linear systems of the form:\n",
    "\n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "\n",
    "where \\( $A$ \\) is a matrix, \\( $x$ \\) is the unknown vector, and \\( $b$ \\) is a given right-hand-side vector. These solvers are particularly useful when direct methods (like LU decomposition) are too expensive in terms of memory and computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison: CG vs. GMRES**\n",
    "\n",
    "\n",
    "| Feature               | Conjugate Gradient (CG)            | GMRES                                      |\n",
    "|-----------------------|-----------------------------------|--------------------------------------------|\n",
    "| **Matrix Type**       | Symmetric Positive Definite      | General (Non-Symmetric, Non-SPD)          |\n",
    "| **Storage**          | Low (only need a few vectors)    | High (grows with iterations)              |\n",
    "| **Convergence**      | Faster for well-conditioned SPD matrices | Can stagnate without restarts    |\n",
    "| **Computational Cost** | Cheaper per iteration           | More expensive due to orthogonalization   |\n",
    "| **Restarting Needed?** | No                              | Yes (for large problems, GMRES(m))        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x: [0.16987741 0.32049037 0.54816112 0.48686515 1.50437828]\n",
      "CG Info: 0\n"
     ]
    }
   ],
   "source": [
    "# sparse matrix A (5x5)\n",
    "E = csr_matrix([\n",
    "    [4, 1, 0, 0, 0],\n",
    "    [1, 4, 1, 0, 0],\n",
    "    [0, 1, 4, 1, 0],\n",
    "    [0, 0, 1, 4, 1],\n",
    "    [0, 0, 0, 1, 3]\n",
    "])\n",
    "\n",
    "# right-hand side vector b\n",
    "F = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Solve\n",
    "\n",
    "x, info = cg(E, F)\n",
    "# Print the solution\n",
    "print(\"Solution x:\", x)\n",
    "print(\"CG Info:\", info) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info=0 means that the solution converged successfully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
